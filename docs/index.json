[{"content":"\nForget about needing Docker\u0026rsquo;s buildx or Redhat\u0026rsquo;s buildah for this OCI container image deep dive. I will explain the OCI container image model and demonstrate building a new container image with a little JSON and a few SHA256 hash sums. You will understand that container images aren\u0026rsquo;t a file you download, but instead a graph of nodes and how the nodes are addressed after reading this article.\nThe Build Artifact A build artifact is the reproducible output created after building an application. The artifact is complete and contains all neccessary directories, compiled binaries, configs, libraries, and a directory structure. Often this is packged as a tar archive (tar) or zip file, but isn\u0026rsquo;t a requriement. The artifact contains everything needed for the application to execute in its intended environment, (arm, x86, linux, windows). There are no container specific files within the artifact, it\u0026rsquo;s just a directory tree of files.\nConvention refers to the directory tree as a root filesystem when working with container images. This is due to the directory tree being mounted at / inside the container. The root fs is not a block device filesystem like ext4.\nOCI Container Image Description A container image consists of a root filesystem and a set of JSON documents describing how the filesystem should be used. Each component of the image is stored as a content addressible blob, identified by the SHA256 hash of the blobs contents. Because identity is derived from content, any modification produces a new digest and a new image identitify instead of mutating the existing image.\nThe image contains a manifest file which ties the blobs together. The manifest references the filesystem layers and configuration file by their SHA256 digests. The configuration file defines how to run the filesystem as a container by listing what process to start, the target operating system, CPU architecture, environment variables, working directory and other execution settings.\nAdditional metadata can be associated with an image after its created, like a Software Bill of Materials. This is done by publishing a seperate artifact that references the images manifest digest. Associating metadata by reference keeps the original image unchanged.\nBecause a container image is composed of multiple content addressible blobs connected by digest references, it does not exist as a single downloadable file. Instead a container image forms a directed graph of nodes, a manifest pointing to configuration and layer blobs, and potentially additional artifacts pointing back to the manifest. Container tooling understands how to traverse this graph and retrieve the referenced content.\nElements of a OCI Container Image When stored on disk, the graph of object addressable objects is represented using a standard layout. The layout stores objects by hash and the JSON descriptors define the edges between those objects. Each object is stored under blobs/\u0026lt;algorithm\u0026gt;/\u0026lt;digest\u0026gt;, where the filename is the cryptographic hash of its contents. If the contents change, the filename must change.\n. ├── blobs │ └── sha256 │ ├── 17eec7bbc9d79fa397ac95c7283ecd04d1fe6978516932a3db110c6206430809 │ ├── 1b44b5a3e06a9aae883e7bf25e45c100be0bb81a0e01b32de604f3ac44711634 │ └── 2771e37a12b7bcb2902456ecf3f29bf9ee11ec348e66e8eb322d9780ad7fc2df ├── index.json └── oci-layout The oci-layout file declares that the top level directory follows the OCI Image Layout specification and indicates the layout version.\n{ \u0026#34;imageLayoutVersion\u0026#34;: \u0026#34;1.0.0\u0026#34; } The index.json file is the entry point of the container image and the root of the graph. The tag \u0026ldquo;latest\u0026rdquo; (\u0026ldquo;org.opencontainers.image.ref.name\u0026rdquo;: \u0026ldquo;latest\u0026rdquo;) is mapped to a specific digest (digest\u0026quot;: \u0026ldquo;sha256:2771e37a12b7bcb2902456ecf3f29bf9ee11ec348e66e8eb322d9780ad7fc2df\u0026rdquo;) that is marked as the manifest (\u0026ldquo;mediaType\u0026rdquo;: \u0026ldquo;application/vnd.oci.image.manifest.v1+json\u0026rdquo;).\n{ \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:2771e37a12b7bcb2902456ecf3f29bf9ee11ec348e66e8eb322d9780ad7fc2df\u0026#34;, \u0026#34;size\u0026#34;: 1035, \u0026#34;annotations\u0026#34;: { \u0026#34;org.opencontainers.image.ref.name\u0026#34;: \u0026#34;latest\u0026#34; } } ] } The manifest at blobs/sha256/2771e\u0026hellip;c2df declares the image configuration and the ordered set of filesystem layers. This example contains a single filesystem layer. In general, the layers array can contain multiple entries, which are applied in order to construct the root filesystem.\n{ \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:1b44b5a3e06a9aae883e7bf25e45c100be0bb81a0e01b32de604f3ac44711634\u0026#34;, \u0026#34;size\u0026#34;: 547 }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:17eec7bbc9d79fa397ac95c7283ecd04d1fe6978516932a3db110c6206430809\u0026#34;, \u0026#34;size\u0026#34;: 2380 } ], ... The trust chain is explicit with a container image. The entry point, index.json, selects a manifest by digest. The manifest selects its config and ordered filesystem layers by digest. The filename of each object under blobs/sha256 must match its contents hash.\nWhen selecting an image by tag, we are unknowingly selecting by digest. While tags are mutable references, digests are not. This is why the tag \u0026ldquo;latest\u0026rdquo; can reference a different digest then the one currently in use.\nOCI Container Image Demo We will construct an OCI container image manually to demonstrate that an image consists only of content addressed blobs and a few JSON documents. This is not a production workflow. The goal is to expose the model, not replace existing tooling. Because higher level objects reference lower level objects by digest, construction begins with the leaves of the graph. Any modification to an object would change the layer digest, which would in turn invalidate the config and manifest digests above it.\nBegin with creating a staging directory for building our conatiner image and the compiled application. The application is statically compiled and named hello. It prints a greeting defined by the GREETING environment variable, else it prints \u0026ldquo;Hello World\u0026rdquo;.\n$ mkdir -p myapp/blobs/sha256 $ tree myapp myapp └── blobs └── sha256 $ ./build/hello HELLO WORLD!!! $ GREETING=\u0026#34;HELLO ADAM\u0026#34; ./build/hello HELLO ADAM Next turn the \u0026ldquo;hello\u0026rdquo; application into a root filesystem blob. Currently the build artifact is in the \u0026ldquo;./build\u0026rdquo; directory which gets archived with tar. Compute the SHA256 digest on the tar archive and the archive is copied to the staging directory.\n$ tree build/ build/ └── hello $ tar --numeric-owner --owner=0 --group=0 -cf layer.tar -C build . $ sha256sum layer.tar 0f11da71a27abfb549ba01cc400d393388116da84abb5f092572c5f2146398cb layer.tar $ cp layer.tar myapp/blobs/sha256/0f11da71a27abfb549ba01cc400d393388116da84abb5f092572c5f2146398cb $ tree myapp/ myapp/ └── blobs └── sha256 └── 0f11da71a27abfb549ba01cc400d393388116da84abb5f092572c5f2146398cb Create a config object which tells the container runtime how to execute the filesystem. For simplicity this example uses an uncompressed tar archive. The diff_ids value matches the layer digest.\n$ cat config.json { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [], \u0026#34;Entrypoint\u0026#34;: [\u0026#34;/hello\u0026#34;] }, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:0f11da71a27abfb549ba01cc400d393388116da84abb5f092572c5f2146398cb\u0026#34; ] } } $ sha256sum config.json f86f75f0d7a7dd4c951a158aca51894ab59f46b0348558a341a589bfcc0d253c config.json $ cp config.json myapp/blobs/sha256/f86f75f0d7a7dd4c951a158aca51894ab59f46b0348558a341a589bfcc0d253c The manifest binds the configuration and layer digests into a single image descriptor. The manifest file also gets hashed and stored in the staging directory.\n$ cat manifest { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:f86f75f0d7a7dd4c951a158aca51894ab59f46b0348558a341a589bfcc0d253c\u0026#34;, \u0026#34;size\u0026#34;: 255 }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:0f11da71a27abfb549ba01cc400d393388116da84abb5f092572c5f2146398cb\u0026#34;, \u0026#34;size\u0026#34;: 1914880 } ] } $ sha256sum manifest d6fceb45932ad49b50f9a1e24b21691b60f861bf46ed9e4a47bd74b8401a2ecd manifest $ cp manifest myapp/blobs/sha256/d6fceb45932ad49b50f9a1e24b21691b60f861bf46ed9e4a47bd74b8401a2ecd Finally create the container entry point and declare the myapp directory is the top level directory of a OCI compliant image.\n$ cat myapp/index.json { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:d6fceb45932ad49b50f9a1e24b21691b60f861bf46ed9e4a47bd74b8401a2ecd\u0026#34;, \u0026#34;size\u0026#34;: 476, \u0026#34;annotations\u0026#34;: { \u0026#34;org.opencontainers.image.ref.name\u0026#34;: \u0026#34;latest\u0026#34; } } ] } $ cat myapp/oci-layout { \u0026#34;imageLayoutVersion\u0026#34;: \u0026#34;1.0.0\u0026#34; } The image layout is now complete. We can import it into a runtime and execute it.\nadam@charizard:~/foo/demo$ skopeo copy oci:./myapp docker-daemon:myapp:latest Getting image source signatures Copying blob 0f11da71a27a done | Copying config f86f75f0d7 done | Writing manifest to image destination $ docker run --rm myapp:latest HELLO WORLD!!! $ docker run --rm -e GREETING=\u0026#34;Hello OCI image\u0026#34; myapp:latest Hello OCI image Why the Model Matters Understanding that a container image is a graph of content addressible blobs isn\u0026rsquo;t trivia. You gain several advantages when you stop seeing the image as a \u0026ldquo;magic file\u0026rdquo; and start seeing it as a verifiable manifest.\nTooling: Stop worrying about whether Docker is better than Podman. They are just different UI wrappers for the same OCI specification. Tooling becomes an implementation detail.\nDebugging: When an image is bloated or a layer is flagged by a security scanner, you don\u0026rsquo;t need specialized tools to tell you why. By inspecting the graph, you can identify exactly which object introduced the content and then inspect that specific content.\nSupply Chain Confidence: By understanding that identity is derived from content you can verify image integrity or attach metadata (like SBOMs) with confidence, knowing that objects bind to the manifest digest.\nClosing My next article will use these principals in constructing a minimal distroless image without using a base image or package manager. Reach out on BlueSky social to let me know if these topics are useful or interesting.\n","permalink":"https://amf3.github.io/articles/virtualization/oci_image/","summary":"\u003cp\u003e\u003cimg alt=\"Image of shipping containers on the pier\" loading=\"lazy\" src=\"/articles/virtualization/oci_image/assets/containers.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eForget about needing Docker\u0026rsquo;s buildx or Redhat\u0026rsquo;s buildah for this OCI container image deep dive. I will explain the OCI container image\nmodel and demonstrate building a new container image with a little JSON and a few SHA256 hash sums.  You will understand that container images\naren\u0026rsquo;t a file you download, but instead a graph of nodes and how the nodes are addressed after reading this article.\u003c/p\u003e","title":"Open Container Image Format"},{"content":"There\u0026rsquo;s a popular myth in the Docker community. The myth is that it\u0026rsquo;s possible to mount /var/run/docker.sock with read only options. Even large container projects like Traefik get this wrong.\nBind mounting the Docker socket with read only options doesn\u0026rsquo;t work the way one thinks it would. This post will explain how using the \u0026quot;:ro\u0026quot; option when mounting the Docker socket is little more than security theater and to discuss alternatives.\nPlease read this post if you\u0026rsquo;re adding /var/run/docker.sock:/var/run/docker.sock:ro to Docker compose files.\nSocket behavior Demo The issue isn\u0026rsquo;t specific to Docker, instead its normal POSIX behavior. This is easily demonstrable by creating a Unix socket with python and not using Docker at all.\nThis code block will create a Unix socket and wait for clients to connect. When the client connects, the server prints received data to standard out.\n#! /usr/bin/env python3 # Save as server.py import socket import os SOCK = \u0026#34;/tmp/control.sock\u0026#34; try: os.unlink(SOCK) except FileNotFoundError: pass srv = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) srv.bind(SOCK) os.chmod(SOCK, 0o660) # same write permissions that are used with docker.sock srv.listen(1) print(\u0026#34;listening\u0026#34;) conn, _ = srv.accept() print(\u0026#34;client connected\u0026#34;) while True: data = conn.recv(1024) if not data: break print(\u0026#34;received:\u0026#34;, data.decode()) conn.sendall(b\u0026#34;ack\\n\u0026#34;) The next block contains the client code. Notice that the client socket path (/mnt/control.sock) is different from the server (/tmp/control.sock) as it\u0026rsquo;s using the read only bind mount created in the next step.\n#! /usr/bin/env python3 # Save as client.py import socket sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) sock.connect(\u0026#34;/mnt/control.sock\u0026#34;) # connect to read-only mount point sock.sendall(b\u0026#34;This is a write string from the client\\n\u0026#34;) print(sock.recv(1024)) Start the server code to create /tmp/control.sock and then bind mount the socket to a new mount point with read-only access.\n$ sudo touch /mnt/control.sock # create the mount point $ sudo mount --bind /tmp/control.sock /mnt/control.sock # mount the socket to a new mount point $ sudo mount -o remount,ro,bind /mnt/control.sock # Remount the socket mount point with read only options $ mount | grep control.sock # Display the read-only mount tmpfs on /mnt/control.sock type tmpfs (ro,nosuid,nodev,size=16037780k,nr_inodes=1048576,inode64) Next run the client. At this point it should not be possible to write to the socket, but this screen shot shows otherwise.\nIf running the demo locally, don\u0026rsquo;t forget to clean up the bind mount.\nsudo umount /mnt/control.sock # unmount the read-only bind mount sudo rm /mnt/control.sock # remove the mount point rm /tmp/control.sock # remove the control.sock socket created by the python server code. How did this work? A bind mount creates a second path to the same underlying kernel object. The \u0026ldquo;ro\u0026rdquo; mount option applies filesystem level restrictions at the mount point, but does not change the behavior or permissions of the object being mounted. When a process opens the object, further access control is determined by the object itself.\nIn this case, I\u0026rsquo;m unable to delete the socket or change its metadata, but I can still connect to it.\nThe read and write behavior for a socket is different from a normal file. Files use the read() and write() system calls, sockets use send() and recv() system calls. The read only mount option to the mount point representing the socket allows the client connection. As send() is a different system call from write(), send() is not blocked by the filesystem and data passes through the socket.\nSocket behavior is the big misunderstanding in the Docker community. People conflate filesystem writes with sending commands over a socket.\nDocker API Docker defaulting to using Unix sockets isn\u0026rsquo;t the problem. The issue is that the Docker API is course grained and was not built with multitenancy in mind. While the Docker API is built on HTTP verbs, the HTTP endpoints don\u0026rsquo;t map cleanly between safe and unsafe operations. This makes verb based authorization ineffective and explains why granting a container access to the Docker socket implicitly grants elevated capabilities for that container.\nThis example only uses HTTP GET requests to demonstrate the missing separation of safe and unsafe operations in the Docker API. The example is able to exfiltrate TLS certificates from a running container. No container restart, privilege escalation, or filesystem write is required.\n$ # Get a list of running containers $ curl -s --unix-socket /var/run/docker.sock -X GET http://localhost/containers/json | jq \u0026#39;.[] | {Names, Id, Image}\u0026#39; { \u0026#34;Names\u0026#34;: [ \u0026#34;/cadvisor\u0026#34; ], \u0026#34;Id\u0026#34;: \u0026#34;dfe1461c0a7a1a34bd0b4df6ccc1ecd77d1fc6483be80c248ef916a3e79d8bc9\u0026#34;, \u0026#34;Image\u0026#34;: \u0026#34;gcr.io/cadvisor/cadvisor:v0.52.1\u0026#34; } $ # Exfiltrate certificates from the running cAdvisor container and save locally to a file named certificates.tar $ curl -D - -s --unix-socket /var/run/docker.sock -X GET http://localhost/containers/dfe1461c0a7a1a34bd0b4df6ccc1ecd77d1fc6483be80c248ef916a3e79d8bc9/archive?path=/etc/ssl -o certificates.tar HTTP/1.1 200 OK Api-Version: 1.51 Content-Type: application/x-tar Docker-Experimental: false Ostype: linux Server: Docker/28.5.2 (linux) X-Docker-Container-Path-Stat: eyJuYW1lIjoic3NsIiwic2l6ZSI6MTAsIm1vZGUiOjIxNDc0ODQxNDEsIm10aW1lIjoiMjAyNS0wMi0xM1QyMzoxNzowOFoiLCJsaW5rVGFyZ2V0IjoiIn0= Date: Fri, 09 Jan 2026 17:04:45 GMT Transfer-Encoding: chunked $ # List the contents of the uncompressed downloaded tar archive $ tar -tf certificates.tar ssl/ ssl/cert.pem ssl/certs/ ssl/certs/ca-certificates.crt ssl/ct_log_list.cnf ssl/ct_log_list.cnf.dist Containers like cAdvisor or Traefik want access to the docker socket. These are large projects and it\u0026rsquo;s unlikely they would intentionally publish harmful code. But supply chain attacks are common and it\u0026rsquo;s possible for malicious behavior to be introduced into a normally trusted container.\nWhen a container is granted access to the Docker socket, it implicitly inherits the full authority of the Docker API. Docker does not have a mechanism to distinguish between intended and unintended use of the API or a way to scope capabilites. This limitation is why socket proxies exist, providing the missing authorization layer in Docker.\nSocket Proxies A local proxy in front of the Docker socket can filter the endpoints a container can access. Identity based solutions like Open Policy Agent are available and work well for human users with authentication tokens. But containers don\u0026rsquo;t use custom headers or use mTLS by default. The application code within the container would need to be modified to send authentication with its requests.\nThis is why most socket proxy projects use topology for authentication and authorization. The proxy is deployed in a side car pattern with the application container, where Docker socket access from the application container passes through the socket proxy container. In topology based sidecar deployments, where the request comes from determins what the request can do, not who is making the request.\nPopular socket proxy projects are Tecnativa socket-proxy which uses HAProxy with configurable endpoint filtering, and Wollomatic socket-proxy a Go based alternative. You configure these proxies to allow specific endpoints. Like allowing Traefik access containers/json to discover services while blocking request to containers/create.\nTradeoffs There\u0026rsquo;s no true identity-based authorization. The proxy can\u0026rsquo;t distinguish between legitmate requests from Traefik versus another container. The proxy only knows requests coming from particular networks get particular permissions.\nDuplicated infrastructure is another concern. Each application needs its own proxy instance. Running three containers that need Docker socket access, results in configuring and deploying three separate proxies.\nPolicy/filter configuration within the proxy is static and set at deployment time. It\u0026rsquo;s not possible to dynamically adjust permissions based on runtime conditions.\nThe Docker API design sneaks through proxy endpoints. It\u0026rsquo;s not possible to grant access to container labels without granting access to environment variables, the latter likely contain application secrets.\nEven with these limitations, socket proxies provide a significant security improvement over direct Docker socket mounts. Socket proxies enforce least privilege access at the API level, which is exactly where the authorization needs to happen.\nWhat\u0026rsquo;s Next? If you\u0026rsquo;re currently mounting the Docker socket from your containers with :ro consider switching to a socket proxy solution. The topology based approach isn\u0026rsquo;t perfect but it provides Docker API filtering that read only mounts can\u0026rsquo;t. Yes a socket proxy requires more setup, but it provides a real security boundary rather than the appearance of one.\nIf you have thoughts on this content, let me know what you think on Bluesky.\n","permalink":"https://amf3.github.io/articles/virtualization/docker_socket/","summary":"\u003cp\u003eThere\u0026rsquo;s a popular myth in the Docker community.\nThe myth is that it\u0026rsquo;s possible to mount /var/run/docker.sock with read only options.  Even large container projects like\n\u003ca href=\"https://github.com/traefik/traefik/blob/f7280439e6378221a541910f43a01323d52db048/docs/content/user-guides/docker-compose/basic-example/index.md?plain=1#L122\"\u003eTraefik\u003c/a\u003e\nget this wrong.\u003c/p\u003e\n\u003cp\u003eBind mounting the Docker socket with read only options doesn\u0026rsquo;t work the way one thinks it would.  This post will explain how using the\n\u003cstrong\u003e\u0026quot;:ro\u0026quot;\u003c/strong\u003e option when mounting the Docker socket is little more than \u003ca href=\"https://en.wikipedia.org/wiki/Security_theater\"\u003esecurity theater\u003c/a\u003e and to\ndiscuss alternatives.\u003c/p\u003e\n\u003cp\u003ePlease read this post if you\u0026rsquo;re adding \u003ccode\u003e/var/run/docker.sock:/var/run/docker.sock:ro\u003c/code\u003e to Docker compose files.\u003c/p\u003e","title":"Docker Socket Myths"},{"content":"Who Should Not Read This Post This post does not discuss:\ndata support for multiple writers having zero downtime cloud-scale best practices Instead I discuss embracing single points of failure (SPOF) in exchange for simplicity within environments that can tolerate downtime.\nPermission to Not Over-Engineer In environments that tolerate downtime, having a SPOF is great because they are simple to deploy and it\u0026rsquo;s obvious what failed.\nLets examine the typical home router. In addition to routing packets between the LAN and the Internet, the home router also provides DHCP and DNS services. Then someone learns of PiHole and runs it on a separate device like a Raspberry PI. Now the router queries PiHole for adblock entries during DNS resolution. Ad-free browsing works works great; until HTTP requests stop working. With this scenario, it\u0026rsquo;s not obvious if it\u0026rsquo;s an issue on the router or an issue with PiHole.\nI recently ran into a situation on deciding how to improve my container deployments.\nCurrently I use Docker Engine with Compose files to manage running containers. Container storage is either named volumes or bind mounts, both of which write to the local disk of the host running Docker Engine. This works but it has a few issues:\nServices go offline during planned maintenance on the host Container placement is manual. Having a second hardware instance doesn\u0026rsquo;t help. Docker Compose deployments cause container restarts Failed Docker Compose deployments have no rollback I started to look at K3S, Nomad, and Docker Swarm to address these short comings. While I still plan on using a single hardware instance, orchestration would help with downtime during planned maintenance. This led to thinking how the design would change by adding a second host and because of distributed storage, would I be trapped by choices made today?\nCompute Does Not Need to Scale with Storage I know many distributed storage solutions use a consensus model requiring three hosts. Does this mean I\u0026rsquo;m required to run three computers and pay for the additional power usage?\nIt feels strange to admit, but this led to two evenings of not knowing how to proceed. I assumed that adding a second host meant I also needed distributed storage and now I need three hosts. This assumption stalled everything. Eventually I realized its okay to have orchestration for compute but not storage.\nThinking about storage and compute as separate problems was the unlock I needed to move forward. I won\u0026rsquo;t need the complications of running Kubernetes or Nomad. The simpler solution of using Docker Swarm can solve my deployment requirements with either one or two hardware instances.\nCompute: What Swarm Provides Swarm is responsible for compute orchestration and overlay networking. It does not provide storage orchestration or multi-writer safety.\nSwarm mode is enabled on each Docker instance, and the instance registers with the cluster as either a manager role or worker role. The Swarm worker provides CPU and memory resources to the running containers and must have sufficient capacity for its workloads.\nAs a Swarm manager, the instance is responsible for orchestration, (scheduling, container placement, maintaining cluster state) by using RAFT consensus. RAFT needs a quorum to make decisions and a quorum is calculated as \u0026ldquo;(N-1)/2 + 1\u0026rdquo;, where N is the number of Swarm managers. In a two node deployment I can only have a single manager, otherwise quorum will never be reached.\nIn a two node Swarm deployment, one node would run both the worker and manager. The second node only joins as a worker. Write heavy apps are assigned to the manager instance and stateless apps assigned to the worker node. Swarm labels are applied to services to ensure container placement.\nStorage Swarm itself does not manage storage. Swarm presumes storage is managed externally to its processes. For example, CephFS would be mounted by external systems on the host OS and treated as a local filesystem by Swarm. A take away is Swarm does not provide multi-writer safety. If two containers attempt to write to the same dataset, Swarm does nothing to provide locking or prevent data corruption. It assumes the underlying storage layer handles this.\nDatabases have built in multi-writer support by providing locking at either the table or row level. Usually they are capable of replicating data to a secondary instance. Because of these features, they solve my multi-instance storage problem by scaling independently of the container platform.\nLocal filesystems are more challenging. Detected filesystem changes must be transferred to secondary storage. I use ZFS snapshots with send and receive over the network. Snapshots are taken on the manager instance and replicated to the worker node every 15 minutes using zrepl.\nThis approach is a tradeoff for simplicity and assumes one is okay with a bounded data-loss window. If the primary ZFS dataset gets corrupted, up to 15 minutes of data could be lost. Write heavy applications are pinned to the manager node so their datasets can be replicated.\nAnother challenge is application-level locking. Databases enforce locking in their clients, but local filesystems do not. While mechanisms like flock or fcntl exist, they only work if the application uses them. To avoid data corruption caused by lack of locking, I make sure to deploy only a single container for each dataset. This greatly simplifies the storage design by avoiding the need of distributed locking.\nFor container storage, I use a mix of named volumes and bind mounts. Persistent data that requires replication uses bind mounts. Ephemeral data that needs to persist between container restarts uses named volumes. Because bind mounts reference host paths, the same directory structure and ownership (UID) must exist on both hardware instances. Configuration management is used to ensure consistency between the two hardware instances.\nFailure Modes Single points of failure are not accidental in this design. They provide predictable and observable failure modes, that act as constraints for modeling the system.\nWorker node failure: This is a non-event. Stateless services are rescheduled onto the manager node by Swarm. No data recovery is required.\nManager node failure: This is an expected control-plane failure where manual intervention is required. The worker node is promoted to manager and services restarted on the manager. This is a deliberate tradeoff in exchange for simplicity.\nData corruption on the manager node: This is treated the same as a manager failure. The worker node is promoted to manager and services are restored using the most recent ZFS snapshot.\nReplication lag: Replication is monitored to ensure snapshot transfer time remains below the snapshot interval. If replication falls behind, the response is to reduce the snapshot interval or increase network capacity. This keeps data loss bounded and predictable.\nMulti-writer data corruption: If because of misconfiguration, more than one container (writer) is launched for the same dataset and corruption occurs, stop all containers for the application and restore the data from the last known-good ZFS snapshot.\nDepending on snapshot age and response time, expected downtime for these scenarios is 15-30 minutes. The goal is not uninterrupted service, but fast understandable recovery.\nWhen the solution is smaller than the problem I\u0026rsquo;m not defending Swarm in this post. I\u0026rsquo;m defending clear failure modes, explicit ownership, and operational practicality. The goal is to start with requirements and choose the smallest system that satisfies them.\nBecause this design keeps the data independent of orchestration, it provides an escape hatch to K3s or Nomad without having to decouple data from Swarm.\nIf my availability requirements change, then I would add a third low-power node like a Raspberry Pi to establish a three node quorum. I would also migrate from Swarm to K3s to leverage the Longhorn distributed filesystem which is only available under Kubernetes. For multi-writer workloads, shared filesystems like NFS could work if the container application supports file locking. Neither Swarm or Longhorn provides multi-writer locking on behalf of the application.\nIf you have any thoughts on this post, feel free to share them on Bluesky.\n","permalink":"https://amf3.github.io/articles/arch/2swarm_nodes/","summary":"\u003ch2 id=\"who-should-not-read-this-post\"\u003eWho Should Not Read This Post\u003c/h2\u003e\n\u003cp\u003eThis post does not discuss:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edata support for multiple writers\u003c/li\u003e\n\u003cli\u003ehaving zero downtime\u003c/li\u003e\n\u003cli\u003ecloud-scale best practices\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInstead I discuss embracing single points of failure (SPOF) in exchange for simplicity within\nenvironments that can tolerate downtime.\u003c/p\u003e\n\u003ch2 id=\"permission-to-not-over-engineer\"\u003ePermission to Not Over-Engineer\u003c/h2\u003e\n\u003cp\u003eIn environments that tolerate downtime, having a SPOF is great because they are simple to deploy and it\u0026rsquo;s obvious what failed.\u003c/p\u003e\n\u003cp\u003eLets examine the typical home router.  In addition to routing packets between the LAN and the Internet, the home router also\nprovides DHCP and DNS services.  Then someone learns of PiHole and runs it on a separate device like a Raspberry PI. Now the router\nqueries PiHole for adblock entries during DNS resolution.  Ad-free browsing works works great; until HTTP requests\nstop working.  With this scenario, it\u0026rsquo;s not obvious if it\u0026rsquo;s an issue on the router or an issue with PiHole.\u003c/p\u003e","title":"Decoupling Compute and Storage"},{"content":"This post is the README for my home network and something I can refer to later. It\u0026rsquo;s the story of how I replaced a underperforming consumer-grade router with a fully virtualized RouterOS instance using QEMU and PCI passthrough for the network.\nLike any software project, this project is source controlled and rebuildable from scratch. If the router dies, I simply deploy a new router instance. This is the reproducibility you\u0026rsquo;d expect in a datacenter, not a home office.\nThis guide will cover hardware planning, automating the installation process, and disaster recovery. Even if you\u0026rsquo;re not an operations person, you\u0026rsquo;ll have the tools to build and rebuild your network.\nWhy a Virtual Router? Using a virtual router allows me to perform experiments or make basic changes against a test instance before taking down the Internet. I\u0026rsquo;ve written about these benefits before.\nHardware upgrades are simple as I\u0026rsquo;m not locked into a vendor\u0026rsquo;s hardware platform. Instead I can use a more powerful x86 CPU or faster network cards. I frequently find used 8th or 10th generation Intel CPUs on Ebay can be less expensive then purchasing the latest consumer grade router.\nIn my case, I noticed both VPN traffic and QoS performance were lagging with the MIPS based CPU inside my consumer router. It seemed like a great opportunity to put this virtual router idea into action as I had old desktop parts in storage.\nArchitecture \u0026amp; Planning I upgraded from a MIPS based Mikrotik Hex-S by migrating to a PC built with old parts. The CPU is a 7th Gen (2017) Intel Celeron (G3930), with 4GB of memory, and a 1GB SSD. I wanted additional network ports and purchased a dual 1Gb Intel network card (I350) for $27 on Ebay. None of these parts would be considered high-end today.\nIn this wiring diagram, you\u0026rsquo;ll see the virtual router has the dual port PCI card assigned to it. The host OS is using only the onboard ethernet controller for connectivity. This allows me to isolate the dual port PCI card from the host OS, ensuring host OS traffic going to the Internet is passing through the switch and then the PCI card assigned to the router. There are no network shortcuts which could allow the host OS to bypass the virtual router.\nThe media converter is something like a DOCIS cable modem or a copper to fiber converter.\nFor controlling the VM itself, I\u0026rsquo;m using QEMU commands (with KVM extensions) inside a Systemd Unit file. Of course there are abstractions like libvirt which wrap QEMU, but it felt unnecessary for this simple use case. Invoking QEMU directly also provides a learning opportunity to understand what\u0026rsquo;s being abstracted.\nPreparing the Host I started by assembling my PC parts into a working desktop computer and installed Ubuntu server image 25.10 as the host OS. Make sure the qemu-utils and qemu-system-x86 packages are installed. I wanted a service account to run the virtual router process. I created a user named qemu and gave it access to the kvm group.\nIf doing this by hand:\nsudo useradd -r -s /usr/sbin/nologin qemu sudo usermod -aG kvm qemu Isolating Network Cards This step is generic to isolating PCI cards for any type of virtual machine. The first step is to get the PCI ID tuple that\u0026rsquo;s burned into the card\u0026rsquo;s firmware.\n$ lspci -nn | grep Ethernet 01:00.0 Ethernet controller [0200]: Intel Corporation I350 Gigabit Network Connection [8086:1521] (rev 01) 01:00.1 Ethernet controller [0200]: Intel Corporation I350 Gigabit Network Connection [8086:1521] (rev 01) 06:00.0 Ethernet controller [0200]: Realtek Semiconductor Co., Ltd. RTL8111/8168/8211/8411 PCI Express Gigabit Ethernet Controller [10ec:8168] (rev 0c) The first column shows the card location on the PCI bus. We\u0026rsquo;ll need those values later but not now. The numbers we are interested in are the \u0026ldquo;8086:1521\u0026rdquo; tuple. The first value (8086) in the tuple is registered to Intel, the second value is the cards model number which maps to the i350 family of network controllers.\nBecause I want to assign the Intel network card to the router VM, I first need to denylist the IGB driver to prevent the host OS from using it.\necho \u0026#34;blacklist igb\u0026#34; | sudo tee /etc/modprobe.d/blacklist-igb.conf sudo update-initramfs -u Next I need to load the vfio-pci module at boot time so the network card can be assigned to the VM.\necho \u0026#39;GRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;$GRUB_CMDLINE_LINUX intel_iommu=on vfio-pci.ids=8086:1521\u0026#34;\u0026#39; | sudo tee /etc/default/grub.d/virtualRouterNic.cfg sudo update-grub sudo reboot Now the steps I listed aren\u0026rsquo;t ideal for everyone but they work for me. Deny listing the IGB kernel module impacts all Intel network cards needing that driver. My motherboard has a Realtek NIC and I have a single Intel PCI network card that\u0026rsquo;s assigned to the router VM, so this approach works for me.\nBut let\u0026rsquo;s pretend that instead, the motherboard has a Intel NIC that also uses the IGB kernel module. The listed steps would prevent the host OS from using the network adaptor that\u0026rsquo;s on the motherboard because we globally deny listed the IGB driver. If this was the case, I\u0026rsquo;d purchase a second PCI network card that didn\u0026rsquo;t use the IGB kernel module and use that for the host OS. It\u0026rsquo;s something to plan for when selecting hardware for this project.\nFinally, I need to create a few udev rules. udev is responsible for detecting new hardware being plugged in and using custom rules can modify behavior during hardware detection. The qemu user, a member of the kvm group, is running the router VM process and normally doesn\u0026rsquo;t have write access to the Intel network card being assigned to the router VM. The updated udev rules will grant the kvm group members write permissions to the vfio device.\n$ sudo vi /etc/udev/rules.d/99-qemu-vfio.rules KERNEL==\u0026#34;vfio\u0026#34;, GROUP=\u0026#34;kvm\u0026#34;, MODE=\u0026#34;0660\u0026#34; KERNEL==\u0026#34;[0-9]*\u0026#34;, SUBSYSTEM==\u0026#34;vfio\u0026#34;, GROUP=\u0026#34;kvm\u0026#34;, MODE=\u0026#34;0660\u0026#34; KERNEL==\u0026#34;kvm\u0026#34;, GROUP=\u0026#34;kvm\u0026#34;, MODE=\u0026#34;0660\u0026#34; Reload the udev rules with:\nsudo udevadm control --reload-rules sudo udevadm trigger --action=add /dev/vfio/1 Then validate that members of the kvm group can write to the vfio device.\n$ id qemu uid=999(qemu) gid=987(qemu) groups=987(qemu),993(kvm) $ ls -l /dev/vfio/ total 0 crw-rw---- 1 root kvm 241, 0 Aug 11 19:23 1 drwxr-xr-x 2 root root 80 Aug 11 19:23 devices crw-rw---- 1 root kvm 10, 196 Aug 11 19:23 vfio Network Time Protocol Due to clock drift on virtual machines, ensure a time service like NTP is enabled on both the host and within the virtual router.\nDocumentation for the host OS will explain how this is enabled. For example, Ubuntu previsouly used systemd-timesyncd but later switched to the chrony package with Ubuntu 25.10. Similarly, the router software should include a NTP client so check it\u0026rsquo;s documentation as well.\nRouting software Mikrotik has a virtual machine image and being familiar with their command line, it seemed like a good place for me to start. I was asked in a Bluesky thread on why I use a Mikrotik VM when a basic Linux VM with traffic-control (tc) can act as a router. I responded by stating that I agree, but familarity with the Mikrotik CLI won out. A second bonus is should this virtualized router not work out, I can always go back to the Mips based Hex-S router I previously used.\nOther platforms like OpenWRT or OpnSense also have x86 virtual machine images available and would work for this project.\nI ended up using the stable 7.x release of Mikrotik\u0026rsquo;s CHR (Cloud Hosted Router) product and downloaded the raw image which I later converted into a qcow2 image that allows taking snapshots. Network card drivers included with CHR are the default drivers from the Linux kernel and a few others. Here\u0026rsquo;s a partial module list from one of Mikrotik\u0026rsquo;s change log entries (v7.12).\nx86 - added support for Mellanox ConnectX-6 Dx NIC; *) x86 - i40e updated driver to 2.23.17 version; *) x86 - igb updated driver to 5.14.16 version; *) x86 - igbvf updated driver from in-tree Linux kernel; *) x86 - igc updated driver to 5.10.194 version; *) x86 - ixgbe updated driver to 5.19.6 version; *) x86 - Realtek r8169 updated driver; Configuring the router itself is out of scope for this document, but this is the high level overview. The router configuration is a text file I keep in git. The file format is fairly simple with each line having a RouterOS command. During initialization, I can upload the text file to the router and have the router execute all the commands in the text file.\nThe config file can be checked into git and used as the source of truth.\nOne consideration with text based configs is managing secrets, which is covered in the next section.\nSecrets With the plain text router config file being checked into git, how are secrets handled?\nI\u0026rsquo;m a fan of using 1Password for managing secrets and this is why. 1Password provides a CLI which reads vault secrets and is able to inject them into templates or environment variables. Additionally if a reference in the template file is missing from the vault, the CLI will fail to generate any output. This prevents generating router configs with incomplete or missing commands.\nHere\u0026rsquo;s an example where I declare a secret reference in a file called router_config.tmpl. I then use the op command to generate a new file containing plain-text secrets named router_config.rsc. router_config.rsc is what\u0026rsquo;s uploaded to the router.\n$ cat router_config.templ /user set [find name=admin] password={{ op://API_Vault/Test_Login/password }} $ op inject -i router_config.templ -o router_config.rsc /Users/adam/work/public/amf3.github.io/router_config.rsc $ cat router_config.rsc /user set [find name=admin] password=my_secret_password A .gitignore entry prevents me from commiting the router_config.rsc file with plain text secrets into git. I still have to remember to delete the .rsc file so it\u0026rsquo;s not perfect. But the process is simple and it works.\nQemu and Systemd Because CHR is distributed as a virtual disk image, there\u0026rsquo;s nothing to install. Treat it like a OS disk when invoking QEMU.\nThese are the qemu options I use when starting the router VM.\n-machine q35 emulates a newer southbridge chipset for better PCI support. -device entries specify the Intel PCI card bus ID. If the card were moved to a different motherboard slot, those values would need to be updated. -serial and -monitor options create sockets to the VM\u0026rsquo;s and QEMU\u0026rsquo;s console that I can access with socat. $ /usr/bin/qemu-system-x86_64 \\ -machine q35,accel=kvm \\ -cpu host -smp cpus=2 \\ -m 512M -boot c \\ -nic none \\ -drive file=/srv/virtual_machines/routeros/chr-7.19.3.qcow2,if=virtio,format=qcow2 \\ -serial unix:/srv/virtual_machines/routeros/console.sock,server,nowait \\ -monitor unix:/srv/virtual_machines/routeros/monitor.sock,server,nowait \\ -display none -vga none \\ -device vfio-pci,host=01:00.0 \\ -device vfio-pci,host=01:00.1 And the socat command to access the VM console or QEMU monitor.\nsudo -u qemu socat STDIO,cfmakeraw,isig=1 UNIX:/srv/virtual_machines/routeros/console.sock To ensure the router is automatically started on reboots, I wrapped the qemu command inside a systemd service unit named routeros.service.\n[Unit] Description=RouterOS VM (QEMU) After=network-online.target Wants=network-online.target StartLimitIntervalSec=60 StartLimitBurst=5 [Service] # Run as unprivileged user/group User=qemu Group=kvm # Resources / limits LimitMEMLOCK=infinity LimitNOFILE=1048576 OOMScoreAdjust=-1000 # Set CAP_SYS_NICE for qemu process. CapabilityBoundingSet=CAP_SYS_NICE AmbientCapabilities=CAP_SYS_NICE # Keep the rest of the system protected; allow explicit VM path writable ProtectSystem=strict ReadWritePaths=/srv/virtual_machines/routeros ProtectHome=yes PrivateTmp=yes # Allow QEMU process to gain capabilities from AmbientCapabilities above NoNewPrivileges=no # Execution: adjust memory/cpu/paths/PCI IDs to your environment ExecStart=/usr/bin/qemu-system-x86_64 \\ -machine q35,accel=kvm \\ -cpu host -smp cpus=2 \\ -m 512M -boot c \\ -nic none \\ -drive file=/srv/virtual_machines/routeros/chr-7.19.3.qcow2,if=virtio,format=qcow2 \\ -serial unix:/srv/virtual_machines/routeros/console.sock,server,nowait \\ -monitor unix:/srv/virtual_machines/routeros/monitor.sock,server,nowait \\ -display none -vga none \\ -device vfio-pci,host=01:00.0 \\ -device vfio-pci,host=01:00.1 # After process starts, restrict access to console and monitor sockets via filesystem permissions ExecStartPost=/bin/sh -c \u0026#39;while [ ! -S /srv/virtual_machines/routeros/monitor.sock ]; do sleep 0.5; done; chmod 600 /srv/virtual_machines/routeros/*.sock\u0026#39; # Attempt a graceful shutdown by sending a powerdown event to the QEMU monitor. ExecStop=/bin/sh -c \u0026#39;echo \u0026#34;system_powerdown\u0026#34; | socat - UNIX-CONNECT:/srv/virtual_machines/routeros/monitor.sock || true\u0026#39; TimeoutStopSec=120 KillMode=mixed Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Don\u0026rsquo;t forget to refresh systemd to pick up the changes before issuing enable or restart commands for the new service.\nsudo systemctl daemon-reload Maintenance and Recovery Monitoring/Alerting I keep alerting for the router uncomplicated. If the Internet stops working, I know there\u0026rsquo;s a problem.\nThis decision is partly due to a alerting design issue. If the Internet is down, local services are unable to post events to Slack channels which is how my phone receives alerts.\nFor a home network, user reports works well enough.\nGeneral Maintenance Maintenance on the host OS means possible downtime for the router if a reboot is required. Conceptually this is no different from a consumer router appliance. Download and apply the updates, reboot the host when network usage is low. When possible, I test updates to router configs in a virtual machine using QEMU. The router only cares about interfaces being available, not that it\u0026rsquo;s using a vfio device.\nWhen configuring the router software, I use a config file instead of making changes in the web UI. Using a config file allows me to track changes in a git repo. Mikrotik uses a script-like text file format where CLI commands are applied as the file is processed. This example shows how the external interface receives a DHCP address from my Internet provider while defining a RFC 1918 address for internal use on a second interface.\n/ip dhcp-client add comment=admin_interface disabled=no interface=ether1 use-peer-dns=yes /ip address add address=192.168.10.1/24 interface=ether2 network=192.168.10.0 The file is transferred to the router\u0026rsquo;s local filesystem by either using the web interface to upload the file or scp. Once stored on the router, I issue a router reset command that wipes all settings and reapplies the configuration.\n/system/reset-configuration keep-users=no no-defaults=yes skip-backup=yes run-after-reset=router_config.rsc Other routing projects offer similar features. OpenWRT supports text based configs with it\u0026rsquo;s Unified Configuration Interface API and Opnsense can be configured with it\u0026rsquo;s config.xml file. Meaning this approach isn\u0026rsquo;t limited to Mikrotik software.\nRouter Recovery The virtual router is using a QCOW2 formated file as a virtual disk. This allows one to create restore points by taking point-in-time snapshots of content on the disk before making changes to the router.\nTo take a snapshot with the qemu-img command, stop the virtual router by issuing a shutdown command with systemd, systemctl stop routeros.service. This is needed so qemu-img can get a write lock on the qcow2 file.\nNext, to create a snapshot within the qcow2 file, run qemu-img snapshot -c mySnapshotLabel /path/to/routeros.qcow2. The label \u0026ldquo;mySnapshotLabel\u0026rdquo; should be unique and meaningful so it can be located later.\nFinally start the virtual machine with systemd and continue with patching the router OS or updating it\u0026rsquo;s configurations.\nIf an error occurs during or after router maintenance, stop the virtual router and apply the most recent snapshot in the qcow2 image to revert the change.\nUse qemu-img snapshot -l /path/to/routeros.qcow2 to view existing snapshots. Then apply the snapshot with qemu-img snapshot -a myGoodSnapshot /path/to/router.qcow2. When the old snapshot is applied, start the router to resume services.\nSometimes when performing maintenance and while the router VM is stopped, I copy the qcow2 file to a local NAS to have a second copy in case of a disk failure.\nHost OS Recovery Things get more complicated if the host OS fails. You need to know what PCI ids are used by which cards and this is why I\u0026rsquo;m approaching this article from manual configuration. It would be simple to point to an Ansible repo specific to my setup, but that doesn\u0026rsquo;t help the reader.\nAlso Ansible needs a working system to apply changes. Because a rebuild implies installing the OS on new hardware, cloud-init is my preferred method to automate recovery.\nOnce the host OS is configured, I can copy the qcow2 image from my NAS and apply my text based router config to the new virtual router instance. While the process isn\u0026rsquo;t fully automated, it\u0026rsquo;s simple and reproducible. While I haven\u0026rsquo;t needed to do a full host rebuild, the process has been tested in VMs.\nIt\u0026rsquo;s important to remember that the internet is unavailable during this time, so having a local copy of host OS ISO\u0026rsquo;s or router images is important.\nLessons Learned I was expecting to hit performance issues due to old CPU and limited RAM. After several of months of use, the virtual router has been a success. It\u0026rsquo;s a stable service and VPN connections are much more performant.\nWhether this is something someone should try themselves is debateable. While the setup is more complicated there are benefits of doing this. Getting faster router hardware or possibly repairing failed hardware with spare on-site commodity components.\nFor future upgrades, I\u0026rsquo;m going to explore using newer generation CPUs. The idle power consumption on 12th generation Intel CPUs is significantly less than the 17-20 watts my 7th generation Celeron system is currently using. A 12th gen i5 desktop I borrowed, went as low as 3-5 watts during idle. My eventual power goal is to get consumption under 10 watts.\nIf you try this approach and have questions, feel free to reach out on Bluesky social. Otherwise, let me know how it worked.\n","permalink":"https://amf3.github.io/articles/virtualization/virtual_router/","summary":"\u003cp\u003eThis post is the README for my home network and something I can refer to later.  It\u0026rsquo;s\nthe story of how I replaced a underperforming consumer-grade router with a fully virtualized RouterOS instance\nusing QEMU and PCI passthrough for the network.\u003c/p\u003e\n\u003cp\u003eLike any software project, this project is source controlled and rebuildable from scratch.  If the\nrouter dies, I simply deploy a new router instance.  This is the reproducibility you\u0026rsquo;d expect\nin a datacenter, not a home office.\u003c/p\u003e","title":"Uh-oh. Is the router down?"},{"content":"Adding Subcommands to Go CLIs Command Line Interfaces (CLIs) use subcommands and flags to enable different program features. A subcommand is a grouping of related features, and flags are options for controlling those features. The openssl command provides a great example of subcommands and flags. openssl rand -base64 8 will generate 8 random bytes of data with hexadecimal output. The subcommand is \u0026ldquo;rand\u0026rdquo; and \u0026ldquo;-base64\u0026rdquo; is the flag. Other openssl subcommands like \u0026ldquo;s_client\u0026rdquo; or \u0026ldquo;x509\u0026rdquo;, provide different features and each has their own options.\nWhen running the openssl command, the shell passes each space separated value into an argument list. It\u0026rsquo;s possible to parse the list by checking if the second value is a subcommand, then looping over the rest to figure out which are flags. Some programs like git take this further by treating subcommands as seperate executables. git status actually runs the git-status command in a subshell. The approach works but keeping help output and flag parsing consistent between executables gets messy.\nFortunately several libraries are available to make subcommands and flags easier to manage. The one that I finally chose is urfave/cli which offers a builder style API. The examples below use version 3 of the library which can be imported as:\nimport \u0026#34;github.com/urfave/cli/v3\u0026#34; urfave/cli with flags This is how simple the urfave/cli library is to use. I configure a cli.Command struct, then call Run() to execute it.\nfunc main() { cmd := \u0026amp;cli.Command{ ... struct goes here ... } if err := cmd.Run(context.Background(), os.Args); err != nil { log.Fatal(err) } } So what does the struct inside cli.Command look like? It\u0026rsquo;s a slice of Flag interfaces. allowing us to freely mix different flag types like StringFlag, BoolFlag, or IntFlag.\nFlags: []cli.Flag{ \u0026amp;cli.IntFlag{ ... }, \u0026amp;cli.BoolFlag{ ... }, }, The other struct field is Action which runs a function after parsing the flags. The function can either be inline or passed as a reference. Here is an inline example.\nAction: func(ctx context.Context, cmd *cli.Command) error { if cmd.Bool(\u0026#34;hello\u0026#34;) { hello(cmd.String(\u0026#34;name\u0026#34;), cmd.Int(\u0026#34;count\u0026#34;)) } return nil }, This is what my populated main() looks like.\nfunc main() { cmd := \u0026amp;cli.Command{ Flags: []cli.Flag{ \u0026amp;cli.BoolFlag{ Name: \u0026#34;hello\u0026#34;, Required: false, Usage: \u0026#34;Will greet a person with \u0026#39;Hello\u0026#39;\u0026#34;, }, \u0026amp;cli.BoolFlag{ Name: \u0026#34;goodbye\u0026#34;, Required: false, Usage: \u0026#34;Will tell a person \u0026#39;Goodbye\u0026#39;\u0026#34;, }, \u0026amp;cli.IntFlag{ Name: \u0026#34;count\u0026#34;, Usage: \u0026#34;How many times to invoke function\u0026#34;, Required: false, Value: 1, }, \u0026amp;cli.StringFlag{ Name: \u0026#34;name\u0026#34;, Usage: \u0026#34;Tell me your name\u0026#34;, Required: true, // \u0026lt;- throws an error and prints help statement if name is not defined }, }, Action: func(ctx context.Context, cmd *cli.Command) error { if cmd.Bool(\u0026#34;hello\u0026#34;) { hello(cmd.String(\u0026#34;name\u0026#34;), cmd.Int(\u0026#34;count\u0026#34;)) } return nil }, } if err := cmd.Run(context.Background(), os.Args); err != nil { log.Fatal(err) } } A complete working example using flags can be downloaded.\nFlags Demo When invoked without the \u0026ndash;name flag which is marked required, urfave/cli will throw an error and print the help statement.\n$ go run flag_example.go --count 2 --hello NAME: flag_example - A new cli application USAGE: flag_example OPTIONS: --hello Will greet a person with \u0026#39;Hello\u0026#39; (default: false) --goodbye Will tell a person \u0026#39;Goodbye\u0026#39; (default: false) --count int How many times to invoke function (default: 1) --name string Tell me your name --help, -h show help 2025/10/02 08:36:19 Required flag \u0026#34;name\u0026#34; not set exit status 1 Adding the \u0026ndash;name flag results in it working as expected.\n$ go run flag_example.go --count 2 --hello --name Adam Hello Adam Hello Adam urfave/cli with subcommands Subcommands are added in a similar way, except instead of using a slice of Flag, we are using a slice of Command which I described above. Instead of using \u0026ndash;hello or \u0026ndash;goodbye as a flag, this example creates the hello and goodbye subcommands.\nfunc hello(ctx context.Context, cmd *cli.Command) error { fmt.Println(\u0026#34;Hello\u0026#34;) return nil } func goodbye(ctx context.Context, cmd *cli.Command) error { fmt.Println(\u0026#34;Goodbye\u0026#34;) return nil } func main() { cmd := \u0026amp;cli.Command{ Commands: []*cli.Command{ { Name: \u0026#34;hello\u0026#34;, Usage: \u0026#34;Greets a person with \u0026#39;Hello\u0026#39;\u0026#34;, Action: hello, }, { Name: \u0026#34;goodbye\u0026#34;, Usage: \u0026#34;Tells a person \u0026#39;Goodbye\u0026#39;\u0026#34;, Action: goodbye, }, }, } if err := cmd.Run(context.Background(), os.Args); err != nil { log.Fatal(err) } } Commands Demo The autogenerated help statement when a subcommand isn\u0026rsquo;t used.\n$ go run command_example.go NAME: command_example - A new cli application USAGE: command_example [global options] [command [command options]] COMMANDS: hello Greets a person with \u0026#39;Hello\u0026#39; goodbye Tells a person \u0026#39;Goodbye\u0026#39; help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help Invoking the goodbye subcommand\n$ go run command_example.go goodbye Goodbye Combining flags and subcommands Now that we know how flags and subcommands work, lets combine them to get that openssl like experience. But this is the great part, because subcommands are a slice of Command, and we know that Command accepts a slice of Flag, we already know how this works. Simply define a Flag for each subcommand.\nHere\u0026rsquo;s an example where the hello subcommand supports a name flag and the goodbye subcommand supports a count flag.\nfunc main() { cmd := \u0026amp;cli.Command{ Commands: []*cli.Command{ { Name: \u0026#34;hello\u0026#34;, Usage: \u0026#34;Greets a person with \u0026#39;Hello\u0026#39;\u0026#34;, Action: runIt, Flags: []cli.Flag{ \u0026amp;cli.StringFlag{ Name: \u0026#34;name\u0026#34;, Usage: \u0026#34;Tell me your name\u0026#34;, Required: true, // \u0026lt;- throws an error and prints help statement if name is not defined }, }, }, { Name: \u0026#34;goodbye\u0026#34;, Usage: \u0026#34;Tells a person \u0026#39;Goodbye\u0026#39;\u0026#34;, Action: runIt, Flags: []cli.Flag{ \u0026amp;cli.IntFlag{ Name: \u0026#34;count\u0026#34;, Usage: \u0026#34;How many times to invoke function\u0026#34;, Required: false, Value: 1, }, }, }, }, } A complete example that combines flags and subcommands is available for download.\nCombination Demo Running the demo without any options\n$ go run combo_example.go NAME: combo_example - A new cli application USAGE: combo_example [global options] [command [command options]] COMMANDS: hello Greets a person with \u0026#39;Hello\u0026#39; goodbye Tells a person \u0026#39;Goodbye\u0026#39; help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help Running the demo with a subcommand missing a required flag\n$ go run combo_example.go hello NAME: combo_example hello - Greets a person with \u0026#39;Hello\u0026#39; USAGE: combo_example hello OPTIONS: --name string Tell me your name --help, -h show help 2025/10/05 09:33:19 Required flag \u0026#34;name\u0026#34; not set exit status 1 Running the demo successfully\n% go run combo_example.go hello --name Adam Hello Adam Accessing Set Values The urfave API offers several functions to access the set values, which are found in online documentation.\nThese are the functions I find myself using:\nfunc (cmd *Command) Names() []string func (cmd *Command) String(name string) string func (cmd *Command) Bool(name string) bool func (cmd *Command) Int(name string) int Let\u0026rsquo;s go back to the earlier combo example using these CLI options go run combo_example.go hello --name Adam. This is the code block used in the example.\nfunc runIt(ctx context.Context, cmd *cli.Command) error { switch cmd.Name { case \u0026#34;hello\u0026#34;: hello(cmd.String(\u0026#34;name\u0026#34;)) case \u0026#34;goodbye\u0026#34;: goodbye(cmd.Int(\u0026#34;count\u0026#34;)) default: fmt.Println(\u0026#34;Unknown command\u0026#34;) } return nil } I use cmd.Name to access the user provided subcommand. To access user provided options (flags), use cmd.String, cmd.Int, or cmd.Bool. Other variations like cmd.Int64 do exist. Check online documentation if needed.\nurfave is now myfave I started this post as a few notes for myself while learning urfave/cli, but it turned into a great reminder of how flexible the library is. It keeps command structure clear, couples help statements and actions to flags and subcommands, resulting in polished CLIs.\nIf you\u0026rsquo;re building a Go CLI and want a simple API for managing subcommands and options, give urfave/cli a try. You\u0026rsquo;ll spend more time thinking about command logic and less time manaaging subcommands or flag options.\nI can be found on Bluesky if you want to trade notes on building CLIs.\n","permalink":"https://amf3.github.io/articles/code/go/cli_args/","summary":"\u003ch2 id=\"adding-subcommands-to-go-clis\"\u003eAdding Subcommands to Go CLIs\u003c/h2\u003e\n\u003cp\u003eCommand Line Interfaces (CLIs) use subcommands and flags to enable different program features.  A\nsubcommand is a grouping of related features, and flags are options for controlling those features.  The openssl\ncommand provides a great example of subcommands and flags. \u003ccode\u003eopenssl rand -base64 8\u003c/code\u003e will generate 8 random bytes of\ndata with hexadecimal output.  The subcommand is \u0026ldquo;rand\u0026rdquo; and \u0026ldquo;-base64\u0026rdquo; is the flag.  Other openssl subcommands\nlike \u0026ldquo;s_client\u0026rdquo; or \u0026ldquo;x509\u0026rdquo;, provide different features and each has their own options.\u003c/p\u003e","title":"Go CLIs: Creating Subcommands and Flags"},{"content":"Until recently, I haven\u0026rsquo;t had many opportunities to use Go\u0026rsquo;s generics. I ran into a case where generics make sense. Best of all, this isn\u0026rsquo;t a contrived example.\nI\u0026rsquo;m working on a project and using openAPI to generate API contracts. One of the generated structs contains optional fields implemented as pointers. The only required field is Name.\nconst ( Gzip PostPailsCompression = \u0026#34;gzip\u0026#34; None PostPailsCompression = \u0026#34;none\u0026#34; ) type PostPails struct { Compression *PostPailsCompression `json:\u0026#34;compression,omitempty\u0026#34;` // MaxArchiveSize Max size (bytes) before rotating to a new archive. MaxArchiveSize *int `json:\u0026#34;max_archive_size,omitempty\u0026#34;` // Name Name of the new pail Name string `json:\u0026#34;name\u0026#34;` } I need to populate the struct with values when writing unit tests. But dealing with pointers in Go test code usually results in using temporary variables. It\u0026rsquo;s not bad, but there\u0026rsquo;s some visual noise.\ngzip := PostPailsCompression(\u0026#34;gzip\u0026#34;) size := 1000000 payload := PostPails{ Name: \u0026#34;testpail\u0026#34; Compression: \u0026amp;gzip, MaxArchiveSize: \u0026amp;size, } Implementing a helper function using generics, provides a much cleaner solution.\nThe temporary variables are no longer needed. Test code becomes much easier to read by naming the helper function ptr. func ptr[T any](v T) *T { return \u0026amp;v } func TestPostPails_CreatesDirectory(t *testing.T) { tmpStorage := t.TempDir() server := NewServer(tmpStorage) payload := PostPails{ Name: \u0026#34;testpail\u0026#34;, Compression: ptr(PostPailsCompression(\u0026#34;gzip\u0026#34;)), MaxArchiveSize: ptr(1000000), ... } Let\u0026rsquo;s discuss the ptr function.\nT is a type parameter and is a placeholder for any type. The any constraint means T can be anything and is equivalent to interface{}. Inside the function, we take a value v and return its pointer. Using generics avoids the temporary variable pattern and provides a means to write cleaner test code. The benefit becomes obvious when dealing with many optional fields.\nUntil now, generics didn\u0026rsquo;t seem to be a feature I needed. The examples I read about didn\u0026rsquo;t feel relevant. This one clicked because it solved a real issue while writing unit tests.\nAny thoughts or clever uses of Go generics? Drop me a line on Bluesky.\n","permalink":"https://amf3.github.io/articles/code/go/real_world_generics/","summary":"\u003cp\u003eUntil recently, I haven\u0026rsquo;t had many opportunities to use Go\u0026rsquo;s generics.  I ran into\na case where generics make sense.  Best of all, this isn\u0026rsquo;t a contrived example.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m working on a project and using openAPI to generate API contracts.  One of the generated\nstructs contains optional fields implemented as pointers. The only required field is Name.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003econst (\n\tGzip PostPailsCompression = \u0026#34;gzip\u0026#34;\n\tNone PostPailsCompression = \u0026#34;none\u0026#34;\n)\n\ntype PostPails struct {\n\tCompression *PostPailsCompression `json:\u0026#34;compression,omitempty\u0026#34;`\n\n\t// MaxArchiveSize Max size (bytes) before rotating to a new archive.\n\tMaxArchiveSize *int `json:\u0026#34;max_archive_size,omitempty\u0026#34;`\n\n\t// Name Name of the new pail\n\tName string `json:\u0026#34;name\u0026#34;`\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eI need to populate the struct with values when writing unit tests. But dealing with pointers in Go\ntest code usually results in using temporary variables.  It\u0026rsquo;s not bad, but there\u0026rsquo;s some visual noise.\u003c/p\u003e","title":"Go Generics: A Real World Use Case"},{"content":"OpenAPI is a specification for documenting HTTP APIs for both humans and machines to consume. As OpenAPI is a specification, it is language agnostic. OpenAPI relies on generators for translating the specification. There\u0026rsquo;s more than just documentation that\u0026rsquo;s generated. Generators also create language-specific interfaces, tooling, and contracts. In some ways the OpenAPI pattern reminds me of either protobuf with gRPC or ORM schema-first design. As a result, a declarative API is created by the tooling.\nBy the end of this post you\u0026rsquo;ll have:\nA working Go http server generated from an OpenAPI specification. A Python http client generated from the same specification and authenticates with basic auth. Insight into common OpenAPI pitfalls and how to avoid them. [openapi.yaml] ↓ +--------------+ | oapi-codegen | ---\u0026gt; [Go Server] +--------------+ ↓ +-----------------------+ | openapi-python-client | ---\u0026gt; [Python Client] +-----------------------+ If you would like to follow along, a complete code example can be downloaded and extracted into a temporary working directory.\nGenerators Because generators are consuming the specification, the OpenAPI version is determined by what the generators support.\nFor example, a popular Go generator is oapi-codegen and supports OpenAPI 3.0. Where a popular Python generator named openapi-python-client can support both OpenAPI 3.0 and 3.1 specifications.\nGenerators can be downloaded and managed as part of the languages tooling. For Go, the oapi-codegen generator is managed with Go modules and invoked with go tool oapi-codegen. With Python, creating a virtual environment, using pip install openapi-python-client, and pip freeze \u0026gt; requirements.txt will work nicely.\nOpenAPI Schema At first it wasn\u0026rsquo;t clear to me on how to get started with OpenAPI or what the benefits were. This is even after reviewing the OpenAPI schema documentation for 3.0.3.\nTo get started one needs to create a specification. A very minimal specification meeting the 3.0.x requirements is listed below. It\u0026rsquo;s not a very interesting example as endpoints in the application server aren\u0026rsquo;t defined, but it shows how minimal a specification can be that meets schema requirements.\nopenapi: \u0026#34;3.0.3\u0026#34; info: version: 1.0.0 title: My Contrived Server paths: Let\u0026rsquo;s get started by extending the simple example defining a path named /status. It will return a 200 response code with a JSON resonse.\npaths: /status: get: responses: \u0026#39;200\u0026#39;: description: Get status of the application server content: application/json: schema: $ref: \u0026#39;#/components/schemas/status\u0026#39; The JSON response is documented in a separate YAML block named components. It defines the response containing a JSON map containing the keys \u0026ldquo;state\u0026rdquo; and \u0026ldquo;message\u0026rdquo;, both of which have a string value.\ncomponents: schemas: status: type: object properties: state: type: string example: \u0026#34;GOOD\u0026#34; message: type: string example: \u0026#34;App running within parameters\u0026#34; OpenAPI supports tags, which let you group related endpoints. This example creates a data grouping and puts create_bucket in the group.\ntags: - name: data description: data manipulation endpoints paths: /create_bucket: post: tags: - data requestBody: required: true content: application/json: schema: $ref: \u0026#39;#/components/schemas/create_bucket\u0026#39; responses: \u0026#39;200\u0026#39;: description: Create a storage object The OpenAPI specification also provides a definition for authentication to the web application.\ncomponents: securitySchemes: basicAuth: type: http scheme: basic description: Endpoints protected by basic auth base64 encoded credentials. paths: /status: get: security: - basicAuth: [] responses: \u0026#39;200\u0026#39;: description: Get status of the application server content: application/json: schema: $ref: \u0026#39;#/components/schemas/status\u0026#39; Earlier I mentioned the generators will create interface files. Declarations which are considered middleware like authentication or logging are out of scope for OpenAPI. In this example, the security entries are there to document that the endpoints require basic authentication.\nGenerate Server Interfaces (Go) The server walkthrough presumes one has both Make and Go installed, and the example code (tar.gz file) has been downloaded and extracted into a temp/work directory.\nDownload the Go dependencies, including oapi-codegen, by running make tidy. Generate the server interfaces by running make server-codegen, which calls go tool oapi-codegen. Feel free to inspect the api/http.gen.go file before proceeding. You\u0026rsquo;ll see it contains an interface named ServerInterface, which has the GetStatus or PostStatus endpoints from the OpenAPI specification. http.gen.go also contains a struct named Status that was defined from components -\u0026gt; schema -\u0026gt; status.\ntype Status struct { Message string `json:\u0026#34;message\u0026#34;` State string `json:\u0026#34;state\u0026#34;` } To see the working application server, run make server-run.\nThe server has Basic Auth enabled with hardcoded credentials. The user is \u0026ldquo;alice\u0026rdquo; and the password \u0026ldquo;mySecretPW\u0026rdquo;. Curl can be used to see the response.\n% curl --basic -u alice:mySecretPW http://localhost:8080/status {\u0026#34;message\u0026#34;:\u0026#34;Initializing\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;Unknown\u0026#34;} Generate Client Interfaces (Python) This is where OpenAPI really shines. I was able to use a generator to create Python libraries to be used by the client implementation code. The walkthrough presumes a recent version of Python3 and pip are installed.\nFirst, create a virtual environment and install the openapi-python-client dependencies. This shell snippet presumes the current working directory is already hello_openapi.\n% python3 -mvenv $PWD/.venv % source $PWD/.venv/bin/activate % pip install -r requirements.txt Then run make client-codegen to build the Python client libraries located in cmd/client/my_contrived_server.\nGenerating the client was easy, but figuring out how to pass authentication took some trial and error. I eventually realized that the token is just a base64-encoded username:password string, and the prefix should be set to Basic.\nclient = AuthenticatedClient( base_url=\u0026#34;http://127.0.0.1:8080\u0026#34;, headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;}, token=\u0026#34;YWxpY2U6bXlTZWNyZXRQVw==\u0026#34;, # Token string is a base64 string containing alice:mySecretPW prefix=\u0026#34;Basic\u0026#34; ) To see the client in action, run make client-run. Also take a look at cmd/client/client.py. It only took a few lines of python code to implement what the openapi-python-client generator had created.\nGotchas \u0026amp; Lessons Learned One issue I have with OpenAPI is the illusion of simplicty. When I first started working with OpenAPI, I noticed the Status struct had keys referencing a pointer of strings which wasn\u0026rsquo;t ideal.\ntype Status struct { Message *string `json:\u0026#34;message\u0026#34;` State *string `json:\u0026#34;state\u0026#34;` } It took some fiddling with the OpenAPI specification to make the generator use strings instead of pointers to strings. Adding \u0026lsquo;required\u0026rsquo; to the schema made the generator do what I wanted.\ncomponents: status: type: object properties: state: type: string example: \u0026#34;GOOD\u0026#34; message: type: string example: \u0026#34;App within parameters\u0026#34; required: - state - message Another issue was not knowing that in Paths, GETs should have a responses entry and POSTS should have a RequestBody entry. It makes sense, but it wasn\u0026rsquo;t obvious to me when stumbling through hello-world.\nThe main takeaway? Always inspect the generated code. If something doesn’t look right, like unexpected pointers or missing method args, chances are your spec needs tweaking.\nWrapping Up Even though I hit some issues with a fairly simple example, I\u0026rsquo;m going to continue using OpenAPI specifcations. Being able to easily generate client code in a different language was a real win. And let\u0026rsquo;s not forget the free API documentation and contract definitions which comes with OpenAPI. I have a more complex OpenAPI project coming up. I\u0026rsquo;m sure I\u0026rsquo;ll have more notes (and probably more gotchas) to share. Stay tuned.\nIf you\u0026rsquo;ve had similar struggles with OpenAPI or tips for improving schema design, I’d love to hear them on Bluesky Social.\n","permalink":"https://amf3.github.io/articles/api/hello_openapi/","summary":"\u003cp\u003eOpenAPI is a specification for documenting HTTP APIs for both humans and machines to consume.  As OpenAPI is a specification,\nit is language agnostic. OpenAPI relies on generators for translating the specification.  There\u0026rsquo;s more\nthan just documentation that\u0026rsquo;s generated. Generators also create language-specific interfaces, tooling, and contracts.  In some\nways the OpenAPI pattern reminds me of either protobuf with gRPC or ORM schema-first design.  As a result, a declarative API is\ncreated by the tooling.\u003c/p\u003e","title":"OpenAPI in Practice: Go Server + Python Client from Spec"},{"content":"With Docker, it’s not always obvious what storage options exist beyond the built-in local volume driver or a traditional bind mount. Exploring Docker volume drivers often turns up archived GitHub repositories or commercially backed plugins tied to specific cloud storage products. The volume ecosystem is especially limited for on-premise storage, and many plugins require more privileges than you\u0026rsquo;d expect.\nIn this post, I’ll cover how Docker handles volume storage under the hood. I’ll also walkthrough how to create a volume plugin that interacts with remote storage without needing CAP_SYS_ADMIN privileges.\nDocker Storage Overview Graph Drivers (also known as Storage Drivers) manage image and container layers. Examples include, overlay2, zfs, or btrfs. Volume Drivers manange named volumes and allow data to persist outside of the container lifecycle. Plugins for Volume Drivers are usually installed as special containers using the docker plugin command. Plugin containers run in their own namespaces and don\u0026rsquo;t behave like normal containers. However, if the plugin includes a shell, it\u0026rsquo;s possible to enter the namespace using runc.\nubuntu@docker-dev:~/work$ sudo runc --root /run/docker/runtime-runc/plugins.moby list ID PID STATUS BUNDLE CREATED OWNER bae595da4deac656921a48f4b3d854992c692777f46db891934310ae863746c1 24503 running /run/containerd/io.containerd.runtime.v2.task/plugins.moby/bae595da4deac656921a48f4b3d854992c692777f46db891934310ae863746c1 2025-06-27T03:55:42.927740463Z root ubuntu@docker-dev:~/work$ sudo runc --root /run/docker/runtime-runc/plugins.moby exec -t bae595da4deac656921a48f4b3d854992c692777f46db891934310ae863746c1 /bin/bash root@docker-dev:/# ls -lh /myplugin -rwxr-xr-x 1 root root 5.0M Jun 24 23:24 /myplugin The Docker daemon communicates with all plugins over HTTP, using either a Unix socket or a TCP socket.\nWhat\u0026rsquo;s Missing While the interface is simple, expectations around what happens after a volume is mounted are not. Most plugins end up mounting a remote filesystem like NFS or CIFS, manipulating files as root, or interacting with device nodes like /dev/fuse, all requiring elevated privileges.\nTo summarize:\nIt’s difficult to find unprivileged plugins in the Docker ecosystem. Using virtual filesystems (like GlusterFS, SSHFS, or S3FS) requires FUSE, and FUSE needs CAP_SYS_ADMIN. A local caching layer for remote storage is not baked into the Docker volume plugin interface. The DIY Approach Let’s say we want a volume plugin that “mounts” a remote volume by downloading files from a remote server.\nHere’s a basic outline of how it might work:\nImplement a volume plugin that exposes the Volume Plugin API to the Docker daemon. Create volume, populates volume metadata needed to later identify the volume within Docker. (The API allows actual filesystem setup to be deferred until Mount.) Mount volume, will fetch data from the remote server, extract it to a known local path, and return that path to Docker. Unmount volume, either cleans up the local path or repackages and uploads any changes back to the remote server. This model avoids the need for root privileges, since it doesn\u0026rsquo;t touch /dev, doesn\u0026rsquo;t rely on FUSE, and doesn’t call mount(2).\nMy Interest in Docker Plugins I like the simplicity of Docker compared to larger orchestration platforms, but I want more from its storage offerings. When I started looking at existing volume plugins, a few things pushed me toward writing my own.\nMany plugins require root-level privileges. I found that avoiding FUSE or skipping filesystems that depend on kernel modules could reduce or eliminate this requirement.\nAnother issue, most volume plugins on GitHub have been archived by their maintainers. I get it, people move on and the Docker community isn\u0026rsquo;t as large as it once was. That said, I found the developer tooling for writing plugins to be a bit clunky. My hope is this post will help fill in gaps and show a practical path forward for building a volume plugin.\nFinally, most active (not archived) volume plugins I find are designed for cloud storage services. There\u0026rsquo;s a lack of unprivileged lightweight volume plugins and I think there\u0026rsquo;s a place for something simplier.\nStreaming read-only config bundles CI/CD ephemeral volumes Lazy-loading assets over HTTP These ideas are viable with the current plugin API, just unexplored.\nA simple plugin After digging into Docker plugins and exploring the current state of the ecosystem, I decided to build a simple plugin to see how far I could get with minimal privileges and lightweight tooling. I have to admit that the process was both educational and a little frustrating.\nWhat Didn\u0026rsquo;t Go Smoothly Docker has a clean CLI and a solid container runtime, but plugin development comes with its share of friction:\nThe development loop is slow. Building, loading, and enabling a plugin requires several manual steps. Debugging inside the plugin container via runc (instead of docker plugin) isn’t intuitive. Plugin files need to follow a specific directory structure, and you must include an exported container root filesystem in a subdirectory before the plugin can be built. The Docker daemon uses a socket path that includes the container ID which is a dynamic value. This caused the daemon to time out when connecting to the plugin until I manually fixed the path. Eventually, I discovered the Go plugin SDK, which handled this more reliably. Build Steps Here’s a high-level overview of the development loop when creating a Docker volume plugin:\nWrite the plugin code. Create a Docker image that contains the plugin. Create a throwaway container from that image. Extract the root filesystem from the container using docker export, and untar it into a directory named rootfs. Finally, run docker plugin create to assemble the plugin from the rootfs and a config.json file. Fortunately, someone wrapped these steps in a Makefile which can be used as a starting point. Here\u0026rsquo;s a snippet from the docker-volume-sshfs repo.\nrootfs: @echo \u0026#34;### docker build: rootfs image with docker-volume-sshfs\u0026#34; @docker build -q -t ${PLUGIN_NAME}:rootfs . @echo \u0026#34;### create rootfs directory in ./plugin/rootfs\u0026#34; @mkdir -p ./plugin/rootfs @docker create --name tmp ${PLUGIN_NAME}:rootfs @docker export tmp | tar -x -C ./plugin/rootfs @echo \u0026#34;### copy config.json to ./plugin/\u0026#34; @cp config.json ./plugin/ @docker rm -vf tmp create: @echo \u0026#34;### remove existing plugin ${PLUGIN_NAME}:${PLUGIN_TAG} if exists\u0026#34; @docker plugin rm -f ${PLUGIN_NAME}:${PLUGIN_TAG} || true @echo \u0026#34;### create new plugin ${PLUGIN_NAME}:${PLUGIN_TAG} from ./plugin\u0026#34; @docker plugin create ${PLUGIN_NAME}:${PLUGIN_TAG} ./plugin You’ll also need a config.json file (docs) that defines the plugin’s name, entrypoint, socket permissions, and other settings. This file goes alongside the rootfs directory when building the plugin.\nPlugin SDK and API documentation The go-plugins-helpers SDK was a big help when building my plugin, though it\u0026rsquo;s not well advertised. It provides an interface with method definitions for handling the HTTP communication between the custom plugin and the Docker Daemon.\nWhile plugins can technically be written in any language (since the API is just HTTP), this Go SDK was the only official helper library I found. That said, using it is optional. I came across several projects like rclone and SeaweedFS that implement the plugin protocol without relying on the SDK.\nDocker’s documentation is spread across a few key pages. The two most useful I found were:\nThe Plugin API reference describes the HTTP interface and includes example request and response payloads. The Volume plugin overview includes sections on creating, installing, developing, and debugging plugins. Code Highlights The plugin I built is intentionaly minimal. When a container calls mount on the volume, the function creates a hello.txt file in the volume directory.\nIt simulates downloading data from remote storage while keeping things simple:\nfunc (d *myDriver) Mount(req *volume.MountRequest) (*volume.MountResponse, error) { volPath := filepath.Join(pluginRoot, req.Name) // Write a hello.txt file helloFile := filepath.Join(volPath, \u0026#34;hello.txt\u0026#34;) err := os.WriteFile(helloFile, []byte(\u0026#34;Hello, world!\\n\u0026#34;), 0644) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to write hello.txt: %w\u0026#34;, err) } log.Printf(\u0026#34;Mount volume: %s -\u0026gt; %s\u0026#34;, req.Name, volPath) return \u0026amp;volume.MountResponse{Mountpoint: volPath}, nil } Before a container exits, unmount is called. This function deletes the file from the volume, demonstrating that the basic lifecycle works with user level permissions. This step could be used to sync local changes back to remote storage in production code.\nfunc (d *myDriver) Unmount(req *volume.UnmountRequest) error { volPath := filepath.Join(pluginRoot, req.Name) helloFile := filepath.Join(volPath, \u0026#34;hello.txt\u0026#34;) // Simulate cleanup if err := os.Remove(helloFile); err != nil \u0026amp;\u0026amp; !os.IsNotExist(err) { return fmt.Errorf(\u0026#34;unmount cleanup error: %w\u0026#34;, err) } log.Printf(\u0026#34;Unmount volume: %s (removed hello.txt)\u0026#34;, req.Name) return nil } Build WalkThrough Now for the good part. A full walkthrough of how I built and tested the custom plugin. Here are the files involved:\ngo.mod go.sum myplugin.go Dockerfile config.json Build the Go code and create a plugin image $ docker build -t rootfsimage . [+] Building 6.1s (13/13) FINISHED docker:default =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 461B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/ubuntu:oracular 0.9s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 32.89kB 0.0s =\u0026gt; [builder 1/6] FROM docker.io/library/ubuntu:oracular@sha256:707879280c0bbfe6cbeb3ae1a85b564ea2356b5310a122c225b92cb3d1ed131b 0.0s =\u0026gt; CACHED [builder 2/6] RUN apt-get update \u0026amp;\u0026amp; apt-get install golang-go ca-certificates -y 0.0s =\u0026gt; [builder 3/6] COPY . /build 0.0s =\u0026gt; [builder 4/6] WORKDIR /build 0.0s =\u0026gt; [builder 5/6] RUN go mod tidy 0.7s =\u0026gt; [builder 6/6] RUN CGO_ENABLED=0 go build -ldflags=\u0026#34;-s -w -extldflags \u0026#34;-static\u0026#34;\u0026#34; -tags netgo,osusergo -o myplugin 4.3s =\u0026gt; CACHED [stage-1 2/3] RUN mkdir -p /run/docker/plugins /var/lib/myplugin/volumes 0.0s =\u0026gt; [stage-1 3/3] COPY --from=builder /build/myplugin /myplugin 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:a3d2dee4d6cb8112f538a16056dc42b9761bda43f7f279b2a1f202a7a8e5f8ae 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/rootfsimage Create a container from the rootfs image and export the container’s root filesystem to a tar file. $ id=$(docker create rootfsimage true) $ echo $id 13cf6219737999bd54f7fc2537bc1218f42d9c45d92f79c4726c1422e81b348e $ sudo docker export \u0026#34;$id\u0026#34; -o rootfs.tar $ ls -l rootfs.tar -rw------- 1 ubuntu ubuntu 110620160 Jun 26 23:07 rootfs.tar Docker plugin tooling expects a rootfs directory and a config.json file in the current directory. The config isn’t part of the exported filesystem, so it\u0026rsquo;s provided separately: $ cp ~/Downloads/config.json . $ sudo tar -xf ./rootfs.tar -C ./rootfs/ $ ls -l total 8 -rw-r--r-- 1 root root 183 Jun 23 15:26 config.json drwxr-xr-x 17 root root 4096 Jun 24 16:28 rootfs $ ls rootfs bin boot dev etc home lib media mnt myplugin opt proc root run sbin srv sys tmp usr var Create and enable the plugin. $ sudo docker plugin create myplugin . myplugin $ docker plugin enable myplugin:latest myplugin:latest $ docker plugin ls ID NAME DESCRIPTION ENABLED d5f63b80f0b0 myplugin:latest Example HTTP-backed volume plugin true Create a volume with the plugin and inspect it. $ docker volume create -d myplugin:latest abc123 abc123 $ docker volume ls DRIVER VOLUME NAME myplugin:latest abc123 $ docker volume inspect abc123 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;0001-01-01T00:00:00Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;myplugin:latest\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/myplugin/volumes/abc123\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;abc123\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] Mount the volume with a container. $ docker run -it --rm -v abc123:/mnt alpine / # ls -l /mnt total 4 -rw-r--r-- 1 root root 14 Jun 27 06:17 hello.txt / # cat /mnt/hello.txt Hello, world! Additional information for Docker events can be found inside systemd logs. $ journalctl -u docker.service | tail -3 Jun 26 23:17:55 docker-dev dockerd[7976]: time=\u0026#34;2025-06-26T23:17:55-07:00\u0026#34; level=error msg=\u0026#34;2025/06/27 06:17:55 Mount volume: abc123 -\u0026gt; /var/lib/myplugin/volumes/abc123\u0026#34; plugin=d5f63b80f0b091582e00bfed7bd6d33e885fef876e151558a37ae5eaf05e5443 Jun 26 23:18:04 docker-dev dockerd[7976]: time=\u0026#34;2025-06-26T23:18:04.543532903-07:00\u0026#34; level=info msg=\u0026#34;ignoring event\u0026#34; container=0e6fdf10db3da9c9b5bc04bbe6abac6b23225578e796e89fe859c1d17fc57f0f module=libcontainerd namespace=moby topic=/tasks/delete type=\u0026#34;*events.TaskDelete\u0026#34; Jun 26 23:18:04 docker-dev dockerd[7976]: time=\u0026#34;2025-06-26T23:18:04-07:00\u0026#34; level=error msg=\u0026#34;2025/06/27 06:18:04 Unmount volume: abc123 (removed hello.txt)\u0026#34; plugin=d5f63b80f0b091582e00bfed7bd6d33e885fef876e151558a37ae5eaf05e5443 Closing I\u0026rsquo;m excited about learning how to create plugins for Docker and turning my notes into this post.\nI made a previous post on using tar archives as an object store and I recently posted about compression ratios on Bluesky social. Stay tuned to find out where these posts are headed.\nI was curious how snappy compared to lz4 and wrote a go program to find out. Snappy seems to be a bit better with both compressed data output and resource usage. Input is the first 1,000,000 numbers of pi.\n\u0026mdash; Adam Faris (@af9.us) 2025-06-18T07:40:31.631Z If you\u0026rsquo;ver ever built a Docker plugin or struggled with Docker storage, I\u0026rsquo;d like to hear about it. Any questions or ideas, reach out and leave a comment.\nUntil next time, keep your volumes clean and your containers stateless.\n","permalink":"https://amf3.github.io/articles/storage/docker_volumes/","summary":"\u003cp\u003eWith Docker, it’s not always obvious what storage options exist beyond the built-in \u003cstrong\u003elocal\u003c/strong\u003e volume driver or a traditional \u003cstrong\u003ebind mount\u003c/strong\u003e.\nExploring Docker volume drivers often turns up archived GitHub repositories or commercially backed plugins tied to specific cloud storage products. The volume\necosystem is especially limited for on-premise storage, and many plugins require more privileges than you\u0026rsquo;d expect.\u003c/p\u003e\n\u003cp\u003eIn this post, I’ll cover how Docker handles volume storage under the hood. I’ll also walkthrough how to create a volume plugin that interacts with remote\nstorage without needing CAP_SYS_ADMIN privileges.\u003c/p\u003e","title":"DIY Docker Volume Drivers: What's Missing"},{"content":"I\u0026rsquo;m looking at how object storage systems manage data on disk. Especially the idea of using append only archives with an index for fast retrieveal. While reading Facebook\u0026rsquo;s Haystack design, I noticed similarities to the tar file format and the potential to implement something similar at the local scale.\nHaystack Overview There are several components mentioned in the original Haystack paper, but at the core is the Haystack Store, where end user image files are physically kept. Instead of writing files directly to the filesystem, images are appended to a large file called a volume, which acts as an append-only archive. Each volume is typically capped at around 100 GB and is aligned to 8-byte offsets. Image files within this volume are referred to as needles.\nA volume begins with a superblock (the paper doesn’t describe this in detail), followed by the header for the first needle (file). Each needle within the volume has its own header, containing metadata like file size, checksums, and flags. The flags field includes a bit to indicate deletion status.\nSince the volume is append-only, deletions don’t reclaim space—they\u0026rsquo;re simply marked as deleted in the needle’s header. A background process can later compact the volume if needed. To keep track of where each needle is within the file, an in-memory index maps file IDs to byte offsets.\nWhen a read request comes in, the Haystack Store performs a direct seek to the needle’s offset, verifies the flags to check if it\u0026rsquo;s deleted, and returns the data if is not tombstoned. Deletions update both the in-memory index and the needle’s header to mark the entry as removed.\nThis model provides two big wins:\nStorage efficiency: Small files, like 1 KB thumbnails, don’t waste space the way they would on a traditional filesystem with 4 KB blocks. Instead of allocating a full block per file, they\u0026rsquo;re packed into a shared archive. Fast retrieval: There’s no need to scan directory structures or fetch inode metadata. With an open file handle to the volume and an in-memory index, reads are just a seek and a read. Tar Storage The tape archive format (tar) is surprisingly similar to the Haystack volume. While tar files don’t implement a superblock, each file entry is stored at a 512-byte aligned offset, and each file includes its own metadata header. This format allows us to calculate the offset of each file within the archive.\nHere’s a hexdump of a simple test.tar archive containing two files: a.txt and b.txt.\nIn this example:\na.txt contains the string \u0026ldquo;foo\\n\u0026rdquo;, and b.txt contains \u0026ldquo;bar\\n\u0026rdquo;. Each file is preceded by a 512-byte header containing metadata like filename, permissions, and ownership. Since a.txt is only 4 bytes long, it’s followed by null padding to align the next file (b.txt) to the 512-byte boundary. The offset for b.txt starts at 0x400 (1024 bytes), which is a clean 512-byte multiple. Although tar uses more padding than Haystack (which aligns to 8-byte offsets), its fixed alignment still enables efficient offset tracking and data retrieval. Once the byte offsets of each file are known, accessing a file is just a matter of seeking to the right position and reading the data.\nTar also provides nice recovery properties:\nAn index of offsets can always be created by reading the tar file and recording the header positions as offsets. Because this is a standard tar file, common tools like tar and cpio can extract the objects directly without the need for custom tooling. Python Prototype Tar archives are typically read sequentially from start to finish. But if we build an index of byte offsets, we can enable random access to individual files. Let’s explore this with a prototype in Python using the test.tar archive shown in the earlier hexdump. A copy of the archive can be downloaded from here.\nWe have two options for building this prototype:\nThe hard way, by manually parsing byte offsets directly from the tar header. The batteries-included way, using Python’s built-in tarfile module to extract header information cleanly. If you’re curious, fields and byte-offsets within file headers are listed in GNU\u0026rsquo;s tar header definition.\nHere’s an example of the batteries-included approach using the tarfile module. I’ll scan the archive, read each file’s size and data offset, and store that in a dictionary:\n#!/usr/bin/env python3 import math import tarfile from collections import defaultdict from typing import Dict ARCHIVE_FILE = \u0026#34;test.tar\u0026#34; BYTE_ALIGNMENT = 512 def read_header(archive: str) -\u0026gt; Dict: entities = defaultdict(list) header_offset = 0 with open(archive, \u0026#39;rb\u0026#39;) as f: while True: f.seek(header_offset) header = f.read(BYTE_ALIGNMENT) if header == b\u0026#39;\\0\u0026#39; * BYTE_ALIGNMENT: break # End of archive, trailer will contain two 512-byte blocks of zeros try: tarinfo = tarfile.TarInfo.frombuf(header, encoding=\u0026#34;utf-8\u0026#34;, errors=\u0026#34;surrogateescape\u0026#34;) file_name = tarinfo.name file_size = tarinfo.size data_offset = header_offset + BYTE_ALIGNMENT entities[file_name].append([file_size, data_offset]) except Exception as e: print(f\u0026#34;Error parsing header at offset {header_offset}: {e}\u0026#34;) break padding = math.ceil(file_size / BYTE_ALIGNMENT) * BYTE_ALIGNMENT header_offset += BYTE_ALIGNMENT + padding return entities tar_data = read_header(ARCHIVE_FILE) for file_name, attributes in tar_data.items(): for attribute in attributes: print(f\u0026#34;filename: {file_name:\u0026lt;10} attributes: file_size: {attribute[0]:\u0026lt;6} data_offset: {attribute[1]:\u0026lt;6}\u0026#34;) Example output.\n% python offsets.py filename: a.txt attributes: file_size: 4 data_offset: 512 filename: a.txt attributes: file_size: 13 data_offset: 2560 filename: b.txt attributes: file_size: 4 data_offset: 1536 Notice that a.txt appears twice, each with a different file size and offset. This is expected. It’s possible to append files to a tar archive using tar -rf. When a file is re-added, it becomes the newer version.\nIn our example archive file, a.txt was modified and appended, producing two versions in the archive. Traditional tar extraction reads from the beginning and overwrites earlier entries as it encounters newer ones. But by having an index of offsets, I can seek directly to either version and extract it manually.\nHere’s a helper function to extract a specific version of a file:\ndef extract_file(archive: str, file_name: str, offset: int, read_bytes: int): try: with open(archive, \u0026#39;rb\u0026#39;) as f: f.seek(offset) data = f.read(read_bytes) with open(f\u0026#34;{file_name}@{offset:08x}\u0026#34;, \u0026#39;wb\u0026#39;) as out: out.write(data) except Exception as e: print(f\u0026#34;Error extracting {file_name} at offset: {offset:08x}\u0026#34;) Add the following lines in main to extract both versions of a.txt:\nextract_file(ARCHIVE_FILE, \u0026#34;a.txt\u0026#34;, 512, 4) extract_file(ARCHIVE_FILE, \u0026#34;a.txt\u0026#34;, 2560, 13) And the result:\n% ls -latr a.txt@* -rw-r--r--@ 1 adam staff 4 Jun 6 22:07 a.txt@00000200 -rw-r--r--@ 1 adam staff 13 Jun 6 22:07 a.txt@00000a00 % cat a.txt@00000200 foo % cat a.txt@00000a00 foo fooooooo This demonstrates simple object versioning using nothing more than tar’s existing append behavior and a bit of byte-level introspection.\nTrade-Offs and Limitations As with Haystack, there\u0026rsquo;s not an efficient way to delete content from a tar archive without rewriting the entire file. Instead, deletion requires marking entries as removed in the offsets database. Unlike Haystack which has explicit flags in its header, tar headers offer no such field. Meaning if we lose the index, we can no longer distinguish active content from deleted entries by scanning the archive.\nThe data removal limitation also contributes to archive fragmentation. Until a process rewrites the archive to remove tombstoned data, deleted files remain in place, consuming storage.\nAnother trade-off lies in tar\u0026rsquo;s alignment strategy, both headers and data are aligned to 512-byte blocks. In typical usage, tar archives are compressed, which minimizes the overhead of null padding. But for this design to support random access, the archive must remain uncompressed. Filesystems like ZFS and Btrfs can apply transparent compression at the block level, but relying on underlying filesystem isn\u0026rsquo;t ideal for portability. Haystack uses 8-byte alignment, which results in less padding and more efficient use of space.\nAlso worth noting, my prototype doesn’t implement any kind of write locking. If this were used in a concurrent setting like a web application storing assets, appends would require locking the archive to prevent corruption.\nFuture Opportunities Sharding across multiple archive files per bucket (directory) would be one enhancement. It would allow for round-robin writes with multiple appenders, improving concurrency. Using multiple archive files per bucket also provides a mechanism to cap archive file sizes.\nA mechanism for tombstoning files within an archive is also needed. As seen in the earlier hexdump, it might be possible to repurpose an existing header field to mark content as deleted. This would allow the offsets database to be reconstructed later, even after a crash or loss of metadata. Another idea is to write custom metadata into the unused space within the 512-byte header block. Whether this breaks compatibility with standard tar utilities remains an open question.\nCompression and encryption are also worth exploring. Because the prototype seeks directly to file offsets and reads raw byte ranges, it’s feasible to compress file content before appending it to the archive. Retrieval would involve decompressing on the fly after seeking to the file location within the archive. Similarly, data-at-rest encryption could be supported by encrypting file contents during the write path and decrypting during reads. This allows per-object confidentiality without relying on full-disk encryption or underlying filesystem support.\nFinal Thoughts It\u0026rsquo;s oddly satisfying to bend old standards to new purposes, like using the tar format as the basis of an object store. Putting this post together has been a reminder on the types of challenges distributed file systems create when separating metadata from the data. Simple things like marking a file as deleted become complicated.\nLet me know if this topic is interesting or you have follow-up suggestions. I can be reached at Bluesky.\n","permalink":"https://amf3.github.io/articles/storage/tar_objectstore/","summary":"\u003cp\u003eI\u0026rsquo;m looking at how object storage systems manage data on disk. Especially the idea of using append only archives with an index for fast retrieveal.  While reading\nFacebook\u0026rsquo;s Haystack design, I noticed similarities to the tar file format and the potential to implement something similar at the local scale.\u003c/p\u003e\n\u003ch2 id=\"haystack-overview\"\u003eHaystack Overview\u003c/h2\u003e\n\u003cp\u003eThere are several components mentioned in the original \u003ca href=\"https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf\"\u003eHaystack paper\u003c/a\u003e, but at\nthe core is the Haystack Store, where end user image files are physically kept. Instead of writing files directly to the filesystem, images are appended\nto a large file called a \u003cstrong\u003evolume\u003c/strong\u003e, which acts as an append-only archive. Each volume is typically capped at around 100 GB and is aligned to 8-byte\noffsets.  Image files within this volume are referred to as \u003cstrong\u003eneedles\u003c/strong\u003e.\u003c/p\u003e","title":"Using Tar Files as Object Store Storage"},{"content":"UTM and Multipass are great apps for virtualization on macOS.\nBut I wanted a lighter-weight approach by invoking QEMU directly. Which meant I needed to understand how QEMU\u0026rsquo;s networking options interact with the vmnet virtualization API on macOS.\nThis becomes especially important when dealing with VM-to-VM connections, network isolation, and bridging on macOS.\nIn this post, I\u0026rsquo;ll walk through creating a simple QEMU-based networking lab.\nSet up RouterOS and Alpine Linux VMs using QEMU on macOS Connect VMs with Apple\u0026rsquo;s Hypervisor vmnet networking APIs Use unified logging to troubleshoot QEMU network issues on macOS Lab Setup Overview The network diagram shows the network topology used in this lab. Both VMs run on on the same macOS host and connected to virtual network interfaces using QEMU\u0026rsquo;s support for Apple\u0026rsquo;s vmnet virtualization API.\nThe RouterOS VM has two virtual network interfaces, which allows it to route traffic between the Alpine Linux VM and the physical local area network.\nQEMU Networking on macOS Let\u0026rsquo;s review the vmnet.shared and vmnet.host labels in the Network1 and Network2 boxes.\nvmnet.shared: Allows traffic from the VM to reach the Internet using a built-in network address translation (NAT) feature. This is similar to how UTM’s \u0026ldquo;shared network\u0026rdquo; mode works.\nvmnet.host: Traffic can only reach the macOS host and other VMs on the same host-mode network. This mode does not provide Internet access.\nSince the Alpine Linux VM is only connected to the vmnet.host network, and that network cannot reach the Internet, we know the RouterOS VM must be acting as the gateway. It routes traffic between Network2 (host-only) and Network1 (shared). You can confirm this by watching interface packet counts on RouterOS.\nTriple NAT! As a fun side note, traffic from the Alpine VM to the Internet passes through three layers of NAT:\nRouterOS VM NAT: Alpine’s traffic is translated as it passes through RouterOS (ether2 → ether1). macOS vmnet NAT: vmnet0 (shared mode) applies another layer of NAT as it exits to the host’s physical LAN. Physical Router NAT: Finally, the home router applies NAT before sending packets to the Internet. Other QEMU Network Backends To see a complete list of network backends supported by QEMU:\n% qemu-system-x86_64 -netdev help Available netdev backend types: socket stream dgram hubport tap user vde bridge vhost-user vmnet-host vmnet-shared A few notes about QEMU network backends:\nsocket and user: Built into QEMU and don’t require elevated privileges. Great for quick VMs, but they don’t behave like traditional network bridges. You can’t easily interconnect multiple VMs.\ntap: Closer to a traditional bridged network and supports full traffic pass-through. However, it requires setup outside of QEMU and isn’t available on macOS, since tap interfaces depend on kernel extensions (which Apple no longer supports).\nvmnet: The backend is macOS-native and works out of the box with Apple’s Hypervisor Framework. It lets QEMU manage the bridge interfaces directly so no extra tooling is needed. Win!\nCreating the VMs RouterOS VM RouterOS \u0026ldquo;Cloud Hosted Router\u0026rdquo; (CHR) is a commercial product with a \u0026ldquo;free to use\u0026rdquo; license that limits upload speed to 1 Mbps. While a paid license is available to remove the upload limit, the restriction doesn\u0026rsquo;t prevent me from validating changes before deploying them to physical networks.\nOne can download the CHR image from MikroTik’s download page. I used the stable 7.x version and chose the Raw disk image — which is x86 (not ARM).\n💡 In hindsight, the ARM image might be more appropriate for Apple Silicon, but the x86 image works fine.\nFirst, convert the raw image to qcow2 format. This allows snapshotting the VM, making it easy to roll back from bad config changes.\n% qemu-img convert -f raw -O qcow2 chr-7.18.2.img chr-7.18.2.qcow2 % qemu-img snapshot -c original_image chr-7.18.2.qcow2 % qemu-img info chr-7.18.2.qcow2 image: chr-7.18.2.qcow2 file format: qcow2 virtual size: 128 MiB (134217728 bytes) disk size: 44.2 MiB cluster_size: 65536 Snapshot list: ID TAG VM_SIZE DATE VM_CLOCK ICOUNT 1 original_image 0 B 2025-05-08 22:40:36 0000:00:00.000 0 Format specific information: compat: 1.1 compression type: zlib lazy refcounts: false refcount bits: 16 corrupt: false extended l2: false Child node \u0026#39;/file\u0026#39;: filename: chr-7.18.2.qcow2 protocol type: file file length: 44.2 MiB (46333952 bytes) disk size: 44.2 MiB Now, start the RouterOS VM and create the two virtual networks with QEMU.\nsudo qemu-system-x86_64 -m 2048 -smp cpus=4 -serial mon:stdio \\ -device virtio-scsi-pci,id=scsi0 \\ -drive file=./chr-7.18.2.qcow2,if=none,format=qcow2,discard=unmap,id=hda \\ -device scsi-hd,drive=hda,bus=scsi0.0 \\ -device virtio-net-pci,netdev=net1 \\ -netdev vmnet-shared,id=net1,start-address=172.16.0.1,end-address=172.31.255.254,subnet-mask=255.240.0.0 \\ -device virtio-net-pci,netdev=net2 \\ -netdev vmnet-host,id=net2,start-address=192.168.2.1,end-address=192.168.2.254,subnet-mask=255.255.255.0,net-uuid=154780B0-F499-4968-9B20-E58C02FDF5FB Uses sudo to create vmnet interfaces (required on macOS). Allocates 2 GB of RAM and 4 vCPUs. Opens a serial console in the terminal (handy for copy/paste). Attaches two network devices: vmnet-shared for simulated external Internet. vmnet-host for internal traffic (private LAN). IP ranges must follow RFC 1918 allocation. Using net-uuid disables the macOS DHCP server for the vmnet-host network. Required as we want the RouterOS VM to respond with DHCP replies for vmnet-host traffic. Generate the UUID with /usr/bin/uuidgen. Once RouterOS boots, log in with username admin and press Enter for a blank password. You’ll be prompted to set a new one.\nTo list interfaces:\n[admin@MikroTik] \u0026gt; /interface print Flags: R - RUNNING Columns: NAME, TYPE, ACTUAL-MTU, MAC-ADDRESS # NAME TYPE ACTUAL-MTU MAC-ADDRESS 0 R ether1 ether 1500 52:54:00:12:34:56 1 R ether2 ether 1500 52:54:00:12:34:57 2 R lo loopback 65536 00:00:00:00:00:00 To check assigned IPs:\n[admin@MikroTik] \u0026gt; /ip/address print Flags: D - DYNAMIC Columns: ADDRESS, NETWORK, INTERFACE # ADDRESS NETWORK INTERFACE 0 D 172.16.0.2/12 172.16.0.0 ether1 Only one IP is listed — why? The vmnet-shared interface (ether1) has DHCP enabled by Apple’s Hypervisor framework. RouterOS sends a DHCP request and gets an IP, similar to how a home router works. Meanwhile, vmnet-host has DHCP disabled, so we must assign a static IP to ether2 on the router.\nMinimal Configuration Steps Here are the minimum configuration steps to route traffic:\nassign a static IP on ether2 create a dhcpd server enable NAT [admin@MikroTik] \u0026gt; /ip address add address=192.168.2.1/24 interface=ether2 network=192.168.2.0 [admin@MikroTik] \u0026gt; /ip pool add name=dhcp ranges=192.168.2.50-192.168.2.100 [admin@MikroTik] \u0026gt; /ip dhcp-server add address-pool=dhcp interface=ether2 lease-time=1h name=defconf [admin@MikroTik] \u0026gt; /ip dhcp-server network add address=192.168.2.0/24 comment=defconf dns-server=172.16.0.1,1.1.1.1 gateway=192.168.2.1 [admin@MikroTik] \u0026gt; /ip firewall nat add action=masquerade chain=srcnat out-interface=ether1 ⚠️ The example does not set any firewall rules. Use it as a starting point only.\nTo gracefully shutdown the router\n[admin@MikroTik] \u0026gt; /system shutdown Answer y when prompted. Or, leave the router running. It will be used again shortly.\nAlpine Linux VM An Alpine ISO needs to be downloaded and installed onto a virtual hard disk. I recommend using the user network mentioned earlier for the install as additional packages will need to be downloaded from the Internet. The standard x86_64 image can be retrieved from the Alpine Linux downloads page.\nCreate a disk image to install the OS to with the qemu-img command. The options will use the qcow2 format with a max size of 2GB.\n% qemu-img create -f qcow2 alpine_disk.qcow2 2G Next step is to start a VM that boots from the Alpine ISO and connects to the Internet with the user network. Because versions change, be sure to replace the ISO filename in the -cdrom option with the one that was downloaded.\n% qemu-system-x86_64 -m 2048 -smp cpus=4 -serial stdio \\ -boot once=d \\ -cdrom ./alpine-standard-3.21.2-x86_64.iso \\ -hda ./alpine_disk.qcow2 \\ -net nic,model=virtio -net user Once the VM has started, login as \u0026ldquo;root\u0026rdquo; and hit Enter for the empty password. Next run setup-alpine and follow the prompts. Here are suggested answers to some of the prompts:\nSelect dhcp for eth0. Choose chrony as the network time server. Accept the default of 1 when asked which \u0026ldquo;apk-mirror\u0026rdquo; to use. When prompted about the install disk, select sda. Answer sys to the \u0026ldquo;how would you like to use it\u0026rdquo; question. When the installation script is complete, type reboot and use the new root password set during the install. With the -boot once=d option, the VM will skip the ISO and boot directly from the newly installed virtual disk.\nLog in as root and install the dig and curl commands.\n# apk add bind-tools curl ca-certificates When the package install has completed, gracefully shutdown the VM with poweroff command.\nTesting the NAT Setup Check that the RouterOS VM is still running in the other terminal. It\u0026rsquo;s acting as the NAT gateway for the Alpine VM and must be active for Internet access to work. Then connect the new Alpine Linux VM to Network2 (vmnet-host) with this QEMU command.\n% sudo qemu-system-x86_64 -m 2048 -smp cpus=4 -serial mon:stdio \\ -boot c \\ -hda alpine_disk.qcow2 \\ -device virtio-net-pci,netdev=net2 \\ -netdev vmnet-host,id=net2,start-address=192.168.2.1,end-address=192.168.2.254,subnet-mask=255.255.255.0,net-uuid=154780B0-F499-4968-9B20-E58C02FDF5FB Log into the Alpine VM and verify it can reach the Internet.\nmyvm:~$ ip addr show eth0 # Confirm the IP is in the 192.168.2.x network range ... inet 192.168.2.100/24 scope global eth0 ... myvm:~$ ip route show # Confirm the default route is 192.168.2.1 default via 192.168.2.1 dev eth0 metric 202 myvm:~$ cat /etc/resolv.conf # Confirm the DNS servers were set nameserver 172.16.0.1 nameserver 1.1.1.1 myvm:~$ ping -qc 3 1.1.1.1 # test ping to 1.1.1.1 on the Internet PING 1.1.1.1 (1.1.1.1): 56 data bytes --- 1.1.1.1 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss ... myvm:~$ dig @172.16.0.1 -t a +short www.github.com # test vmnet name resolution works github.com. 140.82.116.4 myvm:~$ curl -I https://www.github.com # test that I can fetch a webpage HTTP/2 301 ... If all of the above checks pass, your Alpine VM is correctly NAT\u0026rsquo;d through the RouterOS VM and can reach the Internet.\nTroubleshooting \u0026amp; Gotchas Debugging with Unified Logging macOS logs a large volume of network-related events, and it can be tricky to isolate the relevant ones. Fortunately, the log show and log collect tools make it easier to filter and investigate.\nStart by capturing a snapshot of system logs around the time your VMs are active:\n% mkdir ./log_archive % ./start_lab # start the VMs with a shell script % sudo log collect --output ./log_archive --last 3m # capture the previous 3 minutes of log events. This captures the previous 3 minutes of logs and stores them in a binary archive. Then query the relevant subsystem:\n% log show --archive log_archive/system_logs.logarchive --predicate \u0026#39;subsystem == \u0026#34;com.apple.NetworkSharing\u0026#34;\u0026#39; I found the subsystem == \u0026quot;com.apple.NetworkSharing\u0026quot; query provided the most useful results. Using other filters like subsystem == \u0026quot;com.apple.network\u0026quot; or process == \u0026quot;qemu-system-x86_64\u0026quot; included many unrelated events and were a little overwhelming. This screenshot shows the networks being created when the VMs are started and then torn down when the VMs are stopped.\nPacket Captures with Wireshark or tcpdump Because QEMU uses virtual network interfaces, it\u0026rsquo;s possible to monitor VM traffic directly from macOS using tools like tcpdump or Wireshark. Use the ifconfig command to identify which vmenet interfaces are active and which bridge they are mapped to.\nUsing Wireshark on the correct interface reveals detailed traffic flows. In this example capture, we see:\nICMP ping requests to the router’s external interface A DNS lookup for speedtest.net An HTTP GET request initiating the speed test Other quirks I wrote a simple C program to list the interfaces created by QEMU but ran into permission errors. It turns out that macOS protects the vmnet APIs behind System Integrity Protection (SIP). To access them, binaries must be code-signed with a full Apple Developer certificate, which I don’t have (and didn’t want to pay for). Fortunately, macOS’s unified logging system provided helpful insight.\nAnother issue I encountered was with subnet settings in the router configs. At one point, I accidentally assigned 192.168.2.1/32 instead of 192.168.2.1/24 to the host-only interface on the RouterOS VM. This broke routing for the two VMs, blocking the Alpine VM from reaching the Internet.\nAt first glance QEMU’s socket-based networking may seem like a good replacement for bridged networking. It’s built into QEMU and doesn’t require elevated privileges. So why not use it? It turns out that socket networking is intended for point-to-point communication between specific QEMU instances, like TCP client/server setups. Socket networking doesn’t support general Ethernet bridging or broadcast traffic. For a more flexible setup that allows multiple VMs to communicate freely (and with the host), you still need tap or vmnet.\nConclusion Running QEMU directly on macOS isn’t the most beginner-friendly experience, but it was a great learning opportunity. I have a better appreciation for why tools like UTM or Multipass exist as wrappers around QEMU.\nNext time I might try replacing the x86 images with arm64 images to explore the performance differences. I\u0026rsquo;m also considering writing my own QEMU wrapper, partly for fun and partly for continuious integration purposes.\nIf you enjoyed this or want to follow along with future experiments, follow me on Bluesky. Thanks for reading!\n","permalink":"https://amf3.github.io/articles/virtualization/macos_qemu_networks/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/utmapp/UTM\"\u003eUTM\u003c/a\u003e and \u003ca href=\"https://canonical.com/multipass\"\u003eMultipass\u003c/a\u003e are great apps for virtualization on macOS.\u003cbr\u003e\nBut I wanted a lighter-weight approach by invoking QEMU directly.  Which meant I needed to understand how QEMU\u0026rsquo;s networking options interact\nwith the \u003ccode\u003evmnet\u003c/code\u003e virtualization API on macOS.\u003c/p\u003e\n\u003cp\u003eThis becomes especially important when dealing with VM-to-VM connections, network isolation, and bridging on macOS.\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll walk through creating a simple QEMU-based networking lab.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSet up RouterOS and Alpine Linux VMs using QEMU on macOS\u003c/li\u003e\n\u003cli\u003eConnect VMs with Apple\u0026rsquo;s Hypervisor \u003ccode\u003evmnet\u003c/code\u003e networking APIs\u003c/li\u003e\n\u003cli\u003eUse unified logging to troubleshoot QEMU network issues on macOS\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lab-setup-overview\"\u003eLab Setup Overview\u003c/h2\u003e\n\u003cp\u003eThe network diagram shows the network topology used in this lab.  Both VMs run on on the\nsame macOS host and connected to virtual network interfaces using QEMU\u0026rsquo;s support for Apple\u0026rsquo;s \u003cstrong\u003evmnet\u003c/strong\u003e virtualization API.\u003c/p\u003e","title":"Virtual Router Lab on macOS with QEMU"},{"content":"QEMU uses files to emulate storage devices, and the features available depend on how those files are created. While QEMU can emulate disks from Parallels and VirtualBox, I’m going to focus on the formats most commonly used in automation and scripting, raw and qcow2.\nThe default format is raw and raw offers the fewest features. It\u0026rsquo;s just plain storage. The other format qcow2 supports compression, snapshots, and copy-on-write in addition to storage.\nRaw Format Creating a raw disk with qemu-img is similar to using dd to create a block-based file. One can see this with the output of qemu-img info.\nHere I create two storage devices, one with qemu-img which defaults to the raw format and another with the dd command.\n% qemu-img create my_disk.img +1m Formatting \u0026#39;my_disk.img\u0026#39;, fmt=raw size=1048576 % dd if=/dev/zero of=my_block.file count=1 bs=1m 1+0 records in 1+0 records out Now let\u0026rsquo;s use qemu-img info to confirm there\u0026rsquo;s no difference between the two files.\n% qemu-img info my_disk.img image: my_disk.img file format: raw virtual size: 1 MiB (1048576 bytes) disk size: 1 MiB Child node \u0026#39;/file\u0026#39;: filename: my_disk.img protocol type: file file length: 1 MiB (1048576 bytes) disk size: 1 MiB % qemu-img info my_block.file image: my_block.file file format: raw virtual size: 1 MiB (1048576 bytes) disk size: 1 MiB Child node \u0026#39;/file\u0026#39;: filename: my_block.file protocol type: file file length: 1 MiB (1048576 bytes) disk size: 1 MiB Qcow2 Format Creating a disk in qcow2 format enables zlib compression by default.\n% qemu-img create -f qcow2 my_disk.img 1M Formatting \u0026#39;my_disk.img\u0026#39;, fmt=qcow2 cluster_size=65536 extended_l2=off compression_type=zlib size=1048576 lazy_refcounts=off refcount_bits=16 % qemu-img info my_disk.img image: my_disk.img file format: qcow2 virtual size: 1 MiB (1048576 bytes) disk size: 196 KiB cluster_size: 65536 Format specific information: compat: 1.1 compression type: zlib lazy refcounts: false refcount bits: 16 corrupt: false extended l2: false Child node \u0026#39;/file\u0026#39;: filename: my_disk.img protocol type: file file length: 192 KiB (197120 bytes) disk size: 196 KiB Tip One - Resize an image file It\u0026rsquo;s possible to grow or shrink a QEMU storage device. Think of this as expanding the physical SSD itself, not the filesystem that sits on it. Important, when shrinking a image with negative values, always shrink the filesystem first using resize2fs before running qemu-img resize or risk data corruption.\n% qemu-img resize my_disk.img +1m Image resized. When inspecting the new disk image, we see the new capacity is 2MB but the file size on disk is under 200KB. This is because qcow2 supports copy-on-write and compression.\n% qemu-img info my_disk.img image: my_disk.img file format: qcow2 virtual size: 2 MiB (2097152 bytes) disk size: 196 KiB cluster_size: 65536 Format specific information: compat: 1.1 compression type: zlib lazy refcounts: false refcount bits: 16 corrupt: false extended l2: false Child node \u0026#39;/file\u0026#39;: filename: my_disk.img protocol type: file file length: 192 KiB (197120 bytes) disk size: 196 KiB % ls -lh my_disk.img -rw-r--r-- 1 adam staff 192K Apr 6 10:19 my_disk.img If I were to resize a QEMU storage file formatted as raw, the file size on disk of 2MB matches the image capacity of 2MB as raw doesn\u0026rsquo;t support compression or copy-on-write.\n% qemu-img create raw_disk.img +2m Formatting \u0026#39;raw_disk.img\u0026#39;, fmt=raw size=2097152 % ls -lh raw_disk.img -rw-r--r-- 1 adam staff 2.0M Apr 6 10:22 raw_disk.img Tip Two - Snapshots Snapshots are supported with qcow2 devices. These are handy for creating a base disk image that\u0026rsquo;s shareable and later modified for other purposes. Snapshots also make a great backup point before making image changes.\nTo create a snapshot named \u0026ldquo;my_first_snapshot\u0026rdquo; inside an existing qcow2 image.\n% qemu-img snapshot -c my_first_snapshot my_disk.img To list existing snapshots\n% qemu-img snapshot -l my_disk.img Snapshot list: ID TAG VM_SIZE DATE VM_CLOCK ICOUNT 1 my_first_snapshot 0 B 2025-04-06 10:37:07 0000:00:00.000 0 To revert or \u0026ldquo;apply\u0026rdquo; a snapshot\n% qemu-img snapshot -a my_first_snapshot my_disk.img To delete a snapshot from a file\n% qemu-img snapshot -d my_first_snapshot my_disk.img Tip Three - Modify a QEMU image While many online guides suggest using the Network Block Device (NBD) kernel driver in Linux to mount and modify QEMU images, I use a different process that also works on MacOS. My preferred method is to boot a VM using QEMU and attaching the image as a data drive.\nThis example uses the extended x86_64 Alpine Linux ISO and a QEMU command that mounts the image as a data drive. The Alpine extended ISO lets you log in as root with an empty password, which makes quick edits easy.\n#/bin/sh qemu-system-x86_64 \\ -m 2G -smp cpus=4 -serial stdio \\ -boot once=d \\ -drive file=./my_disk.img,format=qcow2,media=disk,cache=unsafe \\ -drive file=./alpine-extended-3.21.2-x86_64.iso,format=raw,media=cdrom \\ -nic user,model=virtio-net-pci,hostfwd=tcp::2222-:22 Once logged in, you\u0026rsquo;ll see the QEMU file we want to modify listed as /dev/sda. The device hasn\u0026rsquo;t been formatted with a filesystem, but if one were present it could be mounted within the VM, files edited within the image, and then unmounted.\nTip Four - Transfer a QEMU image to bare-metal It\u0026rsquo;s possible to use a QEMU image with bare-metal by converting it to raw format. Use the following to convert the image from qcow2 to raw.\n% qemu-img convert -f qcow2 -O raw my_disk.img raw_disk.img % qemu-img info raw_disk.img image: raw_disk.img file format: raw virtual size: 10 MiB (10485760 bytes) disk size: 10 MiB Child node \u0026#39;/file\u0026#39;: filename: raw_disk.img protocol type: file file length: 10 MiB (10485760 bytes) disk size: 10 MiB Once we have the raw image, the dd command can be used to write the data to either a USB stick or physical SSD. To avoid any destructive commands let\u0026rsquo;s pretend raw_disk2.img represents /dev/sdc, your verified USB thumb drive.\n% dd if=raw_disk.img of=raw_disk2.img bs=1m 10+0 records in 10+0 records out 10485760 bytes transferred in 0.006266 secs (1673437600 bytes/sec) Because our raw file is only 10MB in size, only 10MB will be used on the thumb drive. This is where follow up tools like LVM or resize2fs will extend the filesystem to fill the entire thumb drive. Tools used for expansion depends on how the filesystem was created.\nPutting it all together Enough with the documentation, let\u0026rsquo;s put it into practice with a real use case. Presume that after reading my cloud-init tutorials you wish to modify the Alpine Linux cloud-init image before installation.\nWe can see the downloaded file is a qcow2 image with a capacity of 200Mb from qemu-img info.\n% qemu-img info nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 image: nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 file format: qcow2 virtual size: 200 MiB (209715200 bytes) disk size: 181 MiB ... As we want to install our java app into the installer, we need to add space to the image with qemu-img resize. But first, let’s create a snapshot. That way, if we make a mistake, we won’t need to re-download the cloud-init image.\n% qemu-img snapshot -c no_modifications nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 % qemu-img resize nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 +800M Image resized. % qemu-img info nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 image: nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 file format: qcow2 virtual size: 0.977 GiB (1048576000 bytes) disk size: 197 MiB cluster_size: 65536 Snapshot list: ID TAG VM_SIZE DATE VM_CLOCK ICOUNT 1 no_modifications 0 B 2025-04-06 15:23:50 0000:00:00.000 0 Format specific information: ... I\u0026rsquo;m still using the Alpine extended ISO to boot the VM. Alpine cloud images require setup for ssh key authentication to login and an empty root password is much easier to use.\n% qemu-system-x86_64 \\ -m 2G -smp cpus=4 -serial stdio \\ -boot once=d \\ -drive file=./nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2,format=qcow2,media=disk,cache=unsafe \\ -drive file=./alpine-extended-3.21.2-x86_64.iso,format=raw,media=cdrom \\ -nic user,model=virtio-net-pci,hostfwd=tcp::2222-:22 Login as root and mount the disk device under /mnt\nlocalhost:~# mount /dev/sda /mnt localhost:~# ls /mnt bin home mnt run tmp boot lib opt sbin usr dev lost+found proc srv var etc media root sys Then make changes to the cloud image, unmount the filesystem and you\u0026rsquo;re done.\nlocalhost:~# echo \u0026#34;Adam Faris was here\u0026#34; \u0026gt; /mnt/etc/motd localhost:~# cat /mnt/etc/motd Adam Faris was here localhost:~# umount /mnt localhost:~# poweroff Finally, convert our modified cloud image from qcow2 format to raw format, then use dd to write the raw image to a USB device.\n% qemu-img convert -f qcow2 -O nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 alpine_cloudinit.raw % dd if=alpine_cloudinit.raw bs=1m of=/dev/... With the modified image written to the USB device, you can now boot a physical machine from it. Thanks for sticking with me until the end. If you find this content useful, follow me on BlueSky social for future announcements.\n","permalink":"https://amf3.github.io/articles/virtualization/qemuimage_tips/","summary":"\u003cp\u003eQEMU uses files to emulate storage devices, and the features available\ndepend on how those files are created.  While QEMU can emulate disks from Parallels and VirtualBox, I’m going to\nfocus on the formats most commonly used in automation and scripting, \u003cstrong\u003eraw\u003c/strong\u003e and \u003cstrong\u003eqcow2\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe default format is raw and raw offers the fewest features.  It\u0026rsquo;s just plain storage.  The other format qcow2\nsupports compression, snapshots, and copy-on-write in addition to storage.\u003c/p\u003e","title":"Tips for working with qemu images"},{"content":"I previously wrote an introduction to cloud-init. I\u0026rsquo;d like to now follow up with a discussion on troubleshooting. cloud-init failures on remote hosts can be challenging. Depending on the failure point, cloud-init may or may not provide clear error indicators. These are methods I use during provisioning issues related to cloud-init.\nUnderstanding cloud-init execution stages Before continuing, let\u0026rsquo;s cover some background. cloud-init follows five stages during boot which run sequentially. If a stage completes, output will contain a status that can be used to verify that stage was successful.\nDetect stage: The init system is responsible for calling ds_identify to determine whether cloud-init should run. With systemd hosts, this is implemented as a systemd generator.\nLocal stage: Identifies local resources that are available without network access. Configures networking, which if unsuccessful falls back to DHCP.\nNetwork stage: Retrieves user-data, sets up disk partitions, and mounts the filesystem. When complete, serial console or SSH access should become available.\nConfig stage: Runs configuration modules and executes commands specified in user-data.\nFinal stage: Installs packages, applies configuration management plugins like puppet or chef, and runs user or vendor defined scripts.\nChecking Stage Status The status submenu from the cloud-init command provides a method of checking each stage for errors. In this example I intentionally mistyped a schema key name that should be passwd as password. Output shows the failure occurred during the init stage \u0026amp; provides a suggestion on how to resolve it.\n$ cloud-init status --format json ... \u0026#34;extended_status\u0026#34;: \u0026#34;degraded done\u0026#34;, \u0026#34;init\u0026#34;: { \u0026#34;errors\u0026#34;: [], \u0026#34;finished\u0026#34;: 6.52, \u0026#34;recoverable_errors\u0026#34;: { \u0026#34;WARNING\u0026#34;: [ \u0026#34;cloud-config failed schema validation! You may run \u0026#39;sudo cloud-init schema --system\u0026#39; to check the details.\u0026#34; ] }, ... \u0026#34;status\u0026#34;: \u0026#34;done\u0026#34; Checking logs for Errors When the issue is not obvious, there logs are available for further examination.\n/var/log/cloud-init.log (execution details and errors) /var/log/cloud-init-output.log (captured output from executed commands) /run/cloud-init/result.json (summary of execution status) Example log output from cloud-init.log indicating a schema validation failure.\n2025-03-18 11:46:41,379 - schema.py[WARNING]: cloud-config failed schema validation! You may run \u0026#39;sudo cloud-init schema --system\u0026#39; to check the details. Debugging User-Data Issues cloud-init has a defined schema and it’s possible to validate user-data content with the schema submenu. To troubleshoot a possible schema issue on a remote host where a YAML key named passwd was mistyped as password.\n$ sudo cloud-init schema --system Found cloud-config data types: user-data, vendor-data, network-config 1. user-data at /var/lib/cloud/instances/docker-demo/cloud-config.txt: Invalid user-data /var/lib/cloud/instances/docker-demo/cloud-config.txt Error: Cloud config schema errors: users.0: Additional properties are not allowed (\u0026#39;password\u0026#39; was unexpected) … Error: Invalid schema: user-data To test changes made to user-data content prior to provisioning: cloud-init schema -c “my_user_data_file.yaml”.\nFor timeout issues in user or vendor scripts, cloud-init analyze will print execution times which pinpoint delays.\nCommon Failure Scenarios and Fixes A typical source of failures is from syntax errors in the user-data file. As already mentioned, cloud-init schema will show schema issues in user-data. Manually check for typos within the values in user-data. A mistyped value is still a string and can pass the schema validation.\nAnother possible issue is misconfigured network settings preventing package installation. Ensure package mirrors are reachable and contain the package. The cloud-init-output.log file can show additional hints related to package failures. If SSH is unavailable, try accessing the instance over the instance\u0026rsquo;s serial console.\nCheck for missing or incorrectly set permissions on scripts.\nUse cloud-init analyze show to help in identifying long-running stages.\n$ cloud-init analyze show -- Boot Record 01 -- The total time elapsed since completing an event is printed after the \u0026#34;@\u0026#34; character. The time the event takes is printed after the \u0026#34;+\u0026#34; character. Starting stage: init-local |`-\u0026gt;no cache found @00.00100s +00.00000s |`-\u0026gt;found local data from DataSourceNoCloud @00.00400s +00.01500s Finished stage: (init-local) 00.28900 seconds Starting stage: init-network |`-\u0026gt;restored from cache with run check: DataSourceNoCloud [seed=/dev/vda] @02.56800s +00.00100s |`-\u0026gt;setting up datasource @02.57600s +00.00000s |`-\u0026gt;reading and applying user-data @02.58000s +00.00200s |`-\u0026gt;reading and applying vendor-data @02.58200s +00.00200s |`-\u0026gt;reading and applying vendor-data2 @02.58400s +00.00000s ... Recovery and Re-Runs Additional steps are needed after modifying user-data files on the failed instance. When cloud-init runs, output is cached to disk. If the cache exists on reboot, cloud-init will not run again. To clean cached instance data, run cloud-init clean --logs and reboot the instance.\nIssues with a specific module can be exposed by using cloud-init single. This could be useful when troubleshooting user or vendor scripts. Module names can be found with grep \u0026quot;Running module\u0026quot; /var/log/cloud-init.log.\n$ sudo cloud-init single --name set_passwords Cloud-init v. 24.4.1-0ubuntu0~24.04.1 running \u0026#39;single\u0026#39; at Fri, 21 Mar 2025 20:45:47 +0000. Up 16145.16 seconds. When using the single submenu, it won\u0026rsquo;t necessarily fix dependencies unless those are also explicitly re-triggered. It\u0026rsquo;s best to reprovision the instance after troubleshooting any failed modules.\nTakeaways There’s no simple fix for understanding why instance provisioning with cloud-init failed. That’s why I’m closing with a checklist.\nCheck cloud-init status Use cloud-init status --long (or \u0026ndash;json) for execution state Use cloud-init analyze for timing analysis Inspect logs for errors /var/log/cloud-init.log: Shows errors and execution order /var/log/cloud-init-output.log: contains command output Validate user-data input cloud-init schema to validate syntax Ensure values are correct and not only properly formatted YAML Check for missing dependencies or network issues Ensure package mirrors are available and contain the necessary packages. Verify custom scripts are executable. Re-run cloud-init if needed. Clean logs and reset cloud-init: cloud-init clean --logs \u0026amp;\u0026amp; reboot Manually rerun a failed module: cloud-init single -n “some_module_name” Happy provisioning, and follow me on Bluesky if you find content like this interesting.\n","permalink":"https://amf3.github.io/articles/cloudinit/troubleshooting/","summary":"\u003cp\u003eI previously wrote an \u003ca href=\"../../../articles/cloudinit/intro/\"\u003eintroduction\u003c/a\u003e to cloud-init. I\u0026rsquo;d like to now follow up with a discussion on\ntroubleshooting. cloud-init failures on remote hosts can be challenging. Depending on the failure point, cloud-init may or may not\nprovide clear error indicators.  These are methods I use during provisioning issues related to cloud-init.\u003c/p\u003e\n\u003ch1 id=\"understanding-cloud-init-execution-stages\"\u003eUnderstanding cloud-init execution stages\u003c/h1\u003e\n\u003cp\u003eBefore continuing, let\u0026rsquo;s cover some background.  cloud-init follows\n\u003ca href=\"https://cloudinit.readthedocs.io/en/latest/explanation/boot.html\"\u003efive stages\u003c/a\u003e during boot which run sequentially. If\na stage completes, output will contain a status that can be used to verify that stage was successful.\u003c/p\u003e","title":"cloud-init troubleshooting"},{"content":"In a previous post, I discussed using cloud-init and Multipass as a method of provisioning virtual machines on a local computer with a cloud-like API. Today I am going to dive deeper with Ubuntu and how their autoinstall API can simplify on-premise host provisioning.\nautoinstall is a tool that allows for unattended installations of Ubuntu, ensuring consistency, reporducibility, and providing automation across a fleet of hosts. In this post I\u0026rsquo;ll walk through an example of using autoinstall to configure networking, local storage, and demonstrate shell command execution during provisioning.\nPrerequisite Because the final target is a bare-metal instance, I find it quicker to iterate \u0026amp; test autoinstall changes with QEMU on my macOS M2 laptop. QEMU is a hardware emulator which runs on Linux, macOS, \u0026amp; Windows, allowing the emulation of different CPUs, network cards, or storage devices. Instructions to install QEMU can be found online. For macOS, this can be as simple as running brew install qemu.\nNext we need the Ubuntu install media which can be downloaded here.\nQEMU overview Let\u0026rsquo;s get started by creating a virtual disk drive for installing Ubuntu. This can be done with qemu-img create -f qcow2 virtual_disk.img 6G which creates a 6GB virtual disk named virtual_disk.img in the current directory.\nIn the example below, the -boot once=d option instructes QEMU to boot from the virtual CD-ROM on first startup. After which QEMU will boot from the virtual disk. The other options initialize a 4 core CPU with 4GB of memory. The -net user,hostfwd string will port forward from localhost on the host system to port 22 on the virtual machine. If additional port forwarding is needed, like testing https traffic on port 443 of the VM, multiple hostfwd options seperated by commas can used. Be sure to adjust the filename and path to the Ubuntu ISO as needed.\nqemu-system-x86_64 -hda virtual_disk.img -boot once=d -cdrom ./ubuntu-24.10-live-server-amd64.iso -m 4096 -smp cpus=4 -net nic,model=virtio -net user,hostfwd=tcp:127.0.0.1:2222-:22 Autoinstall Autoinstall is included as part of the Ubuntu boot ISO and works with other provisioning tools from Canonical like Subiquity, Curtin, or cloud-init. When reading Autoinstall documentation, it\u0026rsquo;s useful to know which tool is being used during each install stage as often those options are passed to the underlying provisioning tool.\nLike Kickstart for RHEL, Autoinstall is Ubuntu\u0026rsquo;s answer to unattended installations, and uses YAML files for data input. Autoinstall uses default locations for finding the YAML files and locations can also be specified in the GRUB menu when the instance boots. Locations are specified as either a filepath or a URL. I\u0026rsquo;ll be using a URL for the file locations.\nLets create the empty YAML files and use python to create a simple webserver to serve the files. In another terminal type the following as the webserver runs in the foreground. Use cntl-c to terminate the python webserver when it\u0026rsquo;s no longer needed.\ntouch user-data meta-data network-config vendor-data python3 -m http.server -b 127.0.0.1 -d $PWD 8080 Next start the virtual machine.\nqemu-system-x86_64 -hda virtual_disk.img -boot once=d -cdrom ./ubuntu-24.10-live-server-amd64.iso -m 4096 -smp cpus=4 -net nic,model=virtio -net user,hostfwd=tcp:127.0.0.1:2222-:22 This will open a QEMU console window where we\u0026rsquo;ll interact with the GRUB menu to specify the YAML file locations. Change focus to the console window, highlight \u0026ldquo;Try or Install Ubuntu Server\u0026rdquo; and hit the \u0026quot;e\u0026quot; key to edit the grub menu.\nOn the line starting with “linux /casper/vmlinuz” add: autoinstall ds=nocloud\\;s=http://10.0.2.2:8080/ before the three dashes. The grub menu should look something like this when the edits are complete.\nlinux /casper/vmlinuz autoinstall ds=nocloud\\;s=http://10.0.2.2:8080/ --- initrd /casper/initrd Exit grub and boot by following the on-screen instructions to hit F10 or cntl-x. Watch the terminal running the python webserver and requests for the autoinstall YAML files should be seen. As they are empty config files, an interactive menu-driven session will present itself in the QEMU console window. To cancel the install, close the QEMU console window.\nThe GRUB modification tells autoinstall to use the nocloud plugin from cloud-init to download its configuration at the specified URL. QEMU assigns the special IP address of 10.0.2.2 to the host system when using -net user. This allows the VM to reach services running on the host such as our local Python HTTP server and why autoinstall is able to download its configurations over HTTP.\nThe YAML block should be added to the user-data file that was created earlier. The other files will remain empty. The minimal config example assigns a hostname of my-qemu-vm, creates an admin account named ubuntu, and assigns the ubuntu user the password of abc123.\nIt\u0026rsquo;s possible to generate a different secure password hash with openssl, as shown in this example: echo abc123 | openssl passwd -6 -stdin. Restart the QEMU VM so it boots from the virtual CD-ROM and modify the GRUB menu so it loads the new config when the VM boots.\n#cloud-config autoinstall: version: 1 identity: hostname: my-qemu-vm username: ubuntu password: $6$xK2amorOU9tK4jt4$zLA1RZUpo4CzyDBzPDHCT61FLOngjWpV7Q/BH9KieLsJ/VG8r/Y88YIMLIOL.vc4ZHees40IAqORxjqa7GKti/ # password is \u0026#34;abc123\u0026#34; Autoinstall will take several minutes to complete and will reboot when done. In some stages autoinstall can look stuck in some stages. Remember that Linux virtual consoles are available to inspect running processes. Virtual consoles are accessible by typing alt + left/right arrow key or using alt + F2 or alt + F3. (Use the option key for alt when using a Mac keyboard.) Eventually the VM will reboot and the login prompt should be visible if everything went as expected.\nAutoinstall has a list of defaults it uses when the option is present in the user-data file. After logging into the QEMU instance, it\u0026rsquo;s possible to view the specified values from the user-data YAML file that have been merged into the defaults.\nsudo less /var/log/installer/autoinstall-user-data Before continuing lets enable the ssh server, allow passwords for ssh login, and minimize the number of packages used during the install. Other options like locale or the keyboard setup can be found in the autoinstall-user-data file and added ot the example below. Restarting the QEMU VM and modifying the GRUB menu to reinstall the host OS is needed to apply the new changes to the YAML file. Reinstalling the OS also demonstrates the ease of initializing a system to a known state with autoinstall \u0026amp; cloud-init configs.\n#cloud-config autoinstall: version: 1 identity: hostname: my-qemu-vm username: ubuntu password: $6$xK2amorOU9tK4jt4$zLA1RZUpo4CzyDBzPDHCT61FLOngjWpV7Q/BH9KieLsJ/VG8r/Y88YIMLIOL.vc4ZHees40IAqORxjqa7GKti/ # password is \u0026#34;abc123\u0026#34; ssh: # Install SSH server and allow password logins allow-pw: true install-server: true source: # id can also be ubuntu-server id: ubuntu-server-minimal Networking Both autoinstall and cloud-init support a netplan-formatted network configuration, meaning the YAML network example will work with either installer.\nNetwork device names are different between distributions that use Systemd (Ubuntu, Fedora) vs OpenRC (Alpine). Where OpenRC will use easily found device names like \u0026ldquo;eth0\u0026rdquo;, or \u0026ldquo;eth1\u0026rdquo;, Systemd will use the PCI slot number. A Systemd example might look like \u0026ldquo;enp2s0\u0026rdquo;, where \u0026ldquo;en\u0026rdquo; means ethernet, and \u0026ldquo;p2s0\u0026rdquo; is the physical PCI slot. This value will change based on which slot a PCI card is plugged into. Luckily autoinstall lets us wildcard the device names.\nThis network example will work with either OpenRC or Systemd device names. It\u0026rsquo;s similar to what\u0026rsquo;s used by Ubuntu\u0026rsquo;s LiveCD.\nnetwork: version: 2 ethernets: my-en-devices: match: # This will match Systemd naming conventions for ethernet devices which start with \u0026#34;en\u0026#34; and set them to use DHCPv4 name: \u0026#34;en*\u0026#34; dhcp4: true my-eth-devices: match: # This will match OpenRC naming conventions like \u0026#34;eth0\u0026#34; name: \u0026#34;eth*\u0026#34; addresses: # This will specify a static network address - 10.10.10.2/24 nameservers: # We can modify the DNS search path \u0026amp; specify DNS name servers. search: - \u0026#34;mycompany.local\u0026#34; addresses: - 10.10.10.253 - 8.8.8.8 Storage Configuring storage can be complex when configuring per partition byte offsets. Luckily we can provide a storage device name and let defaults handle the details. I\u0026rsquo;ll show a basic lvm example but the other supported layouts are direct, and zfs.\nHere we specify a LVM configuration with a sizing policy to use the entire disk for the logical volume. If sizing-policy were set to scaled, free space would be left on the storage device for things like snapshots or further expansion.\nstorage: layout: name: lvm sizing-policy: all Its possible to target a specific drive to wipe and install a new OS with a match statement. There are multiple ways to select a storage device, model name, serial number, path, whether its rotational or not, or even big or little in size. These values can be found in smartctl output, which comes from the smartmontools package.\nubuntu@my-qemu-vm:~$ sudo apt-get install -y smartmontools ... install stuff ... ubuntu@my-qemu-vm:~$ sudo smartctl -i /dev/sda smartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.11.0-18-generic] (local build) Copyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org === START OF INFORMATION SECTION === Device Model: QEMU HARDDISK Serial Number: QM00001 Firmware Version: 2.5+ User Capacity: 8,589,934,592 bytes [8.58 GB] Sector Size: 512 bytes logical/physical TRIM Command: Available, deterministic Device is: Not in smartctl database 7.3/5528 ATA Version is: ATA/ATAPI-7, ATA/ATAPI-5 published, ANSI NCITS 340-2000 Local Time is: Tue Mar 4 05:45:56 2025 UTC SMART support is: Available - device has SMART capability. SMART support is: Enabled If we wanted to match this disk by wild-carding the model name, we would use the following.\n#cloud-config autoinstall: storage: layout: name: lvm sizing-policy: all match: model: QEMU* Alternatively if our on-premise hardware instance had a 1GB SSD for the OS and a second 12GB spinning disk for data storage, we could use a match with size size: smallest to install the OS on the 1GB disk.\n#cloud-config autoinstall: storage: layout: name: lvm sizing-policy: all match: size: smallest Commands Running arbitrary commands is possible when autoinstall runs. Commands are specified as a list and run under \u0026ldquo;sh -c\u0026rdquo;. Its possible to specify if commands should run early in the autoinstall process, late, or when an error occurs.\nFor example we want to hit a web endpoint when the installer has completed.\n#cloud-config autoinstall: late-commands: - curl -H \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;host\u0026#34;: \u0026#34;\u0026#39;$HOSTNAME\u0026#39;\u0026#34;}\u0026#39; http://myapi.example.com/success - echo \u0026#34;Install Success\u0026#34; \u0026gt; /var/log/my.log To run a command before the autoinstall process runs, like downloading internal x509 certificates:\n#cloud-config autoinstall: early-commands: - mkdir /etc/ssl/mycerts - wget -O /etc/ssl/mycerts/internal.pem \u0026#34;http://x509api.example.com/certs/$HOSTNAME\u0026#34; Or reporting an error when autoinstall fails\n#cloud-config autoinstall: error-commands: - echo \u0026#34;Install failed\u0026#34; \u0026gt; /var/log/my.log - curl -H \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;host\u0026#34;: \u0026#34;\u0026#39;$HOSTNAME\u0026#39;\u0026#34;}\u0026#39; http://myapi.example.com/failures cloud-init It\u0026rsquo;s possible to invoke cloud-init from autoinstall, allowing for additional functionality. This is done by placing the cloud-init entries under a user-data key. Here\u0026rsquo;s a cloud-init example that installs a few packages.\ncloud-init and autoinstall sometimes perform similar tasks. When configuring a storage device with cloud-init, I found it was better to use autoinstall as the cloud-init changes were overwritten.\n#cloud-config autoinstall: user-data: package_update: true # update the list of available packages package_upgrade: true # upgrade currently installed packages. packages: - curl - ca-certificates Other It\u0026rsquo;s possible to configure a local proxy for downloading software packages. Running apt-cacher-ng as a package proxy inside a docker container on my laptop helps when I\u0026rsquo;m on a high latency Internet connection.\n#cloud-config autoinstall: proxy: http://10.0.2.2:3142 Provision a physical host A complete autoinstall user-data file can be downloaded from here. It contains all the examples listed in this post.\nProvisioning a physical host is very similar to using QEMU. The only change is when starting the python webserver. Instead of python binding to 127.0.0.1, have it bind to all interfaces so configs can be downloaded by remote hosts.\npython3 -m http.server -d $PWD 8080 A USB thumb drive is needed to make the Ubuntu ISO available to the physical host; and a monitor \u0026amp; keyboard are needed to modify the GRUB menu when the on-premise hosts boots. When modifying the GRUB menu, instead of using http://10.0.2.2 in the nocloud URL, specify the hostname of the host running the python webserver. In my scenario, the hostname would resolve to my personal laptop.\nWrapping Up By leveraging autoinstall, it\u0026rsquo;s possible to reliably reproduce system setups, whether for virtual machines or bare-metal hosts. In this post, autoinstall was explored as a method to streamline unattended provisioning for Ubuntu instances. Using a QEMU-based test environment, it was possible to quickly iterate on autoinstall configurations by modifying the GRUB menu to pull configuration files over HTTP. The process demonstrated how to format storage devices, set up networking, and run shell commands during installation.\nNext steps? If looking to extend this setup, consider integrating additional automation, such as PXE boot for network-based installs or using cloud-init to interact with configuration management systems like Puppet or Chef. If you have insights from your own experiences, feel free to share them on Bluesky.\n","permalink":"https://amf3.github.io/articles/cloudinit/autoinstall/","summary":"\u003cp\u003eIn a \u003ca href=\"../../../articles/cloudinit/intro/\"\u003eprevious post\u003c/a\u003e, I discussed using cloud-init and Multipass as a method of provisioning\nvirtual machines on a local computer with a cloud-like API.  Today I am going\nto dive deeper with Ubuntu and how their autoinstall API can simplify on-premise host provisioning.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://canonical-subiquity.readthedocs-hosted.com/en/latest/intro-to-autoinstall.html\"\u003eautoinstall\u003c/a\u003e\nis a tool that allows for unattended installations of Ubuntu, ensuring consistency, reporducibility, and providing automation\nacross a fleet of hosts.  In this post I\u0026rsquo;ll walk through an example of using autoinstall\nto configure networking, local storage, and demonstrate shell command execution during provisioning.\u003c/p\u003e","title":"Unattended Ubuntu Installs - Virtual Machines to Bare-Metal"},{"content":"Cloud compute companies like GCP, AWS, or Azure offer a management API for allocating resources. In the on-premise space, services such as Docker or Incus provide APIs for managing containers or virtual machines (VMs). But what about installing the operating system (OS) on bare-metal hosts? What API exists for this task? This is where cloud-init enters the picture, providing the ability to provision VMs or bare-metal hardware.\ncloud-init is a useful tool that doesn\u0026rsquo;t rely on network services like PXE as a dependency. Its simplicity saves time by removing the need to navigate OS installation menus, while ensuring user accounts and installed software packages are consistent across hosts. So why should one bother using cloud-init if they are managing a single host at home? In the event the OS needs to be reinstalled due to failure, cloud-init allows one to quickly restore the system to a known state.\nThis example will use cloud-init to configure a Personal Package Archive (PPA), install Docker, and create a user account inside a Ubuntu VM.\nPrerequisite I find that using cloud-init with Multipass is a easy way to get started. Multipass is a virtual machine manager that works with Linux, MacOS (arm \u0026amp; intel), and Windows. When launching a new VM, Multipass is capable of initializing the VM with cloud-init. If Multipass isn\u0026rsquo;t already installed, this link will provide instructions for installing Multipass. For this cloud-init introduction, I\u0026rsquo;m using Multipass on a M2 Macbook running MacOS Sequoia.\ncloud-init Like many infrastructure tools, the input data for cloud-init is a YAML file. For specifics of this schema, consult the official cloud-init documentation. There one will find that cloud-init input file will need to be prefixed with #cloud-config.\nPackage Management Lets get started with package management for our Multipass instance. This section will show how to add an external PPA (software repository) to the VM with cloud-init to provide additional software packages and define a list of software packages to be installed on the VM.\nAdd External PPA Add the 3rd-party PPA provided by Docker, Inc.\n# Add Docker\u0026#39;s PPA for Ubuntu apt: sources: docker.list: # This snippet comes from https://stackoverflow.com/a/62540068 source: deb [arch=arm64] https://download.docker.com/linux/ubuntu $RELEASE stable # Key ID can be found with “gpg --show-keys \u0026lt;(curl -s https://download.docker.com/linux/ubuntu/gpg)” keyid: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88 Should the GPG key ID for the Docker PPA change, I have left a comment above on how to find that value.\nThis is how the GPG output appears in 2025.\n$ gpg --show-keys \u0026lt;(curl -s https://download.docker.com/linux/ubuntu/gpg) pub rsa4096 2017-02-22 [SCEA] 9DC858229FC7DD38854AE2D88D81803C0EBFCD88 uid Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt; sub rsa4096 2017-02-22 [S] Define Package List Specify a list of packages to install.\n# Update the list of packages available online package_update: true # Upgrade all installed packages package_upgrade: true # Install docker \u0026amp; other utilities packages: - apt-transport-https - ca-certificates - curl - gnupg-agent - software-properties-common - docker-ce - docker-ce-cli - containerd.io - docker-buildx-plugin - docker-compose-plugin User Management Here a new user account is created and added to the docker group with cloud-init. Its likely our user will require both a password \u0026amp; ssh key for remote access. A public ssh key and a password hash is needed for cloud-init input.\nSecrets: Generating a Password Hash To create a password hash, use the mkpasswd command from Ubuntu\u0026rsquo;s whois package. This example will hash the weak password of \u0026ldquo;abc123\u0026rdquo; with the sha512 algorithm. A password better than \u0026ldquo;abc123\u0026rdquo; should be used if following these examples.\n$ mkpasswd -m sha-512 \u0026#34;abc123\u0026#34; $6$EkwQ38oDCPnJDuui$QKw3IISzY3emHXgJ/QHeEH8xyzGOKB3N6.bU/wAkwf4KDRsreB2iApa/EHULbunx6v9o9Q8foq4K.d8WtHukU/ As mkpasswd is specific to Linux and doesn\u0026rsquo;t work with MacOS, one can alternatively use openssl to create a password hash.\n$ echo abc123 | openssl passwd -6 -stdin $6$tdPON3RwkVViXg41$4O9euMZeGFJQXgJ3bvP3YtVcCw9BwIMHLLkix1s/R7woSuAAFvWWtrqqQ.33ESzgcUi9/HdEwelqB9jJUIrpU0 Secrets: Generating a SSH public private key pair To create a SSH key pair, use ssh-keygen: ssh-keygen -t ed25519 -f ./docker_vm_key -C \u0026quot;app@docker_vm\u0026quot; -P abc123. This will create a public \u0026amp; private ssh key in the current directory, with the easily guessable passphrase of abc123. Once again, use a better passphrase if following these examples.\nDefining the User Account This defines an application account named \u0026ldquo;app\u0026rdquo;. The ssh_authorized_keys value comes from the contents of docker_vm_key.pub.\nAs a convenience, the public and private ssh keys from this example are provided.\n# create the docker group groups: - docker users: - name: app groups: [docker, admin, users] gecos: Application User shell: /bin/bash lock_passwd: true passwd: $6$tdPON3RwkVViXg41$4O9euMZeGFJQXgJ3bvP3YtVcCw9BwIMHLLkix1s/R7woSuAAFvWWtrqqQ.33ESzgcUi9/HdEwelqB9jJUIrpU0 ssh_authorized_keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHsPNGa1NJLd4edDLRI033Sw33Nkl6qO+52qNAhY556C app@docker_vm Putting it all together I\u0026rsquo;ve combined the YAML snippets into a single file named docker-install.yaml which can be downloaded here.\nRun the following to see cloud-init in action. This will create a virtual machine with 2 virtual CPU cores, 2 GB of ram, with a 4GB virtual disk using the LTS release of Ubuntu. Depending on your Internet speed, this may take a few minutes as you\u0026rsquo;ll be downloading packages from the Internet.\n$ multipass launch -n docker-demo --cloud-init docker-install.yaml -c 2 -m 2G -d 4G lts To find the new VM and access it over SSH with the private key so a docker command can be ran from a remote shell.\n% mp list Name State IPv4 Image docker-demo Running 192.168.64.32 Ubuntu 24.04 LTS 172.17.0.1 % ssh -l app -i ./docker_vm_key 192.168.64.32 The authenticity of host \u0026#39;192.168.64.32 (192.168.64.32)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:EUqLjr9n9CyjKY6Y8EzNQGomeEtpePMFo5BXjO8YfHY. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes ... app@docker-demo:~$ docker run hello-world Unable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world c9c5fd25a1bd: Pull complete Digest: sha256:e0b569a5163a5e6be84e210a2587e7d447e08f87a0e90798363fa44a0464a1e8 Status: Downloaded newer image for hello-world:latest Hello from Docker! ... Conclusion Several cloud-init basics have been covered in this introduction. Like adding a PPA, installing software packages, and creating a user account.\nWhile I understand that installing Docker in my example might not represent the typical workflow. Combining cloud-init concepts with Multipass creates a local mini-cloud on my Macbook. I can quickly iterate through cloud-init data file changes for other platforms like AWS or on-premise hardware.\ncloud-init is capable of much more, like formatting hard drives or managing network interfaces. These \u0026amp; other topics will be covered in followups which I will announce on Bluesky. Follow me for notifications of when its made available. Otherwise, try out these examples and let me know what works.\n","permalink":"https://amf3.github.io/articles/cloudinit/intro/","summary":"\u003cp\u003eCloud compute companies like GCP, AWS, or Azure offer a management API for allocating resources. In the on-premise space,\nservices such as Docker or Incus provide APIs for managing containers or virtual machines (VMs). But what about installing\nthe operating system (OS) on bare-metal hosts? What API exists for this task? This is where\n\u003ca href=\"https://github.com/canonical/cloud-init\"\u003ecloud-init\u003c/a\u003e enters the picture, providing the ability to provision VMs or\nbare-metal hardware.\u003c/p\u003e\n\u003cp\u003ecloud-init is a useful tool that doesn\u0026rsquo;t rely on network services like PXE as a dependency.  Its simplicity saves time by\nremoving the need to navigate OS installation menus, while ensuring user accounts and installed software packages are consistent\nacross hosts. So why should one bother using cloud-init if they are managing a single host at home? In the event\nthe OS needs to be reinstalled due to failure, cloud-init allows one to quickly restore the system to a known state.\u003c/p\u003e","title":"Getting started with cloud-init for unattended Linux deployments"}]
[{"content":"Intro I have a small home lab that I use for testing differnet scenarios. Lately I have been choosing a distributed storage solution for use at home. Usually I look at industry and scale down the solution. This has lead to conversations with colleages and the question of \u0026ldquo;How did Amazon Web Services implement their Elastic Block Store offering?\u0026rdquo;.\nBackground There are solutions that provide network storage like iSCSI, CIFS, or NFS. These protocols can be grouped into two data access methods. One method will access data as a file \u0026amp; the method is to access data as a block device. Block storage is a device that stores data in evenly spaced chunks called blocks. An example block storage device is the hard drive inside a computer.\nCIFS \u0026amp; NFS are protocols where data is accessed as a file. iSCSI will access data at the block level. Sometimes backup solutions like Apple\u0026rsquo;s TimeMachine will obfuscate this by treating the file as a block devic. From the perspective of the storage server, it\u0026rsquo;s providing NFS or CIFS access to the file so its file based access.\nFor me, iSCSI is the more interesting use case as it allows a physical host, VM, or container 1, to access the block device over the network. Having access to the underlying block device provides flexiblity with data management.\nAn example is being able to snapshot the block device as a backup or a point in time image for use in future provisionings.\nIn my home lab I have tinkered with iSCSI and found that data access latency is bad without a dedicated storage-only network connection. Even with a dedictated connection, its argueable that a 1Gb/s network isn\u0026rsquo;t fast enough for a SAN. Typcially when SANs are mentioned companies pull out the checkbook because it involves custom hardware. Elastic Block Storage is no exception to this statement.\nElastic Block Storage (EBS) is a storage offering from Amazon AWS. EBS is a service that emulates a block device where data is persistently stored over the network.\nEBS Findings AWS uses a combined approach to providing a performant storage solution to clients. 2 This is done by using a custom network stack that uses datagrams, specialized hardware that provides a bridge between storage \u0026amp; networking, and a minimized Linux kernel running the EC2 servers.\nNetwork AWS replaced TCP with a datagram based protocol named Scalable Reliable Datagram (SRD). SRD supports equal cost multipathing, congestion control, and reliable out of order delivery, leaving ordering to networking (OSI) layers above SRD.\nA summary of why SRD matters is it removes latecy caused by the three-way TCP handshake \u0026amp; congestion control delays in TCP.\nCustom Hardware - Data Processing Units Data Processing Units (DPU) are an interesting bit of hardware. The DPU is a PCI card that has a onboard CPU, Memory, \u0026amp; SFP+ networking ports. The DPU plugs into the server and gains access to the servers PCI lanes. This allows it emulate both a storage device and network interface card (NIC).\nI quickly realized the DPU, which AWS calls a Nitro card in marketing, is the magic sauce for EBS. The Nitro card will represent itself as a NVME storage device to the server operating system (OS) but will handle the networking to the EBS volumes without the server\u0026rsquo;s OS knowing it. AWS even moved the SRD network stack into the Nitro card which means networking is handled by the hardware, not software, making networking more efficent.\nIn addition to emulating the storage device, the Nitro card will also provide networking for the server OS.\nOne other function the Nitro card performs, is during server resets (reboots), the Nitro card will checksum the bios to validate it hasn\u0026rsquo;t been tampered with. Remember that its a computer on a PCI card with access to the PCI lanes of the server its plugged into.\nKernel AWS announced in 2019 that they were moving away from Xen to using KVM with Qemu for hosting their EC2 offering. This is relevant to the NVME storage interface provided by the Nitro card.\nQemu has the ability to directly access a NVME device by using a userspace driver. Because the Nitro card provides a NVME storage interface and system networking, there\u0026rsquo;s no need for the linux kernel on the server to need a storage or network driver. This leads to a very lean kernel on the EC2 host, essentially turning it into a appliance.\nFollow up Gluster FS? QEMU gfapi driver https://docs.gluster.org/en/latest/Administrator-Guide/Building-QEMU-With-gfapi-For-Debian-Based-Systems/#building-qemu\nReferences Bouffler B. In the search for performance, thereâ€™s more than one way to build a network. Amazon Web Services. June 22, 2021. Accessed August 20, 2023. https://aws.amazon.com/blogs/hpc/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network/\nThe Security Design of the AWS Nitro System. Amazon Web Services. November 18, 2022. Accessed August 20, 2023. https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/security-design-of-aws-nitro-system.html\nBaligh H. Serag E. Talaat S. Gaballah Y. DPUS: Acceleration Through Disaggregation. Dell Technologies. 2021. Accessed August 20, 2023 https://education.dell.com/content/dam/dell-emc/documents/en-us/2021KS_Baligh-DPUs_Acceleration_Through_Disaggregation.pdf\nDisk Images. Qemu Project August 2023. Accessed August 20, 2023. https://qemu-project.gitlab.io/qemu/system/images.html#nvme-disk-images\nThorstensen T. NVMe Namespaces. Drew Thorstensen. November 21, 2020. Accessed August 20, 2023 http://www.drewthorst.com/posts/nvme/namespaces/readme/\nOTHER Do something with this\nhttps://en.wikipedia.org/wiki/ISCSI#Concepts (maybe link?) Want to discuss IP vs Datagrams\nKubernetes can access block storage with Container Storage Interface (CSI) plugins. Docker volumes also have plugins that support block storage.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI am not affiliated with AWS nor do I have internal contacts with AWS. The content on this page is my interpretation from reading publicly available marketing documentation. I feel I\u0026rsquo;m close enough in my interpretation for a layman\u0026rsquo;s understanding of how EBS is implmmented.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://amf3.github.io/articles/arch/aws_ebs/ebs/","summary":"Intro I have a small home lab that I use for testing differnet scenarios. Lately I have been choosing a distributed storage solution for use at home. Usually I look at industry and scale down the solution. This has lead to conversations with colleages and the question of \u0026ldquo;How did Amazon Web Services implement their Elastic Block Store offering?\u0026rdquo;.\nBackground There are solutions that provide network storage like iSCSI, CIFS, or NFS.","title":"Elastic Block Store Observations"},{"content":"How to get in touch GitHub: https://github.com/amf3 LinkedIn: https://www.linkedin.com/in/adammfaris/ BlueSky Social: https://@amf3.bsky.social ","permalink":"http://amf3.github.io/about/contact/","summary":"How to get in touch GitHub: https://github.com/amf3 LinkedIn: https://www.linkedin.com/in/adammfaris/ BlueSky Social: https://@amf3.bsky.social ","title":"Contact"}]
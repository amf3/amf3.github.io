[{"content":"OpenAPI is a specification for documenting HTTP APIs for both humans and machines to consume. As OpenAPI is a specification, it is language agnostic. OpenAPI relies on generators for translating the specification. There\u0026rsquo;s more than just documentation that\u0026rsquo;s generated. Generators also create language-specific interfaces, tooling, and contracts. In some ways the OpenAPI pattern reminds me of either protobuf with gRPC or ORM schema-first design. As a result, a declarative API is created by the tooling.\nBy the end of this post you\u0026rsquo;ll have:\nA working Go http server generated from an OpenAPI specification. A Python http client generated from the same specification and authenticates with basic auth. Insight into common OpenAPI pitfalls and how to avoid them. [openapi.yaml] ‚Üì +--------------+ | oapi-codegen | ---\u0026gt; [Go Server] +--------------+ ‚Üì +-----------------------+ | openapi-python-client | ---\u0026gt; [Python Client] +-----------------------+ If you would like to follow along, a complete code example can be downloaded and extracted into a temporary working directory.\nGenerators Because generators are consuming the specification, the OpenAPI version is determined by what the generators support.\nFor example, a popular Go generator is oapi-codegen and supports OpenAPI 3.0. Where a popular Python generator named openapi-python-client can support both OpenAPI 3.0 and 3.1 specifications.\nGenerators can be downloaded and managed as part of the languages tooling. For Go, the oapi-codegen generator is managed with Go modules and invoked with go tool oapi-codegen. With Python, creating a virtual environment, using pip install openapi-python-client, and pip freeze \u0026gt; requirements.txt will work nicely.\nOpenAPI Schema At first it wasn\u0026rsquo;t clear to me on how to get started with OpenAPI or what the benefits were. This is even after reviewing the OpenAPI schema documentation for 3.0.3.\nTo get started one needs to create a specification. A very minimal specification meeting the 3.0.x requirements is listed below. It\u0026rsquo;s not a very interesting example as endpoints in the application server aren\u0026rsquo;t defined, but it shows how minimal a specification can be that meets schema requirements.\nopenapi: \u0026#34;3.0.3\u0026#34; info: version: 1.0.0 title: My Contrived Server paths: Let\u0026rsquo;s get started by extending the simple example defining a path named /status. It will return a 200 response code with a JSON resonse.\npaths: /status: get: responses: \u0026#39;200\u0026#39;: description: Get status of the application server content: application/json: schema: $ref: \u0026#39;#/components/schemas/status\u0026#39; The JSON response is documented in a separate YAML block named components. It defines the response containing a JSON map containing the keys \u0026ldquo;state\u0026rdquo; and \u0026ldquo;message\u0026rdquo;, both of which have a string value.\ncomponents: schemas: status: type: object properties: state: type: string example: \u0026#34;GOOD\u0026#34; message: type: string example: \u0026#34;App running within parameters\u0026#34; OpenAPI supports tags, which let you group related endpoints. This example creates a data grouping and puts create_bucket in the group.\ntags: - name: data description: data manipulation endpoints paths: /create_bucket: post: tags: - data requestBody: required: true content: application/json: schema: $ref: \u0026#39;#/components/schemas/create_bucket\u0026#39; responses: \u0026#39;200\u0026#39;: description: Create a storage object The OpenAPI specification also provides a definition for authentication to the web application.\ncomponents: securitySchemes: basicAuth: type: http scheme: basic description: Endpoints protected by basic auth base64 encoded credentials. paths: /status: get: security: - basicAuth: [] responses: \u0026#39;200\u0026#39;: description: Get status of the application server content: application/json: schema: $ref: \u0026#39;#/components/schemas/status\u0026#39; Earlier I mentioned the generators will create interface files. Declarations which are considered middleware like authentication or logging are out of scope for OpenAPI. In this example, the security entries are there to document that the endpoints require basic authentication.\nGenerate Server Interfaces (Go) The server walkthrough presumes one has both Make and Go installed, and the example code (tar.gz file) has been downloaded and extracted into a temp/work directory.\nDownload the Go dependencies, including oapi-codegen, by running make tidy. Generate the server interfaces by running make server-codegen, which calls go tool oapi-codegen. Feel free to inspect the api/http.gen.go file before proceeding. You\u0026rsquo;ll see it contains an interface named ServerInterface, which has the GetStatus or PostStatus endpoints from the OpenAPI specification. http.gen.go also contains a struct named Status that was defined from components -\u0026gt; schema -\u0026gt; status.\ntype Status struct { Message string `json:\u0026#34;message\u0026#34;` State string `json:\u0026#34;state\u0026#34;` } To see the working application server, run make server-run.\nThe server has Basic Auth enabled with hardcoded credentials. The user is \u0026ldquo;alice\u0026rdquo; and the password \u0026ldquo;mySecretPW\u0026rdquo;. Curl can be used to see the response.\n% curl --basic -u alice:mySecretPW http://localhost:8080/status {\u0026#34;message\u0026#34;:\u0026#34;Initializing\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;Unknown\u0026#34;} Generate Client Interfaces (Python) This is where OpenAPI really shines. I was able to use a generator to create Python libraries to be used by the client implementation code. The walkthrough presumes a recent version of Python3 and pip are installed.\nFirst, create a virtual environment and install the openapi-python-client dependencies. This shell snippet presumes the current working directory is already hello_openapi.\n% python3 -mvenv $PWD/.venv % source $PWD/.venv/bin/activate % pip install -r requirements.txt Then run make client-codegen to build the Python client libraries located in cmd/client/my_contrived_server.\nGenerating the client was easy, but figuring out how to pass authentication took some trial and error. I eventually realized that the token is just a base64-encoded username:password string, and the prefix should be set to Basic.\nclient = AuthenticatedClient( base_url=\u0026#34;http://127.0.0.1:8080\u0026#34;, headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;}, token=\u0026#34;YWxpY2U6bXlTZWNyZXRQVw==\u0026#34;, # Token string is a base64 string containing alice:mySecretPW prefix=\u0026#34;Basic\u0026#34; ) To see the client in action, run make client-run. Also take a look at cmd/client/client.py. It only took a few lines of python code to implement what the openapi-python-client generator had created.\nGotchas \u0026amp; Lessons Learned One issue I have with OpenAPI is the illusion of simplicty. When I first started working with OpenAPI, I noticed the Status struct had keys referencing a pointer of strings which wasn\u0026rsquo;t ideal.\ntype Status struct { Message *string `json:\u0026#34;message\u0026#34;` State *string `json:\u0026#34;state\u0026#34;` } It took some fiddling with the OpenAPI specification to make the generator use strings instead of pointers to strings. Adding \u0026lsquo;required\u0026rsquo; to the schema made the generator do what I wanted.\ncomponents: status: type: object properties: state: type: string example: \u0026#34;GOOD\u0026#34; message: type: string example: \u0026#34;App within parameters\u0026#34; required: - state - message Another issue was not knowing that in Paths, GETs should have a responses entry and POSTS should have a RequestBody entry. It makes sense, but it wasn\u0026rsquo;t obvious to me when stumbling through hello-world.\nThe main takeaway? Always inspect the generated code. If something doesn‚Äôt look right, like unexpected pointers or missing method args, chances are your spec needs tweaking.\nWrapping Up Even though I hit some issues with a fairly simple example, I\u0026rsquo;m going to continue using OpenAPI specifcations. Being able to easily generate client code in a different language was a real win. And let\u0026rsquo;s not forget the free API documentation and contract definitions which comes with OpenAPI. I have a more complex OpenAPI project coming up. I\u0026rsquo;m sure I\u0026rsquo;ll have more notes (and probably more gotchas) to share. Stay tuned.\nIf you\u0026rsquo;ve had similar struggles with OpenAPI or tips for improving schema design, I‚Äôd love to hear them on Bluesky Social.\n","permalink":"https://amf3.github.io/articles/api/hello_openapi/","summary":"\u003cp\u003eOpenAPI is a specification for documenting HTTP APIs for both humans and machines to consume.  As OpenAPI is a specification,\nit is language agnostic. OpenAPI relies on generators for translating the specification.  There\u0026rsquo;s more\nthan just documentation that\u0026rsquo;s generated. Generators also create language-specific interfaces, tooling, and contracts.  In some\nways the OpenAPI pattern reminds me of either protobuf with gRPC or ORM schema-first design.  As a result, a declarative API is\ncreated by the tooling.\u003c/p\u003e","title":"OpenAPI in Practice: Go Server + Python Client from Spec"},{"content":"With Docker, it‚Äôs not always obvious what storage options exist beyond the built-in local volume driver or a traditional bind mount. Exploring Docker volume drivers often turns up archived GitHub repositories or commercially backed plugins tied to specific cloud storage products. The volume ecosystem is especially limited for on-premise storage, and many plugins require more privileges than you\u0026rsquo;d expect.\nIn this post, I‚Äôll cover how Docker handles volume storage under the hood. I‚Äôll also walkthrough how to create a volume plugin that interacts with remote storage without needing CAP_SYS_ADMIN privileges.\nDocker Storage Overview Graph Drivers (also known as Storage Drivers) manage image and container layers. Examples include, overlay2, zfs, or btrfs. Volume Drivers manange named volumes and allow data to persist outside of the container lifecycle. Plugins for Volume Drivers are usually installed as special containers using the docker plugin command. Plugin containers run in their own namespaces and don\u0026rsquo;t behave like normal containers. However, if the plugin includes a shell, it\u0026rsquo;s possible to enter the namespace using runc.\nubuntu@docker-dev:~/work$ sudo runc --root /run/docker/runtime-runc/plugins.moby list ID PID STATUS BUNDLE CREATED OWNER bae595da4deac656921a48f4b3d854992c692777f46db891934310ae863746c1 24503 running /run/containerd/io.containerd.runtime.v2.task/plugins.moby/bae595da4deac656921a48f4b3d854992c692777f46db891934310ae863746c1 2025-06-27T03:55:42.927740463Z root ubuntu@docker-dev:~/work$ sudo runc --root /run/docker/runtime-runc/plugins.moby exec -t bae595da4deac656921a48f4b3d854992c692777f46db891934310ae863746c1 /bin/bash root@docker-dev:/# ls -lh /myplugin -rwxr-xr-x 1 root root 5.0M Jun 24 23:24 /myplugin The Docker daemon communicates with all plugins over HTTP, using either a Unix socket or a TCP socket.\nWhat\u0026rsquo;s Missing While the interface is simple, expectations around what happens after a volume is mounted are not. Most plugins end up mounting a remote filesystem like NFS or CIFS, manipulating files as root, or interacting with device nodes like /dev/fuse, all requiring elevated privileges.\nTo summarize:\nIt‚Äôs difficult to find unprivileged plugins in the Docker ecosystem. Using virtual filesystems (like GlusterFS, SSHFS, or S3FS) requires FUSE, and FUSE needs CAP_SYS_ADMIN. A local caching layer for remote storage is not baked into the Docker volume plugin interface. The DIY Approach Let‚Äôs say we want a volume plugin that ‚Äúmounts‚Äù a remote volume by downloading files from a remote server.\nHere‚Äôs a basic outline of how it might work:\nImplement a volume plugin that exposes the Volume Plugin API to the Docker daemon. Create volume, populates volume metadata needed to later identify the volume within Docker. (The API allows actual filesystem setup to be deferred until Mount.) Mount volume, will fetch data from the remote server, extract it to a known local path, and return that path to Docker. Unmount volume, either cleans up the local path or repackages and uploads any changes back to the remote server. This model avoids the need for root privileges, since it doesn\u0026rsquo;t touch /dev, doesn\u0026rsquo;t rely on FUSE, and doesn‚Äôt call mount(2).\nMy Interest in Docker Plugins I like the simplicity of Docker compared to larger orchestration platforms, but I want more from its storage offerings. When I started looking at existing volume plugins, a few things pushed me toward writing my own.\nMany plugins require root-level privileges. I found that avoiding FUSE or skipping filesystems that depend on kernel modules could reduce or eliminate this requirement.\nAnother issue, most volume plugins on GitHub have been archived by their maintainers. I get it, people move on and the Docker community isn\u0026rsquo;t as large as it once was. That said, I found the developer tooling for writing plugins to be a bit clunky. My hope is this post will help fill in gaps and show a practical path forward for building a volume plugin.\nFinally, most active (not archived) volume plugins I find are designed for cloud storage services. There\u0026rsquo;s a lack of unprivileged lightweight volume plugins and I think there\u0026rsquo;s a place for something simplier.\nStreaming read-only config bundles CI/CD ephemeral volumes Lazy-loading assets over HTTP These ideas are viable with the current plugin API, just unexplored.\nA simple plugin After digging into Docker plugins and exploring the current state of the ecosystem, I decided to build a simple plugin to see how far I could get with minimal privileges and lightweight tooling. I have to admit that the process was both educational and a little frustrating.\nWhat Didn\u0026rsquo;t Go Smoothly Docker has a clean CLI and a solid container runtime, but plugin development comes with its share of friction:\nThe development loop is slow. Building, loading, and enabling a plugin requires several manual steps. Debugging inside the plugin container via runc (instead of docker plugin) isn‚Äôt intuitive. Plugin files need to follow a specific directory structure, and you must include an exported container root filesystem in a subdirectory before the plugin can be built. The Docker daemon uses a socket path that includes the container ID which is a dynamic value. This caused the daemon to time out when connecting to the plugin until I manually fixed the path. Eventually, I discovered the Go plugin SDK, which handled this more reliably. Build Steps Here‚Äôs a high-level overview of the development loop when creating a Docker volume plugin:\nWrite the plugin code. Create a Docker image that contains the plugin. Create a throwaway container from that image. Extract the root filesystem from the container using docker export, and untar it into a directory named rootfs. Finally, run docker plugin create to assemble the plugin from the rootfs and a config.json file. Fortunately, someone wrapped these steps in a Makefile which can be used as a starting point. Here\u0026rsquo;s a snippet from the docker-volume-sshfs repo.\nrootfs: @echo \u0026#34;### docker build: rootfs image with docker-volume-sshfs\u0026#34; @docker build -q -t ${PLUGIN_NAME}:rootfs . @echo \u0026#34;### create rootfs directory in ./plugin/rootfs\u0026#34; @mkdir -p ./plugin/rootfs @docker create --name tmp ${PLUGIN_NAME}:rootfs @docker export tmp | tar -x -C ./plugin/rootfs @echo \u0026#34;### copy config.json to ./plugin/\u0026#34; @cp config.json ./plugin/ @docker rm -vf tmp create: @echo \u0026#34;### remove existing plugin ${PLUGIN_NAME}:${PLUGIN_TAG} if exists\u0026#34; @docker plugin rm -f ${PLUGIN_NAME}:${PLUGIN_TAG} || true @echo \u0026#34;### create new plugin ${PLUGIN_NAME}:${PLUGIN_TAG} from ./plugin\u0026#34; @docker plugin create ${PLUGIN_NAME}:${PLUGIN_TAG} ./plugin You‚Äôll also need a config.json file (docs) that defines the plugin‚Äôs name, entrypoint, socket permissions, and other settings. This file goes alongside the rootfs directory when building the plugin.\nPlugin SDK and API documentation The go-plugins-helpers SDK was a big help when building my plugin, though it\u0026rsquo;s not well advertised. It provides an interface with method definitions for handling the HTTP communication between the custom plugin and the Docker Daemon.\nWhile plugins can technically be written in any language (since the API is just HTTP), this Go SDK was the only official helper library I found. That said, using it is optional. I came across several projects like rclone and SeaweedFS that implement the plugin protocol without relying on the SDK.\nDocker‚Äôs documentation is spread across a few key pages. The two most useful I found were:\nThe Plugin API reference describes the HTTP interface and includes example request and response payloads. The Volume plugin overview includes sections on creating, installing, developing, and debugging plugins. Code Highlights The plugin I built is intentionaly minimal. When a container calls mount on the volume, the function creates a hello.txt file in the volume directory.\nIt simulates downloading data from remote storage while keeping things simple:\nfunc (d *myDriver) Mount(req *volume.MountRequest) (*volume.MountResponse, error) { volPath := filepath.Join(pluginRoot, req.Name) // Write a hello.txt file helloFile := filepath.Join(volPath, \u0026#34;hello.txt\u0026#34;) err := os.WriteFile(helloFile, []byte(\u0026#34;Hello, world!\\n\u0026#34;), 0644) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to write hello.txt: %w\u0026#34;, err) } log.Printf(\u0026#34;Mount volume: %s -\u0026gt; %s\u0026#34;, req.Name, volPath) return \u0026amp;volume.MountResponse{Mountpoint: volPath}, nil } Before a container exits, unmount is called. This function deletes the file from the volume, demonstrating that the basic lifecycle works with user level permissions. This step could be used to sync local changes back to remote storage in production code.\nfunc (d *myDriver) Unmount(req *volume.UnmountRequest) error { volPath := filepath.Join(pluginRoot, req.Name) helloFile := filepath.Join(volPath, \u0026#34;hello.txt\u0026#34;) // Simulate cleanup if err := os.Remove(helloFile); err != nil \u0026amp;\u0026amp; !os.IsNotExist(err) { return fmt.Errorf(\u0026#34;unmount cleanup error: %w\u0026#34;, err) } log.Printf(\u0026#34;Unmount volume: %s (removed hello.txt)\u0026#34;, req.Name) return nil } Build WalkThrough Now for the good part. A full walkthrough of how I built and tested the custom plugin. Here are the files involved:\ngo.mod go.sum myplugin.go Dockerfile config.json Build the Go code and create a plugin image $ docker build -t rootfsimage . [+] Building 6.1s (13/13) FINISHED docker:default =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 461B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/ubuntu:oracular 0.9s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 32.89kB 0.0s =\u0026gt; [builder 1/6] FROM docker.io/library/ubuntu:oracular@sha256:707879280c0bbfe6cbeb3ae1a85b564ea2356b5310a122c225b92cb3d1ed131b 0.0s =\u0026gt; CACHED [builder 2/6] RUN apt-get update \u0026amp;\u0026amp; apt-get install golang-go ca-certificates -y 0.0s =\u0026gt; [builder 3/6] COPY . /build 0.0s =\u0026gt; [builder 4/6] WORKDIR /build 0.0s =\u0026gt; [builder 5/6] RUN go mod tidy 0.7s =\u0026gt; [builder 6/6] RUN CGO_ENABLED=0 go build -ldflags=\u0026#34;-s -w -extldflags \u0026#34;-static\u0026#34;\u0026#34; -tags netgo,osusergo -o myplugin 4.3s =\u0026gt; CACHED [stage-1 2/3] RUN mkdir -p /run/docker/plugins /var/lib/myplugin/volumes 0.0s =\u0026gt; [stage-1 3/3] COPY --from=builder /build/myplugin /myplugin 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:a3d2dee4d6cb8112f538a16056dc42b9761bda43f7f279b2a1f202a7a8e5f8ae 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/rootfsimage Create a container from the rootfs image and export the container‚Äôs root filesystem to a tar file. $ id=$(docker create rootfsimage true) $ echo $id 13cf6219737999bd54f7fc2537bc1218f42d9c45d92f79c4726c1422e81b348e $ sudo docker export \u0026#34;$id\u0026#34; -o rootfs.tar $ ls -l rootfs.tar -rw------- 1 ubuntu ubuntu 110620160 Jun 26 23:07 rootfs.tar Docker plugin tooling expects a rootfs directory and a config.json file in the current directory. The config isn‚Äôt part of the exported filesystem, so it\u0026rsquo;s provided separately: $ cp ~/Downloads/config.json . $ sudo tar -xf ./rootfs.tar -C ./rootfs/ $ ls -l total 8 -rw-r--r-- 1 root root 183 Jun 23 15:26 config.json drwxr-xr-x 17 root root 4096 Jun 24 16:28 rootfs $ ls rootfs bin boot dev etc home lib media mnt myplugin opt proc root run sbin srv sys tmp usr var Create and enable the plugin. $ sudo docker plugin create myplugin . myplugin $ docker plugin enable myplugin:latest myplugin:latest $ docker plugin ls ID NAME DESCRIPTION ENABLED d5f63b80f0b0 myplugin:latest Example HTTP-backed volume plugin true Create a volume with the plugin and inspect it. $ docker volume create -d myplugin:latest abc123 abc123 $ docker volume ls DRIVER VOLUME NAME myplugin:latest abc123 $ docker volume inspect abc123 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;0001-01-01T00:00:00Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;myplugin:latest\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/myplugin/volumes/abc123\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;abc123\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] Mount the volume with a container. $ docker run -it --rm -v abc123:/mnt alpine / # ls -l /mnt total 4 -rw-r--r-- 1 root root 14 Jun 27 06:17 hello.txt / # cat /mnt/hello.txt Hello, world! Additional information for Docker events can be found inside systemd logs. $ journalctl -u docker.service | tail -3 Jun 26 23:17:55 docker-dev dockerd[7976]: time=\u0026#34;2025-06-26T23:17:55-07:00\u0026#34; level=error msg=\u0026#34;2025/06/27 06:17:55 Mount volume: abc123 -\u0026gt; /var/lib/myplugin/volumes/abc123\u0026#34; plugin=d5f63b80f0b091582e00bfed7bd6d33e885fef876e151558a37ae5eaf05e5443 Jun 26 23:18:04 docker-dev dockerd[7976]: time=\u0026#34;2025-06-26T23:18:04.543532903-07:00\u0026#34; level=info msg=\u0026#34;ignoring event\u0026#34; container=0e6fdf10db3da9c9b5bc04bbe6abac6b23225578e796e89fe859c1d17fc57f0f module=libcontainerd namespace=moby topic=/tasks/delete type=\u0026#34;*events.TaskDelete\u0026#34; Jun 26 23:18:04 docker-dev dockerd[7976]: time=\u0026#34;2025-06-26T23:18:04-07:00\u0026#34; level=error msg=\u0026#34;2025/06/27 06:18:04 Unmount volume: abc123 (removed hello.txt)\u0026#34; plugin=d5f63b80f0b091582e00bfed7bd6d33e885fef876e151558a37ae5eaf05e5443 Closing I\u0026rsquo;m excited about learning how to create plugins for Docker and turning my notes into this post.\nI made a previous post on using tar archives as an object store and I recently posted about compression ratios on Bluesky social. Stay tuned to find out where these posts are headed.\nI was curious how snappy compared to lz4 and wrote a go program to find out. Snappy seems to be a bit better with both compressed data output and resource usage. Input is the first 1,000,000 numbers of pi.\n\u0026mdash; Adam Faris (@af9.us) 2025-06-18T07:40:31.631Z If you\u0026rsquo;ver ever built a Docker plugin or struggled with Docker storage, I\u0026rsquo;d like to hear about it. Any questions or ideas, reach out and leave a comment.\nUntil next time, keep your volumes clean and your containers stateless.\n","permalink":"https://amf3.github.io/articles/storage/docker_volumes/","summary":"\u003cp\u003eWith Docker, it‚Äôs not always obvious what storage options exist beyond the built-in \u003cstrong\u003elocal\u003c/strong\u003e volume driver or a traditional \u003cstrong\u003ebind mount\u003c/strong\u003e.\nExploring Docker volume drivers often turns up archived GitHub repositories or commercially backed plugins tied to specific cloud storage products. The volume\necosystem is especially limited for on-premise storage, and many plugins require more privileges than you\u0026rsquo;d expect.\u003c/p\u003e\n\u003cp\u003eIn this post, I‚Äôll cover how Docker handles volume storage under the hood. I‚Äôll also walkthrough how to create a volume plugin that interacts with remote\nstorage without needing CAP_SYS_ADMIN privileges.\u003c/p\u003e","title":"DIY Docker Volume Drivers: What's Missing"},{"content":"I\u0026rsquo;m looking at how object storage systems manage data on disk. Especially the idea of using append only archives with an index for fast retrieveal. While reading Facebook\u0026rsquo;s Haystack design, I noticed similarities to the tar file format and the potential to implement something similar at the local scale.\nHaystack Overview There are several components mentioned in the original Haystack paper, but at the core is the Haystack Store, where end user image files are physically kept. Instead of writing files directly to the filesystem, images are appended to a large file called a volume, which acts as an append-only archive. Each volume is typically capped at around 100‚ÄØGB and is aligned to 8-byte offsets. Image files within this volume are referred to as needles.\nA volume begins with a superblock (the paper doesn‚Äôt describe this in detail), followed by the header for the first needle (file). Each needle within the volume has its own header, containing metadata like file size, checksums, and flags. The flags field includes a bit to indicate deletion status.\nSince the volume is append-only, deletions don‚Äôt reclaim space‚Äîthey\u0026rsquo;re simply marked as deleted in the needle‚Äôs header. A background process can later compact the volume if needed. To keep track of where each needle is within the file, an in-memory index maps file IDs to byte offsets.\nWhen a read request comes in, the Haystack Store performs a direct seek to the needle‚Äôs offset, verifies the flags to check if it\u0026rsquo;s deleted, and returns the data if is not tombstoned. Deletions update both the in-memory index and the needle‚Äôs header to mark the entry as removed.\nThis model provides two big wins:\nStorage efficiency: Small files, like 1‚ÄØKB thumbnails, don‚Äôt waste space the way they would on a traditional filesystem with 4‚ÄØKB blocks. Instead of allocating a full block per file, they\u0026rsquo;re packed into a shared archive. Fast retrieval: There‚Äôs no need to scan directory structures or fetch inode metadata. With an open file handle to the volume and an in-memory index, reads are just a seek and a read. Tar Storage The tape archive format (tar) is surprisingly similar to the Haystack volume. While tar files don‚Äôt implement a superblock, each file entry is stored at a 512-byte aligned offset, and each file includes its own metadata header. This format allows us to calculate the offset of each file within the archive.\nHere‚Äôs a hexdump of a simple test.tar archive containing two files: a.txt and b.txt.\nIn this example:\na.txt contains the string \u0026ldquo;foo\\n\u0026rdquo;, and b.txt contains \u0026ldquo;bar\\n\u0026rdquo;. Each file is preceded by a 512-byte header containing metadata like filename, permissions, and ownership. Since a.txt is only 4 bytes long, it‚Äôs followed by null padding to align the next file (b.txt) to the 512-byte boundary. The offset for b.txt starts at 0x400 (1024 bytes), which is a clean 512-byte multiple. Although tar uses more padding than Haystack (which aligns to 8-byte offsets), its fixed alignment still enables efficient offset tracking and data retrieval. Once the byte offsets of each file are known, accessing a file is just a matter of seeking to the right position and reading the data.\nTar also provides nice recovery properties:\nAn index of offsets can always be created by reading the tar file and recording the header positions as offsets. Because this is a standard tar file, common tools like tar and cpio can extract the objects directly without the need for custom tooling. Python Prototype Tar archives are typically read sequentially from start to finish. But if we build an index of byte offsets, we can enable random access to individual files. Let‚Äôs explore this with a prototype in Python using the test.tar archive shown in the earlier hexdump. A copy of the archive can be downloaded from here.\nWe have two options for building this prototype:\nThe hard way, by manually parsing byte offsets directly from the tar header. The batteries-included way, using Python‚Äôs built-in tarfile module to extract header information cleanly. If you‚Äôre curious, fields and byte-offsets within file headers are listed in GNU\u0026rsquo;s tar header definition.\nHere‚Äôs an example of the batteries-included approach using the tarfile module. I‚Äôll scan the archive, read each file‚Äôs size and data offset, and store that in a dictionary:\n#!/usr/bin/env python3 import math import tarfile from collections import defaultdict from typing import Dict ARCHIVE_FILE = \u0026#34;test.tar\u0026#34; BYTE_ALIGNMENT = 512 def read_header(archive: str) -\u0026gt; Dict: entities = defaultdict(list) header_offset = 0 with open(archive, \u0026#39;rb\u0026#39;) as f: while True: f.seek(header_offset) header = f.read(BYTE_ALIGNMENT) if header == b\u0026#39;\\0\u0026#39; * BYTE_ALIGNMENT: break # End of archive, trailer will contain two 512-byte blocks of zeros try: tarinfo = tarfile.TarInfo.frombuf(header, encoding=\u0026#34;utf-8\u0026#34;, errors=\u0026#34;surrogateescape\u0026#34;) file_name = tarinfo.name file_size = tarinfo.size data_offset = header_offset + BYTE_ALIGNMENT entities[file_name].append([file_size, data_offset]) except Exception as e: print(f\u0026#34;Error parsing header at offset {header_offset}: {e}\u0026#34;) break padding = math.ceil(file_size / BYTE_ALIGNMENT) * BYTE_ALIGNMENT header_offset += BYTE_ALIGNMENT + padding return entities tar_data = read_header(ARCHIVE_FILE) for file_name, attributes in tar_data.items(): for attribute in attributes: print(f\u0026#34;filename: {file_name:\u0026lt;10} attributes: file_size: {attribute[0]:\u0026lt;6} data_offset: {attribute[1]:\u0026lt;6}\u0026#34;) Example output.\n% python offsets.py filename: a.txt attributes: file_size: 4 data_offset: 512 filename: a.txt attributes: file_size: 13 data_offset: 2560 filename: b.txt attributes: file_size: 4 data_offset: 1536 Notice that a.txt appears twice, each with a different file size and offset. This is expected. It‚Äôs possible to append files to a tar archive using tar -rf. When a file is re-added, it becomes the newer version.\nIn our example archive file, a.txt was modified and appended, producing two versions in the archive. Traditional tar extraction reads from the beginning and overwrites earlier entries as it encounters newer ones. But by having an index of offsets, I can seek directly to either version and extract it manually.\nHere‚Äôs a helper function to extract a specific version of a file:\ndef extract_file(archive: str, file_name: str, offset: int, read_bytes: int): try: with open(archive, \u0026#39;rb\u0026#39;) as f: f.seek(offset) data = f.read(read_bytes) with open(f\u0026#34;{file_name}@{offset:08x}\u0026#34;, \u0026#39;wb\u0026#39;) as out: out.write(data) except Exception as e: print(f\u0026#34;Error extracting {file_name} at offset: {offset:08x}\u0026#34;) Add the following lines in main to extract both versions of a.txt:\nextract_file(ARCHIVE_FILE, \u0026#34;a.txt\u0026#34;, 512, 4) extract_file(ARCHIVE_FILE, \u0026#34;a.txt\u0026#34;, 2560, 13) And the result:\n% ls -latr a.txt@* -rw-r--r--@ 1 adam staff 4 Jun 6 22:07 a.txt@00000200 -rw-r--r--@ 1 adam staff 13 Jun 6 22:07 a.txt@00000a00 % cat a.txt@00000200 foo % cat a.txt@00000a00 foo fooooooo This demonstrates simple object versioning using nothing more than tar‚Äôs existing append behavior and a bit of byte-level introspection.\nTrade-Offs and Limitations As with Haystack, there\u0026rsquo;s not an efficient way to delete content from a tar archive without rewriting the entire file. Instead, deletion requires marking entries as removed in the offsets database. Unlike Haystack which has explicit flags in its header, tar headers offer no such field. Meaning if we lose the index, we can no longer distinguish active content from deleted entries by scanning the archive.\nThe data removal limitation also contributes to archive fragmentation. Until a process rewrites the archive to remove tombstoned data, deleted files remain in place, consuming storage.\nAnother trade-off lies in tar\u0026rsquo;s alignment strategy, both headers and data are aligned to 512-byte blocks. In typical usage, tar archives are compressed, which minimizes the overhead of null padding. But for this design to support random access, the archive must remain uncompressed. Filesystems like ZFS and Btrfs can apply transparent compression at the block level, but relying on underlying filesystem isn\u0026rsquo;t ideal for portability. Haystack uses 8-byte alignment, which results in less padding and more efficient use of space.\nAlso worth noting, my prototype doesn‚Äôt implement any kind of write locking. If this were used in a concurrent setting like a web application storing assets, appends would require locking the archive to prevent corruption.\nFuture Opportunities Sharding across multiple archive files per bucket (directory) would be one enhancement. It would allow for round-robin writes with multiple appenders, improving concurrency. Using multiple archive files per bucket also provides a mechanism to cap archive file sizes.\nA mechanism for tombstoning files within an archive is also needed. As seen in the earlier hexdump, it might be possible to repurpose an existing header field to mark content as deleted. This would allow the offsets database to be reconstructed later, even after a crash or loss of metadata. Another idea is to write custom metadata into the unused space within the 512-byte header block. Whether this breaks compatibility with standard tar utilities remains an open question.\nCompression and encryption are also worth exploring. Because the prototype seeks directly to file offsets and reads raw byte ranges, it‚Äôs feasible to compress file content before appending it to the archive. Retrieval would involve decompressing on the fly after seeking to the file location within the archive. Similarly, data-at-rest encryption could be supported by encrypting file contents during the write path and decrypting during reads. This allows per-object confidentiality without relying on full-disk encryption or underlying filesystem support.\nFinal Thoughts It\u0026rsquo;s oddly satisfying to bend old standards to new purposes, like using the tar format as the basis of an object store. Putting this post together has been a reminder on the types of challenges distributed file systems create when separating metadata from the data. Simple things like marking a file as deleted become complicated.\nLet me know if this topic is interesting or you have follow-up suggestions. I can be reached at Bluesky.\n","permalink":"https://amf3.github.io/articles/storage/tar_objectstore/","summary":"\u003cp\u003eI\u0026rsquo;m looking at how object storage systems manage data on disk. Especially the idea of using append only archives with an index for fast retrieveal.  While reading\nFacebook\u0026rsquo;s Haystack design, I noticed similarities to the tar file format and the potential to implement something similar at the local scale.\u003c/p\u003e\n\u003ch2 id=\"haystack-overview\"\u003eHaystack Overview\u003c/h2\u003e\n\u003cp\u003eThere are several components mentioned in the original \u003ca href=\"https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf\"\u003eHaystack paper\u003c/a\u003e, but at\nthe core is the Haystack Store, where end user image files are physically kept. Instead of writing files directly to the filesystem, images are appended\nto a large file called a \u003cstrong\u003evolume\u003c/strong\u003e, which acts as an append-only archive. Each volume is typically capped at around 100‚ÄØGB and is aligned to 8-byte\noffsets.  Image files within this volume are referred to as \u003cstrong\u003eneedles\u003c/strong\u003e.\u003c/p\u003e","title":"Using Tar Files as Object Store Storage"},{"content":"UTM and Multipass are great apps for virtualization on macOS.\nBut I wanted a lighter-weight approach by invoking QEMU directly. Which meant I needed to understand how QEMU\u0026rsquo;s networking options interact with the vmnet virtualization API on macOS.\nThis becomes especially important when dealing with VM-to-VM connections, network isolation, and bridging on macOS.\nIn this post, I\u0026rsquo;ll walk through creating a simple QEMU-based networking lab.\nSet up RouterOS and Alpine Linux VMs using QEMU on macOS Connect VMs with Apple\u0026rsquo;s Hypervisor vmnet networking APIs Use unified logging to troubleshoot QEMU network issues on macOS Lab Setup Overview The network diagram shows the network topology used in this lab. Both VMs run on on the same macOS host and connected to virtual network interfaces using QEMU\u0026rsquo;s support for Apple\u0026rsquo;s vmnet virtualization API.\nThe RouterOS VM has two virtual network interfaces, which allows it to route traffic between the Alpine Linux VM and the physical local area network.\nQEMU Networking on macOS Let\u0026rsquo;s review the vmnet.shared and vmnet.host labels in the Network1 and Network2 boxes.\nvmnet.shared: Allows traffic from the VM to reach the Internet using a built-in network address translation (NAT) feature. This is similar to how UTM‚Äôs \u0026ldquo;shared network\u0026rdquo; mode works.\nvmnet.host: Traffic can only reach the macOS host and other VMs on the same host-mode network. This mode does not provide Internet access.\nSince the Alpine Linux VM is only connected to the vmnet.host network, and that network cannot reach the Internet, we know the RouterOS VM must be acting as the gateway. It routes traffic between Network2 (host-only) and Network1 (shared). You can confirm this by watching interface packet counts on RouterOS.\nTriple NAT! As a fun side note, traffic from the Alpine VM to the Internet passes through three layers of NAT:\nRouterOS VM NAT: Alpine‚Äôs traffic is translated as it passes through RouterOS (ether2 ‚Üí ether1). macOS vmnet NAT: vmnet0 (shared mode) applies another layer of NAT as it exits to the host‚Äôs physical LAN. Physical Router NAT: Finally, the home router applies NAT before sending packets to the Internet. Other QEMU Network Backends To see a complete list of network backends supported by QEMU:\n% qemu-system-x86_64 -netdev help Available netdev backend types: socket stream dgram hubport tap user vde bridge vhost-user vmnet-host vmnet-shared A few notes about QEMU network backends:\nsocket and user: Built into QEMU and don‚Äôt require elevated privileges. Great for quick VMs, but they don‚Äôt behave like traditional network bridges. You can‚Äôt easily interconnect multiple VMs.\ntap: Closer to a traditional bridged network and supports full traffic pass-through. However, it requires setup outside of QEMU and isn‚Äôt available on macOS, since tap interfaces depend on kernel extensions (which Apple no longer supports).\nvmnet: The backend is macOS-native and works out of the box with Apple‚Äôs Hypervisor Framework. It lets QEMU manage the bridge interfaces directly so no extra tooling is needed. Win!\nCreating the VMs RouterOS VM RouterOS \u0026ldquo;Cloud Hosted Router\u0026rdquo; (CHR) is a commercial product with a \u0026ldquo;free to use\u0026rdquo; license that limits upload speed to 1 Mbps. While a paid license is available to remove the upload limit, the restriction doesn\u0026rsquo;t prevent me from validating changes before deploying them to physical networks.\nOne can download the CHR image from MikroTik‚Äôs download page. I used the stable 7.x version and chose the Raw disk image ‚Äî which is x86 (not ARM).\nüí° In hindsight, the ARM image might be more appropriate for Apple Silicon, but the x86 image works fine.\nFirst, convert the raw image to qcow2 format. This allows snapshotting the VM, making it easy to roll back from bad config changes.\n% qemu-img convert -f raw -O qcow2 chr-7.18.2.img chr-7.18.2.qcow2 % qemu-img snapshot -c original_image chr-7.18.2.qcow2 % qemu-img info chr-7.18.2.qcow2 image: chr-7.18.2.qcow2 file format: qcow2 virtual size: 128 MiB (134217728 bytes) disk size: 44.2 MiB cluster_size: 65536 Snapshot list: ID TAG VM_SIZE DATE VM_CLOCK ICOUNT 1 original_image 0 B 2025-05-08 22:40:36 0000:00:00.000 0 Format specific information: compat: 1.1 compression type: zlib lazy refcounts: false refcount bits: 16 corrupt: false extended l2: false Child node \u0026#39;/file\u0026#39;: filename: chr-7.18.2.qcow2 protocol type: file file length: 44.2 MiB (46333952 bytes) disk size: 44.2 MiB Now, start the RouterOS VM and create the two virtual networks with QEMU.\nsudo qemu-system-x86_64 -m 2048 -smp cpus=4 -serial mon:stdio \\ -device virtio-scsi-pci,id=scsi0 \\ -drive file=./chr-7.18.2.qcow2,if=none,format=qcow2,discard=unmap,id=hda \\ -device scsi-hd,drive=hda,bus=scsi0.0 \\ -device virtio-net-pci,netdev=net1 \\ -netdev vmnet-shared,id=net1,start-address=172.16.0.1,end-address=172.31.255.254,subnet-mask=255.240.0.0 \\ -device virtio-net-pci,netdev=net2 \\ -netdev vmnet-host,id=net2,start-address=192.168.2.1,end-address=192.168.2.254,subnet-mask=255.255.255.0,net-uuid=154780B0-F499-4968-9B20-E58C02FDF5FB Uses sudo to create vmnet interfaces (required on macOS). Allocates 2 GB of RAM and 4 vCPUs. Opens a serial console in the terminal (handy for copy/paste). Attaches two network devices: vmnet-shared for simulated external Internet. vmnet-host for internal traffic (private LAN). IP ranges must follow RFC 1918 allocation. Using net-uuid disables the macOS DHCP server for the vmnet-host network. Required as we want the RouterOS VM to respond with DHCP replies for vmnet-host traffic. Generate the UUID with /usr/bin/uuidgen. Once RouterOS boots, log in with username admin and press Enter for a blank password. You‚Äôll be prompted to set a new one.\nTo list interfaces:\n[admin@MikroTik] \u0026gt; /interface print Flags: R - RUNNING Columns: NAME, TYPE, ACTUAL-MTU, MAC-ADDRESS # NAME TYPE ACTUAL-MTU MAC-ADDRESS 0 R ether1 ether 1500 52:54:00:12:34:56 1 R ether2 ether 1500 52:54:00:12:34:57 2 R lo loopback 65536 00:00:00:00:00:00 To check assigned IPs:\n[admin@MikroTik] \u0026gt; /ip/address print Flags: D - DYNAMIC Columns: ADDRESS, NETWORK, INTERFACE # ADDRESS NETWORK INTERFACE 0 D 172.16.0.2/12 172.16.0.0 ether1 Only one IP is listed ‚Äî why? The vmnet-shared interface (ether1) has DHCP enabled by Apple‚Äôs Hypervisor framework. RouterOS sends a DHCP request and gets an IP, similar to how a home router works. Meanwhile, vmnet-host has DHCP disabled, so we must assign a static IP to ether2 on the router.\nMinimal Configuration Steps Here are the minimum configuration steps to route traffic:\nassign a static IP on ether2 create a dhcpd server enable NAT [admin@MikroTik] \u0026gt; /ip address add address=192.168.2.1/24 interface=ether2 network=192.168.2.0 [admin@MikroTik] \u0026gt; /ip pool add name=dhcp ranges=192.168.2.50-192.168.2.100 [admin@MikroTik] \u0026gt; /ip dhcp-server add address-pool=dhcp interface=ether2 lease-time=1h name=defconf [admin@MikroTik] \u0026gt; /ip dhcp-server network add address=192.168.2.0/24 comment=defconf dns-server=172.16.0.1,1.1.1.1 gateway=192.168.2.1 [admin@MikroTik] \u0026gt; /ip firewall nat add action=masquerade chain=srcnat out-interface=ether1 ‚ö†Ô∏è The example does not set any firewall rules. Use it as a starting point only.\nTo gracefully shutdown the router\n[admin@MikroTik] \u0026gt; /system shutdown Answer y when prompted. Or, leave the router running. It will be used again shortly.\nAlpine Linux VM An Alpine ISO needs to be downloaded and installed onto a virtual hard disk. I recommend using the user network mentioned earlier for the install as additional packages will need to be downloaded from the Internet. The standard x86_64 image can be retrieved from the Alpine Linux downloads page.\nCreate a disk image to install the OS to with the qemu-img command. The options will use the qcow2 format with a max size of 2GB.\n% qemu-img create -f qcow2 alpine_disk.qcow2 2G Next step is to start a VM that boots from the Alpine ISO and connects to the Internet with the user network. Because versions change, be sure to replace the ISO filename in the -cdrom option with the one that was downloaded.\n% qemu-system-x86_64 -m 2048 -smp cpus=4 -serial stdio \\ -boot once=d \\ -cdrom ./alpine-standard-3.21.2-x86_64.iso \\ -hda ./alpine_disk.qcow2 \\ -net nic,model=virtio -net user Once the VM has started, login as \u0026ldquo;root\u0026rdquo; and hit Enter for the empty password. Next run setup-alpine and follow the prompts. Here are suggested answers to some of the prompts:\nSelect dhcp for eth0. Choose chrony as the network time server. Accept the default of 1 when asked which \u0026ldquo;apk-mirror\u0026rdquo; to use. When prompted about the install disk, select sda. Answer sys to the \u0026ldquo;how would you like to use it\u0026rdquo; question. When the installation script is complete, type reboot and use the new root password set during the install. With the -boot once=d option, the VM will skip the ISO and boot directly from the newly installed virtual disk.\nLog in as root and install the dig and curl commands.\n# apk add bind-tools curl ca-certificates When the package install has completed, gracefully shutdown the VM with poweroff command.\nTesting the NAT Setup Check that the RouterOS VM is still running in the other terminal. It\u0026rsquo;s acting as the NAT gateway for the Alpine VM and must be active for Internet access to work. Then connect the new Alpine Linux VM to Network2 (vmnet-host) with this QEMU command.\n% sudo qemu-system-x86_64 -m 2048 -smp cpus=4 -serial mon:stdio \\ -boot c \\ -hda alpine_disk.qcow2 \\ -device virtio-net-pci,netdev=net2 \\ -netdev vmnet-host,id=net2,start-address=192.168.2.1,end-address=192.168.2.254,subnet-mask=255.255.255.0,net-uuid=154780B0-F499-4968-9B20-E58C02FDF5FB Log into the Alpine VM and verify it can reach the Internet.\nmyvm:~$ ip addr show eth0 # Confirm the IP is in the 192.168.2.x network range ... inet 192.168.2.100/24 scope global eth0 ... myvm:~$ ip route show # Confirm the default route is 192.168.2.1 default via 192.168.2.1 dev eth0 metric 202 myvm:~$ cat /etc/resolv.conf # Confirm the DNS servers were set nameserver 172.16.0.1 nameserver 1.1.1.1 myvm:~$ ping -qc 3 1.1.1.1 # test ping to 1.1.1.1 on the Internet PING 1.1.1.1 (1.1.1.1): 56 data bytes --- 1.1.1.1 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss ... myvm:~$ dig @172.16.0.1 -t a +short www.github.com # test vmnet name resolution works github.com. 140.82.116.4 myvm:~$ curl -I https://www.github.com # test that I can fetch a webpage HTTP/2 301 ... If all of the above checks pass, your Alpine VM is correctly NAT\u0026rsquo;d through the RouterOS VM and can reach the Internet.\nTroubleshooting \u0026amp; Gotchas Debugging with Unified Logging macOS logs a large volume of network-related events, and it can be tricky to isolate the relevant ones. Fortunately, the log show and log collect tools make it easier to filter and investigate.\nStart by capturing a snapshot of system logs around the time your VMs are active:\n% mkdir ./log_archive % ./start_lab # start the VMs with a shell script % sudo log collect --output ./log_archive --last 3m # capture the previous 3 minutes of log events. This captures the previous 3 minutes of logs and stores them in a binary archive. Then query the relevant subsystem:\n% log show --archive log_archive/system_logs.logarchive --predicate \u0026#39;subsystem == \u0026#34;com.apple.NetworkSharing\u0026#34;\u0026#39; I found the subsystem == \u0026quot;com.apple.NetworkSharing\u0026quot; query provided the most useful results. Using other filters like subsystem == \u0026quot;com.apple.network\u0026quot; or process == \u0026quot;qemu-system-x86_64\u0026quot; included many unrelated events and were a little overwhelming. This screenshot shows the networks being created when the VMs are started and then torn down when the VMs are stopped.\nPacket Captures with Wireshark or tcpdump Because QEMU uses virtual network interfaces, it\u0026rsquo;s possible to monitor VM traffic directly from macOS using tools like tcpdump or Wireshark. Use the ifconfig command to identify which vmenet interfaces are active and which bridge they are mapped to.\nUsing Wireshark on the correct interface reveals detailed traffic flows. In this example capture, we see:\nICMP ping requests to the router‚Äôs external interface A DNS lookup for speedtest.net An HTTP GET request initiating the speed test Other quirks I wrote a simple C program to list the interfaces created by QEMU but ran into permission errors. It turns out that macOS protects the vmnet APIs behind System Integrity Protection (SIP). To access them, binaries must be code-signed with a full Apple Developer certificate, which I don‚Äôt have (and didn‚Äôt want to pay for). Fortunately, macOS‚Äôs unified logging system provided helpful insight.\nAnother issue I encountered was with subnet settings in the router configs. At one point, I accidentally assigned 192.168.2.1/32 instead of 192.168.2.1/24 to the host-only interface on the RouterOS VM. This broke routing for the two VMs, blocking the Alpine VM from reaching the Internet.\nAt first glance QEMU‚Äôs socket-based networking may seem like a good replacement for bridged networking. It‚Äôs built into QEMU and doesn‚Äôt require elevated privileges. So why not use it? It turns out that socket networking is intended for point-to-point communication between specific QEMU instances, like TCP client/server setups. Socket networking doesn‚Äôt support general Ethernet bridging or broadcast traffic. For a more flexible setup that allows multiple VMs to communicate freely (and with the host), you still need tap or vmnet.\nConclusion Running QEMU directly on macOS isn‚Äôt the most beginner-friendly experience, but it was a great learning opportunity. I have a better appreciation for why tools like UTM or Multipass exist as wrappers around QEMU.\nNext time I might try replacing the x86 images with arm64 images to explore the performance differences. I\u0026rsquo;m also considering writing my own QEMU wrapper, partly for fun and partly for continuious integration purposes.\nIf you enjoyed this or want to follow along with future experiments, follow me on Bluesky. Thanks for reading!\n","permalink":"https://amf3.github.io/articles/virtualization/macos_qemu_networks/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/utmapp/UTM\"\u003eUTM\u003c/a\u003e and \u003ca href=\"https://canonical.com/multipass\"\u003eMultipass\u003c/a\u003e are great apps for virtualization on macOS.\u003cbr\u003e\nBut I wanted a lighter-weight approach by invoking QEMU directly.  Which meant I needed to understand how QEMU\u0026rsquo;s networking options interact\nwith the \u003ccode\u003evmnet\u003c/code\u003e virtualization API on macOS.\u003c/p\u003e\n\u003cp\u003eThis becomes especially important when dealing with VM-to-VM connections, network isolation, and bridging on macOS.\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll walk through creating a simple QEMU-based networking lab.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSet up RouterOS and Alpine Linux VMs using QEMU on macOS\u003c/li\u003e\n\u003cli\u003eConnect VMs with Apple\u0026rsquo;s Hypervisor \u003ccode\u003evmnet\u003c/code\u003e networking APIs\u003c/li\u003e\n\u003cli\u003eUse unified logging to troubleshoot QEMU network issues on macOS\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lab-setup-overview\"\u003eLab Setup Overview\u003c/h2\u003e\n\u003cp\u003eThe network diagram shows the network topology used in this lab.  Both VMs run on on the\nsame macOS host and connected to virtual network interfaces using QEMU\u0026rsquo;s support for Apple\u0026rsquo;s \u003cstrong\u003evmnet\u003c/strong\u003e virtualization API.\u003c/p\u003e","title":"Virtual Router Lab on macOS with QEMU"},{"content":"QEMU uses files to emulate storage devices, and the features available depend on how those files are created. While QEMU can emulate disks from Parallels and VirtualBox, I‚Äôm going to focus on the formats most commonly used in automation and scripting, raw and qcow2.\nThe default format is raw and raw offers the fewest features. It\u0026rsquo;s just plain storage. The other format qcow2 supports compression, snapshots, and copy-on-write in addition to storage.\nRaw Format Creating a raw disk with qemu-img is similar to using dd to create a block-based file. One can see this with the output of qemu-img info.\nHere I create two storage devices, one with qemu-img which defaults to the raw format and another with the dd command.\n% qemu-img create my_disk.img +1m Formatting \u0026#39;my_disk.img\u0026#39;, fmt=raw size=1048576 % dd if=/dev/zero of=my_block.file count=1 bs=1m 1+0 records in 1+0 records out Now let\u0026rsquo;s use qemu-img info to confirm there\u0026rsquo;s no difference between the two files.\n% qemu-img info my_disk.img image: my_disk.img file format: raw virtual size: 1 MiB (1048576 bytes) disk size: 1 MiB Child node \u0026#39;/file\u0026#39;: filename: my_disk.img protocol type: file file length: 1 MiB (1048576 bytes) disk size: 1 MiB % qemu-img info my_block.file image: my_block.file file format: raw virtual size: 1 MiB (1048576 bytes) disk size: 1 MiB Child node \u0026#39;/file\u0026#39;: filename: my_block.file protocol type: file file length: 1 MiB (1048576 bytes) disk size: 1 MiB Qcow2 Format Creating a disk in qcow2 format enables zlib compression by default.\n% qemu-img create -f qcow2 my_disk.img 1M Formatting \u0026#39;my_disk.img\u0026#39;, fmt=qcow2 cluster_size=65536 extended_l2=off compression_type=zlib size=1048576 lazy_refcounts=off refcount_bits=16 % qemu-img info my_disk.img image: my_disk.img file format: qcow2 virtual size: 1 MiB (1048576 bytes) disk size: 196 KiB cluster_size: 65536 Format specific information: compat: 1.1 compression type: zlib lazy refcounts: false refcount bits: 16 corrupt: false extended l2: false Child node \u0026#39;/file\u0026#39;: filename: my_disk.img protocol type: file file length: 192 KiB (197120 bytes) disk size: 196 KiB Tip One - Resize an image file It\u0026rsquo;s possible to grow or shrink a QEMU storage device. Think of this as expanding the physical SSD itself, not the filesystem that sits on it. Important, when shrinking a image with negative values, always shrink the filesystem first using resize2fs before running qemu-img resize or risk data corruption.\n% qemu-img resize my_disk.img +1m Image resized. When inspecting the new disk image, we see the new capacity is 2MB but the file size on disk is under 200KB. This is because qcow2 supports copy-on-write and compression.\n% qemu-img info my_disk.img image: my_disk.img file format: qcow2 virtual size: 2 MiB (2097152 bytes) disk size: 196 KiB cluster_size: 65536 Format specific information: compat: 1.1 compression type: zlib lazy refcounts: false refcount bits: 16 corrupt: false extended l2: false Child node \u0026#39;/file\u0026#39;: filename: my_disk.img protocol type: file file length: 192 KiB (197120 bytes) disk size: 196 KiB % ls -lh my_disk.img -rw-r--r-- 1 adam staff 192K Apr 6 10:19 my_disk.img If I were to resize a QEMU storage file formatted as raw, the file size on disk of 2MB matches the image capacity of 2MB as raw doesn\u0026rsquo;t support compression or copy-on-write.\n% qemu-img create raw_disk.img +2m Formatting \u0026#39;raw_disk.img\u0026#39;, fmt=raw size=2097152 % ls -lh raw_disk.img -rw-r--r-- 1 adam staff 2.0M Apr 6 10:22 raw_disk.img Tip Two - Snapshots Snapshots are supported with qcow2 devices. These are handy for creating a base disk image that\u0026rsquo;s shareable and later modified for other purposes. Snapshots also make a great backup point before making image changes.\nTo create a snapshot named \u0026ldquo;my_first_snapshot\u0026rdquo; inside an existing qcow2 image.\n% qemu-img snapshot -c my_first_snapshot my_disk.img To list existing snapshots\n% qemu-img snapshot -l my_disk.img Snapshot list: ID TAG VM_SIZE DATE VM_CLOCK ICOUNT 1 my_first_snapshot 0 B 2025-04-06 10:37:07 0000:00:00.000 0 To revert or \u0026ldquo;apply\u0026rdquo; a snapshot\n% qemu-img snapshot -a my_first_snapshot my_disk.img To delete a snapshot from a file\n% qemu-img snapshot -d my_first_snapshot my_disk.img Tip Three - Modify a QEMU image While many online guides suggest using the Network Block Device (NBD) kernel driver in Linux to mount and modify QEMU images, I use a different process that also works on MacOS. My preferred method is to boot a VM using QEMU and attaching the image as a data drive.\nThis example uses the extended x86_64 Alpine Linux ISO and a QEMU command that mounts the image as a data drive. The Alpine extended ISO lets you log in as root with an empty password, which makes quick edits easy.\n#/bin/sh qemu-system-x86_64 \\ -m 2G -smp cpus=4 -serial stdio \\ -boot once=d \\ -drive file=./my_disk.img,format=qcow2,media=disk,cache=unsafe \\ -drive file=./alpine-extended-3.21.2-x86_64.iso,format=raw,media=cdrom \\ -nic user,model=virtio-net-pci,hostfwd=tcp::2222-:22 Once logged in, you\u0026rsquo;ll see the QEMU file we want to modify listed as /dev/sda. The device hasn\u0026rsquo;t been formatted with a filesystem, but if one were present it could be mounted within the VM, files edited within the image, and then unmounted.\nTip Four - Transfer a QEMU image to bare-metal It\u0026rsquo;s possible to use a QEMU image with bare-metal by converting it to raw format. Use the following to convert the image from qcow2 to raw.\n% qemu-img convert -f qcow2 -O raw my_disk.img raw_disk.img % qemu-img info raw_disk.img image: raw_disk.img file format: raw virtual size: 10 MiB (10485760 bytes) disk size: 10 MiB Child node \u0026#39;/file\u0026#39;: filename: raw_disk.img protocol type: file file length: 10 MiB (10485760 bytes) disk size: 10 MiB Once we have the raw image, the dd command can be used to write the data to either a USB stick or physical SSD. To avoid any destructive commands let\u0026rsquo;s pretend raw_disk2.img represents /dev/sdc, your verified USB thumb drive.\n% dd if=raw_disk.img of=raw_disk2.img bs=1m 10+0 records in 10+0 records out 10485760 bytes transferred in 0.006266 secs (1673437600 bytes/sec) Because our raw file is only 10MB in size, only 10MB will be used on the thumb drive. This is where follow up tools like LVM or resize2fs will extend the filesystem to fill the entire thumb drive. Tools used for expansion depends on how the filesystem was created.\nPutting it all together Enough with the documentation, let\u0026rsquo;s put it into practice with a real use case. Presume that after reading my cloud-init tutorials you wish to modify the Alpine Linux cloud-init image before installation.\nWe can see the downloaded file is a qcow2 image with a capacity of 200Mb from qemu-img info.\n% qemu-img info nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 image: nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 file format: qcow2 virtual size: 200 MiB (209715200 bytes) disk size: 181 MiB ... As we want to install our java app into the installer, we need to add space to the image with qemu-img resize. But first, let‚Äôs create a snapshot. That way, if we make a mistake, we won‚Äôt need to re-download the cloud-init image.\n% qemu-img snapshot -c no_modifications nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 % qemu-img resize nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 +800M Image resized. % qemu-img info nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 image: nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 file format: qcow2 virtual size: 0.977 GiB (1048576000 bytes) disk size: 197 MiB cluster_size: 65536 Snapshot list: ID TAG VM_SIZE DATE VM_CLOCK ICOUNT 1 no_modifications 0 B 2025-04-06 15:23:50 0000:00:00.000 0 Format specific information: ... I\u0026rsquo;m still using the Alpine extended ISO to boot the VM. Alpine cloud images require setup for ssh key authentication to login and an empty root password is much easier to use.\n% qemu-system-x86_64 \\ -m 2G -smp cpus=4 -serial stdio \\ -boot once=d \\ -drive file=./nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2,format=qcow2,media=disk,cache=unsafe \\ -drive file=./alpine-extended-3.21.2-x86_64.iso,format=raw,media=cdrom \\ -nic user,model=virtio-net-pci,hostfwd=tcp::2222-:22 Login as root and mount the disk device under /mnt\nlocalhost:~# mount /dev/sda /mnt localhost:~# ls /mnt bin home mnt run tmp boot lib opt sbin usr dev lost+found proc srv var etc media root sys Then make changes to the cloud image, unmount the filesystem and you\u0026rsquo;re done.\nlocalhost:~# echo \u0026#34;Adam Faris was here\u0026#34; \u0026gt; /mnt/etc/motd localhost:~# cat /mnt/etc/motd Adam Faris was here localhost:~# umount /mnt localhost:~# poweroff Finally, convert our modified cloud image from qcow2 format to raw format, then use dd to write the raw image to a USB device.\n% qemu-img convert -f qcow2 -O nocloud_alpine-3.21.2-x86_64-bios-cloudinit-r0.qcow2 alpine_cloudinit.raw % dd if=alpine_cloudinit.raw bs=1m of=/dev/... With the modified image written to the USB device, you can now boot a physical machine from it. Thanks for sticking with me until the end. If you find this content useful, follow me on BlueSky social for future announcements.\n","permalink":"https://amf3.github.io/articles/virtualization/qemuimage_tips/","summary":"\u003cp\u003eQEMU uses files to emulate storage devices, and the features available\ndepend on how those files are created.  While QEMU can emulate disks from Parallels and VirtualBox, I‚Äôm going to\nfocus on the formats most commonly used in automation and scripting, \u003cstrong\u003eraw\u003c/strong\u003e and \u003cstrong\u003eqcow2\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe default format is raw and raw offers the fewest features.  It\u0026rsquo;s just plain storage.  The other format qcow2\nsupports compression, snapshots, and copy-on-write in addition to storage.\u003c/p\u003e","title":"Tips for working with qemu images"},{"content":"I previously wrote an introduction to cloud-init. I\u0026rsquo;d like to now follow up with a discussion on troubleshooting. cloud-init failures on remote hosts can be challenging. Depending on the failure point, cloud-init may or may not provide clear error indicators. These are methods I use during provisioning issues related to cloud-init.\nUnderstanding cloud-init execution stages Before continuing, let\u0026rsquo;s cover some background. cloud-init follows five stages during boot which run sequentially. If a stage completes, output will contain a status that can be used to verify that stage was successful.\nDetect stage: The init system is responsible for calling ds_identify to determine whether cloud-init should run. With systemd hosts, this is implemented as a systemd generator.\nLocal stage: Identifies local resources that are available without network access. Configures networking, which if unsuccessful falls back to DHCP.\nNetwork stage: Retrieves user-data, sets up disk partitions, and mounts the filesystem. When complete, serial console or SSH access should become available.\nConfig stage: Runs configuration modules and executes commands specified in user-data.\nFinal stage: Installs packages, applies configuration management plugins like puppet or chef, and runs user or vendor defined scripts.\nChecking Stage Status The status submenu from the cloud-init command provides a method of checking each stage for errors. In this example I intentionally mistyped a schema key name that should be passwd as password. Output shows the failure occurred during the init stage \u0026amp; provides a suggestion on how to resolve it.\n$ cloud-init status --format json ... \u0026#34;extended_status\u0026#34;: \u0026#34;degraded done\u0026#34;, \u0026#34;init\u0026#34;: { \u0026#34;errors\u0026#34;: [], \u0026#34;finished\u0026#34;: 6.52, \u0026#34;recoverable_errors\u0026#34;: { \u0026#34;WARNING\u0026#34;: [ \u0026#34;cloud-config failed schema validation! You may run \u0026#39;sudo cloud-init schema --system\u0026#39; to check the details.\u0026#34; ] }, ... \u0026#34;status\u0026#34;: \u0026#34;done\u0026#34; Checking logs for Errors When the issue is not obvious, there logs are available for further examination.\n/var/log/cloud-init.log (execution details and errors) /var/log/cloud-init-output.log (captured output from executed commands) /run/cloud-init/result.json (summary of execution status) Example log output from cloud-init.log indicating a schema validation failure.\n2025-03-18 11:46:41,379 - schema.py[WARNING]: cloud-config failed schema validation! You may run \u0026#39;sudo cloud-init schema --system\u0026#39; to check the details. Debugging User-Data Issues cloud-init has a defined schema and it‚Äôs possible to validate user-data content with the schema submenu. To troubleshoot a possible schema issue on a remote host where a YAML key named passwd was mistyped as password.\n$ sudo cloud-init schema --system Found cloud-config data types: user-data, vendor-data, network-config 1. user-data at /var/lib/cloud/instances/docker-demo/cloud-config.txt: Invalid user-data /var/lib/cloud/instances/docker-demo/cloud-config.txt Error: Cloud config schema errors: users.0: Additional properties are not allowed (\u0026#39;password\u0026#39; was unexpected) ‚Ä¶ Error: Invalid schema: user-data To test changes made to user-data content prior to provisioning: cloud-init schema -c ‚Äúmy_user_data_file.yaml‚Äù.\nFor timeout issues in user or vendor scripts, cloud-init analyze will print execution times which pinpoint delays.\nCommon Failure Scenarios and Fixes A typical source of failures is from syntax errors in the user-data file. As already mentioned, cloud-init schema will show schema issues in user-data. Manually check for typos within the values in user-data. A mistyped value is still a string and can pass the schema validation.\nAnother possible issue is misconfigured network settings preventing package installation. Ensure package mirrors are reachable and contain the package. The cloud-init-output.log file can show additional hints related to package failures. If SSH is unavailable, try accessing the instance over the instance\u0026rsquo;s serial console.\nCheck for missing or incorrectly set permissions on scripts.\nUse cloud-init analyze show to help in identifying long-running stages.\n$ cloud-init analyze show -- Boot Record 01 -- The total time elapsed since completing an event is printed after the \u0026#34;@\u0026#34; character. The time the event takes is printed after the \u0026#34;+\u0026#34; character. Starting stage: init-local |`-\u0026gt;no cache found @00.00100s +00.00000s |`-\u0026gt;found local data from DataSourceNoCloud @00.00400s +00.01500s Finished stage: (init-local) 00.28900 seconds Starting stage: init-network |`-\u0026gt;restored from cache with run check: DataSourceNoCloud [seed=/dev/vda] @02.56800s +00.00100s |`-\u0026gt;setting up datasource @02.57600s +00.00000s |`-\u0026gt;reading and applying user-data @02.58000s +00.00200s |`-\u0026gt;reading and applying vendor-data @02.58200s +00.00200s |`-\u0026gt;reading and applying vendor-data2 @02.58400s +00.00000s ... Recovery and Re-Runs Additional steps are needed after modifying user-data files on the failed instance. When cloud-init runs, output is cached to disk. If the cache exists on reboot, cloud-init will not run again. To clean cached instance data, run cloud-init clean --logs and reboot the instance.\nIssues with a specific module can be exposed by using cloud-init single. This could be useful when troubleshooting user or vendor scripts. Module names can be found with grep \u0026quot;Running module\u0026quot; /var/log/cloud-init.log.\n$ sudo cloud-init single --name set_passwords Cloud-init v. 24.4.1-0ubuntu0~24.04.1 running \u0026#39;single\u0026#39; at Fri, 21 Mar 2025 20:45:47 +0000. Up 16145.16 seconds. When using the single submenu, it won\u0026rsquo;t necessarily fix dependencies unless those are also explicitly re-triggered. It\u0026rsquo;s best to reprovision the instance after troubleshooting any failed modules.\nTakeaways There‚Äôs no simple fix for understanding why instance provisioning with cloud-init failed. That‚Äôs why I‚Äôm closing with a checklist.\nCheck cloud-init status Use cloud-init status --long (or \u0026ndash;json) for execution state Use cloud-init analyze for timing analysis Inspect logs for errors /var/log/cloud-init.log: Shows errors and execution order /var/log/cloud-init-output.log: contains command output Validate user-data input cloud-init schema to validate syntax Ensure values are correct and not only properly formatted YAML Check for missing dependencies or network issues Ensure package mirrors are available and contain the necessary packages. Verify custom scripts are executable. Re-run cloud-init if needed. Clean logs and reset cloud-init: cloud-init clean --logs \u0026amp;\u0026amp; reboot Manually rerun a failed module: cloud-init single -n ‚Äúsome_module_name‚Äù Happy provisioning, and follow me on Bluesky if you find content like this interesting.\n","permalink":"https://amf3.github.io/articles/cloudinit/troubleshooting/","summary":"\u003cp\u003eI previously wrote an \u003ca href=\"../../../articles/cloudinit/intro/\"\u003eintroduction\u003c/a\u003e to cloud-init. I\u0026rsquo;d like to now follow up with a discussion on\ntroubleshooting. cloud-init failures on remote hosts can be challenging. Depending on the failure point, cloud-init may or may not\nprovide clear error indicators.  These are methods I use during provisioning issues related to cloud-init.\u003c/p\u003e\n\u003ch1 id=\"understanding-cloud-init-execution-stages\"\u003eUnderstanding cloud-init execution stages\u003c/h1\u003e\n\u003cp\u003eBefore continuing, let\u0026rsquo;s cover some background.  cloud-init follows\n\u003ca href=\"https://cloudinit.readthedocs.io/en/latest/explanation/boot.html\"\u003efive stages\u003c/a\u003e during boot which run sequentially. If\na stage completes, output will contain a status that can be used to verify that stage was successful.\u003c/p\u003e","title":"cloud-init troubleshooting"},{"content":"In a previous post, I discussed using cloud-init and Multipass as a method of provisioning virtual machines on a local computer with a cloud-like API. Today I am going to dive deeper with Ubuntu and how their autoinstall API can simplify on-premise host provisioning.\nautoinstall is a tool that allows for unattended installations of Ubuntu, ensuring consistency, reporducibility, and providing automation across a fleet of hosts. In this post I\u0026rsquo;ll walk through an example of using autoinstall to configure networking, local storage, and demonstrate shell command execution during provisioning.\nPrerequisite Because the final target is a bare-metal instance, I find it quicker to iterate \u0026amp; test autoinstall changes with QEMU on my macOS M2 laptop. QEMU is a hardware emulator which runs on Linux, macOS, \u0026amp; Windows, allowing the emulation of different CPUs, network cards, or storage devices. Instructions to install QEMU can be found online. For macOS, this can be as simple as running brew install qemu.\nNext we need the Ubuntu install media which can be downloaded here.\nQEMU overview Let\u0026rsquo;s get started by creating a virtual disk drive for installing Ubuntu. This can be done with qemu-img create -f qcow2 virtual_disk.img 6G which creates a 6GB virtual disk named virtual_disk.img in the current directory.\nIn the example below, the -boot once=d option instructes QEMU to boot from the virtual CD-ROM on first startup. After which QEMU will boot from the virtual disk. The other options initialize a 4 core CPU with 4GB of memory. The -net user,hostfwd string will port forward from localhost on the host system to port 22 on the virtual machine. If additional port forwarding is needed, like testing https traffic on port 443 of the VM, multiple hostfwd options seperated by commas can used. Be sure to adjust the filename and path to the Ubuntu ISO as needed.\nqemu-system-x86_64 -hda virtual_disk.img -boot once=d -cdrom ./ubuntu-24.10-live-server-amd64.iso -m 4096 -smp cpus=4 -net nic,model=virtio -net user,hostfwd=tcp:127.0.0.1:2222-:22 Autoinstall Autoinstall is included as part of the Ubuntu boot ISO and works with other provisioning tools from Canonical like Subiquity, Curtin, or cloud-init. When reading Autoinstall documentation, it\u0026rsquo;s useful to know which tool is being used during each install stage as often those options are passed to the underlying provisioning tool.\nLike Kickstart for RHEL, Autoinstall is Ubuntu\u0026rsquo;s answer to unattended installations, and uses YAML files for data input. Autoinstall uses default locations for finding the YAML files and locations can also be specified in the GRUB menu when the instance boots. Locations are specified as either a filepath or a URL. I\u0026rsquo;ll be using a URL for the file locations.\nLets create the empty YAML files and use python to create a simple webserver to serve the files. In another terminal type the following as the webserver runs in the foreground. Use cntl-c to terminate the python webserver when it\u0026rsquo;s no longer needed.\ntouch user-data meta-data network-config vendor-data python3 -m http.server -b 127.0.0.1 -d $PWD 8080 Next start the virtual machine.\nqemu-system-x86_64 -hda virtual_disk.img -boot once=d -cdrom ./ubuntu-24.10-live-server-amd64.iso -m 4096 -smp cpus=4 -net nic,model=virtio -net user,hostfwd=tcp:127.0.0.1:2222-:22 This will open a QEMU console window where we\u0026rsquo;ll interact with the GRUB menu to specify the YAML file locations. Change focus to the console window, highlight \u0026ldquo;Try or Install Ubuntu Server\u0026rdquo; and hit the \u0026quot;e\u0026quot; key to edit the grub menu.\nOn the line starting with ‚Äúlinux /casper/vmlinuz‚Äù add: autoinstall ds=nocloud\\;s=http://10.0.2.2:8080/ before the three dashes. The grub menu should look something like this when the edits are complete.\nlinux /casper/vmlinuz autoinstall ds=nocloud\\;s=http://10.0.2.2:8080/ --- initrd /casper/initrd Exit grub and boot by following the on-screen instructions to hit F10 or cntl-x. Watch the terminal running the python webserver and requests for the autoinstall YAML files should be seen. As they are empty config files, an interactive menu-driven session will present itself in the QEMU console window. To cancel the install, close the QEMU console window.\nThe GRUB modification tells autoinstall to use the nocloud plugin from cloud-init to download its configuration at the specified URL. QEMU assigns the special IP address of 10.0.2.2 to the host system when using -net user. This allows the VM to reach services running on the host such as our local Python HTTP server and why autoinstall is able to download its configurations over HTTP.\nThe YAML block should be added to the user-data file that was created earlier. The other files will remain empty. The minimal config example assigns a hostname of my-qemu-vm, creates an admin account named ubuntu, and assigns the ubuntu user the password of abc123.\nIt\u0026rsquo;s possible to generate a different secure password hash with openssl, as shown in this example: echo abc123 | openssl passwd -6 -stdin. Restart the QEMU VM so it boots from the virtual CD-ROM and modify the GRUB menu so it loads the new config when the VM boots.\n#cloud-config autoinstall: version: 1 identity: hostname: my-qemu-vm username: ubuntu password: $6$xK2amorOU9tK4jt4$zLA1RZUpo4CzyDBzPDHCT61FLOngjWpV7Q/BH9KieLsJ/VG8r/Y88YIMLIOL.vc4ZHees40IAqORxjqa7GKti/ # password is \u0026#34;abc123\u0026#34; Autoinstall will take several minutes to complete and will reboot when done. In some stages autoinstall can look stuck in some stages. Remember that Linux virtual consoles are available to inspect running processes. Virtual consoles are accessible by typing alt + left/right arrow key or using alt + F2 or alt + F3. (Use the option key for alt when using a Mac keyboard.) Eventually the VM will reboot and the login prompt should be visible if everything went as expected.\nAutoinstall has a list of defaults it uses when the option is present in the user-data file. After logging into the QEMU instance, it\u0026rsquo;s possible to view the specified values from the user-data YAML file that have been merged into the defaults.\nsudo less /var/log/installer/autoinstall-user-data Before continuing lets enable the ssh server, allow passwords for ssh login, and minimize the number of packages used during the install. Other options like locale or the keyboard setup can be found in the autoinstall-user-data file and added ot the example below. Restarting the QEMU VM and modifying the GRUB menu to reinstall the host OS is needed to apply the new changes to the YAML file. Reinstalling the OS also demonstrates the ease of initializing a system to a known state with autoinstall \u0026amp; cloud-init configs.\n#cloud-config autoinstall: version: 1 identity: hostname: my-qemu-vm username: ubuntu password: $6$xK2amorOU9tK4jt4$zLA1RZUpo4CzyDBzPDHCT61FLOngjWpV7Q/BH9KieLsJ/VG8r/Y88YIMLIOL.vc4ZHees40IAqORxjqa7GKti/ # password is \u0026#34;abc123\u0026#34; ssh: # Install SSH server and allow password logins allow-pw: true install-server: true source: # id can also be ubuntu-server id: ubuntu-server-minimal Networking Both autoinstall and cloud-init support a netplan-formatted network configuration, meaning the YAML network example will work with either installer.\nNetwork device names are different between distributions that use Systemd (Ubuntu, Fedora) vs OpenRC (Alpine). Where OpenRC will use easily found device names like \u0026ldquo;eth0\u0026rdquo;, or \u0026ldquo;eth1\u0026rdquo;, Systemd will use the PCI slot number. A Systemd example might look like \u0026ldquo;enp2s0\u0026rdquo;, where \u0026ldquo;en\u0026rdquo; means ethernet, and \u0026ldquo;p2s0\u0026rdquo; is the physical PCI slot. This value will change based on which slot a PCI card is plugged into. Luckily autoinstall lets us wildcard the device names.\nThis network example will work with either OpenRC or Systemd device names. It\u0026rsquo;s similar to what\u0026rsquo;s used by Ubuntu\u0026rsquo;s LiveCD.\nnetwork: version: 2 ethernets: my-en-devices: match: # This will match Systemd naming conventions for ethernet devices which start with \u0026#34;en\u0026#34; and set them to use DHCPv4 name: \u0026#34;en*\u0026#34; dhcp4: true my-eth-devices: match: # This will match OpenRC naming conventions like \u0026#34;eth0\u0026#34; name: \u0026#34;eth*\u0026#34; addresses: # This will specify a static network address - 10.10.10.2/24 nameservers: # We can modify the DNS search path \u0026amp; specify DNS name servers. search: - \u0026#34;mycompany.local\u0026#34; addresses: - 10.10.10.253 - 8.8.8.8 Storage Configuring storage can be complex when configuring per partition byte offsets. Luckily we can provide a storage device name and let defaults handle the details. I\u0026rsquo;ll show a basic lvm example but the other supported layouts are direct, and zfs.\nHere we specify a LVM configuration with a sizing policy to use the entire disk for the logical volume. If sizing-policy were set to scaled, free space would be left on the storage device for things like snapshots or further expansion.\nstorage: layout: name: lvm sizing-policy: all Its possible to target a specific drive to wipe and install a new OS with a match statement. There are multiple ways to select a storage device, model name, serial number, path, whether its rotational or not, or even big or little in size. These values can be found in smartctl output, which comes from the smartmontools package.\nubuntu@my-qemu-vm:~$ sudo apt-get install -y smartmontools ... install stuff ... ubuntu@my-qemu-vm:~$ sudo smartctl -i /dev/sda smartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.11.0-18-generic] (local build) Copyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org === START OF INFORMATION SECTION === Device Model: QEMU HARDDISK Serial Number: QM00001 Firmware Version: 2.5+ User Capacity: 8,589,934,592 bytes [8.58 GB] Sector Size: 512 bytes logical/physical TRIM Command: Available, deterministic Device is: Not in smartctl database 7.3/5528 ATA Version is: ATA/ATAPI-7, ATA/ATAPI-5 published, ANSI NCITS 340-2000 Local Time is: Tue Mar 4 05:45:56 2025 UTC SMART support is: Available - device has SMART capability. SMART support is: Enabled If we wanted to match this disk by wild-carding the model name, we would use the following.\n#cloud-config autoinstall: storage: layout: name: lvm sizing-policy: all match: model: QEMU* Alternatively if our on-premise hardware instance had a 1GB SSD for the OS and a second 12GB spinning disk for data storage, we could use a match with size size: smallest to install the OS on the 1GB disk.\n#cloud-config autoinstall: storage: layout: name: lvm sizing-policy: all match: size: smallest Commands Running arbitrary commands is possible when autoinstall runs. Commands are specified as a list and run under \u0026ldquo;sh -c\u0026rdquo;. Its possible to specify if commands should run early in the autoinstall process, late, or when an error occurs.\nFor example we want to hit a web endpoint when the installer has completed.\n#cloud-config autoinstall: late-commands: - curl -H \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;host\u0026#34;: \u0026#34;\u0026#39;$HOSTNAME\u0026#39;\u0026#34;}\u0026#39; http://myapi.example.com/success - echo \u0026#34;Install Success\u0026#34; \u0026gt; /var/log/my.log To run a command before the autoinstall process runs, like downloading internal x509 certificates:\n#cloud-config autoinstall: early-commands: - mkdir /etc/ssl/mycerts - wget -O /etc/ssl/mycerts/internal.pem \u0026#34;http://x509api.example.com/certs/$HOSTNAME\u0026#34; Or reporting an error when autoinstall fails\n#cloud-config autoinstall: error-commands: - echo \u0026#34;Install failed\u0026#34; \u0026gt; /var/log/my.log - curl -H \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;host\u0026#34;: \u0026#34;\u0026#39;$HOSTNAME\u0026#39;\u0026#34;}\u0026#39; http://myapi.example.com/failures cloud-init It\u0026rsquo;s possible to invoke cloud-init from autoinstall, allowing for additional functionality. This is done by placing the cloud-init entries under a user-data key. Here\u0026rsquo;s a cloud-init example that installs a few packages.\ncloud-init and autoinstall sometimes perform similar tasks. When configuring a storage device with cloud-init, I found it was better to use autoinstall as the cloud-init changes were overwritten.\n#cloud-config autoinstall: user-data: package_update: true # update the list of available packages package_upgrade: true # upgrade currently installed packages. packages: - curl - ca-certificates Other It\u0026rsquo;s possible to configure a local proxy for downloading software packages. Running apt-cacher-ng as a package proxy inside a docker container on my laptop helps when I\u0026rsquo;m on a high latency Internet connection.\n#cloud-config autoinstall: proxy: http://10.0.2.2:3142 Provision a physical host A complete autoinstall user-data file can be downloaded from here. It contains all the examples listed in this post.\nProvisioning a physical host is very similar to using QEMU. The only change is when starting the python webserver. Instead of python binding to 127.0.0.1, have it bind to all interfaces so configs can be downloaded by remote hosts.\npython3 -m http.server -d $PWD 8080 A USB thumb drive is needed to make the Ubuntu ISO available to the physical host; and a monitor \u0026amp; keyboard are needed to modify the GRUB menu when the on-premise hosts boots. When modifying the GRUB menu, instead of using http://10.0.2.2 in the nocloud URL, specify the hostname of the host running the python webserver. In my scenario, the hostname would resolve to my personal laptop.\nWrapping Up By leveraging autoinstall, it\u0026rsquo;s possible to reliably reproduce system setups, whether for virtual machines or bare-metal hosts. In this post, autoinstall was explored as a method to streamline unattended provisioning for Ubuntu instances. Using a QEMU-based test environment, it was possible to quickly iterate on autoinstall configurations by modifying the GRUB menu to pull configuration files over HTTP. The process demonstrated how to format storage devices, set up networking, and run shell commands during installation.\nNext steps? If looking to extend this setup, consider integrating additional automation, such as PXE boot for network-based installs or using cloud-init to interact with configuration management systems like Puppet or Chef. If you have insights from your own experiences, feel free to share them on Bluesky.\n","permalink":"https://amf3.github.io/articles/cloudinit/autoinstall/","summary":"\u003cp\u003eIn a \u003ca href=\"../../../articles/cloudinit/intro/\"\u003eprevious post\u003c/a\u003e, I discussed using cloud-init and Multipass as a method of provisioning\nvirtual machines on a local computer with a cloud-like API.  Today I am going\nto dive deeper with Ubuntu and how their autoinstall API can simplify on-premise host provisioning.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://canonical-subiquity.readthedocs-hosted.com/en/latest/intro-to-autoinstall.html\"\u003eautoinstall\u003c/a\u003e\nis a tool that allows for unattended installations of Ubuntu, ensuring consistency, reporducibility, and providing automation\nacross a fleet of hosts.  In this post I\u0026rsquo;ll walk through an example of using autoinstall\nto configure networking, local storage, and demonstrate shell command execution during provisioning.\u003c/p\u003e","title":"Unattended Ubuntu Installs - Virtual Machines to Bare-Metal"},{"content":"Cloud compute companies like GCP, AWS, or Azure offer a management API for allocating resources. In the on-premise space, services such as Docker or Incus provide APIs for managing containers or virtual machines (VMs). But what about installing the operating system (OS) on bare-metal hosts? What API exists for this task? This is where cloud-init enters the picture, providing the ability to provision VMs or bare-metal hardware.\ncloud-init is a useful tool that doesn\u0026rsquo;t rely on network services like PXE as a dependency. Its simplicity saves time by removing the need to navigate OS installation menus, while ensuring user accounts and installed software packages are consistent across hosts. So why should one bother using cloud-init if they are managing a single host at home? In the event the OS needs to be reinstalled due to failure, cloud-init allows one to quickly restore the system to a known state.\nThis example will use cloud-init to configure a Personal Package Archive (PPA), install Docker, and create a user account inside a Ubuntu VM.\nPrerequisite I find that using cloud-init with Multipass is a easy way to get started. Multipass is a virtual machine manager that works with Linux, MacOS (arm \u0026amp; intel), and Windows. When launching a new VM, Multipass is capable of initializing the VM with cloud-init. If Multipass isn\u0026rsquo;t already installed, this link will provide instructions for installing Multipass. For this cloud-init introduction, I\u0026rsquo;m using Multipass on a M2 Macbook running MacOS Sequoia.\ncloud-init Like many infrastructure tools, the input data for cloud-init is a YAML file. For specifics of this schema, consult the official cloud-init documentation. There one will find that cloud-init input file will need to be prefixed with #cloud-config.\nPackage Management Lets get started with package management for our Multipass instance. This section will show how to add an external PPA (software repository) to the VM with cloud-init to provide additional software packages and define a list of software packages to be installed on the VM.\nAdd External PPA Add the 3rd-party PPA provided by Docker, Inc.\n# Add Docker\u0026#39;s PPA for Ubuntu apt: sources: docker.list: # This snippet comes from https://stackoverflow.com/a/62540068 source: deb [arch=arm64] https://download.docker.com/linux/ubuntu $RELEASE stable # Key ID can be found with ‚Äúgpg --show-keys \u0026lt;(curl -s https://download.docker.com/linux/ubuntu/gpg)‚Äù keyid: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88 Should the GPG key ID for the Docker PPA change, I have left a comment above on how to find that value.\nThis is how the GPG output appears in 2025.\n$ gpg --show-keys \u0026lt;(curl -s https://download.docker.com/linux/ubuntu/gpg) pub rsa4096 2017-02-22 [SCEA] 9DC858229FC7DD38854AE2D88D81803C0EBFCD88 uid Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt; sub rsa4096 2017-02-22 [S] Define Package List Specify a list of packages to install.\n# Update the list of packages available online package_update: true # Upgrade all installed packages package_upgrade: true # Install docker \u0026amp; other utilities packages: - apt-transport-https - ca-certificates - curl - gnupg-agent - software-properties-common - docker-ce - docker-ce-cli - containerd.io - docker-buildx-plugin - docker-compose-plugin User Management Here a new user account is created and added to the docker group with cloud-init. Its likely our user will require both a password \u0026amp; ssh key for remote access. A public ssh key and a password hash is needed for cloud-init input.\nSecrets: Generating a Password Hash To create a password hash, use the mkpasswd command from Ubuntu\u0026rsquo;s whois package. This example will hash the weak password of \u0026ldquo;abc123\u0026rdquo; with the sha512 algorithm. A password better than \u0026ldquo;abc123\u0026rdquo; should be used if following these examples.\n$ mkpasswd -m sha-512 \u0026#34;abc123\u0026#34; $6$EkwQ38oDCPnJDuui$QKw3IISzY3emHXgJ/QHeEH8xyzGOKB3N6.bU/wAkwf4KDRsreB2iApa/EHULbunx6v9o9Q8foq4K.d8WtHukU/ As mkpasswd is specific to Linux and doesn\u0026rsquo;t work with MacOS, one can alternatively use openssl to create a password hash.\n$ echo abc123 | openssl passwd -6 -stdin $6$tdPON3RwkVViXg41$4O9euMZeGFJQXgJ3bvP3YtVcCw9BwIMHLLkix1s/R7woSuAAFvWWtrqqQ.33ESzgcUi9/HdEwelqB9jJUIrpU0 Secrets: Generating a SSH public private key pair To create a SSH key pair, use ssh-keygen: ssh-keygen -t ed25519 -f ./docker_vm_key -C \u0026quot;app@docker_vm\u0026quot; -P abc123. This will create a public \u0026amp; private ssh key in the current directory, with the easily guessable passphrase of abc123. Once again, use a better passphrase if following these examples.\nDefining the User Account This defines an application account named \u0026ldquo;app\u0026rdquo;. The ssh_authorized_keys value comes from the contents of docker_vm_key.pub.\nAs a convenience, the public and private ssh keys from this example are provided.\n# create the docker group groups: - docker users: - name: app groups: [docker, admin, users] gecos: Application User shell: /bin/bash lock_passwd: true passwd: $6$tdPON3RwkVViXg41$4O9euMZeGFJQXgJ3bvP3YtVcCw9BwIMHLLkix1s/R7woSuAAFvWWtrqqQ.33ESzgcUi9/HdEwelqB9jJUIrpU0 ssh_authorized_keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHsPNGa1NJLd4edDLRI033Sw33Nkl6qO+52qNAhY556C app@docker_vm Putting it all together I\u0026rsquo;ve combined the YAML snippets into a single file named docker-install.yaml which can be downloaded here.\nRun the following to see cloud-init in action. This will create a virtual machine with 2 virtual CPU cores, 2 GB of ram, with a 4GB virtual disk using the LTS release of Ubuntu. Depending on your Internet speed, this may take a few minutes as you\u0026rsquo;ll be downloading packages from the Internet.\n$ multipass launch -n docker-demo --cloud-init docker-install.yaml -c 2 -m 2G -d 4G lts To find the new VM and access it over SSH with the private key so a docker command can be ran from a remote shell.\n% mp list Name State IPv4 Image docker-demo Running 192.168.64.32 Ubuntu 24.04 LTS 172.17.0.1 % ssh -l app -i ./docker_vm_key 192.168.64.32 The authenticity of host \u0026#39;192.168.64.32 (192.168.64.32)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:EUqLjr9n9CyjKY6Y8EzNQGomeEtpePMFo5BXjO8YfHY. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes ... app@docker-demo:~$ docker run hello-world Unable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world c9c5fd25a1bd: Pull complete Digest: sha256:e0b569a5163a5e6be84e210a2587e7d447e08f87a0e90798363fa44a0464a1e8 Status: Downloaded newer image for hello-world:latest Hello from Docker! ... Conclusion Several cloud-init basics have been covered in this introduction. Like adding a PPA, installing software packages, and creating a user account.\nWhile I understand that installing Docker in my example might not represent the typical workflow. Combining cloud-init concepts with Multipass creates a local mini-cloud on my Macbook. I can quickly iterate through cloud-init data file changes for other platforms like AWS or on-premise hardware.\ncloud-init is capable of much more, like formatting hard drives or managing network interfaces. These \u0026amp; other topics will be covered in followups which I will announce on Bluesky. Follow me for notifications of when its made available. Otherwise, try out these examples and let me know what works.\n","permalink":"https://amf3.github.io/articles/cloudinit/intro/","summary":"\u003cp\u003eCloud compute companies like GCP, AWS, or Azure offer a management API for allocating resources. In the on-premise space,\nservices such as Docker or Incus provide APIs for managing containers or virtual machines (VMs). But what about installing\nthe operating system (OS) on bare-metal hosts? What API exists for this task? This is where\n\u003ca href=\"https://github.com/canonical/cloud-init\"\u003ecloud-init\u003c/a\u003e enters the picture, providing the ability to provision VMs or\nbare-metal hardware.\u003c/p\u003e\n\u003cp\u003ecloud-init is a useful tool that doesn\u0026rsquo;t rely on network services like PXE as a dependency.  Its simplicity saves time by\nremoving the need to navigate OS installation menus, while ensuring user accounts and installed software packages are consistent\nacross hosts. So why should one bother using cloud-init if they are managing a single host at home? In the event\nthe OS needs to be reinstalled due to failure, cloud-init allows one to quickly restore the system to a known state.\u003c/p\u003e","title":"Getting started with cloud-init for unattended Linux deployments"}]
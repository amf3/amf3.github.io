[{"content":"Introduction I previously wrote an introduction to cloud-init. I\u0026rsquo;d like to now follow up with a discussion on troubleshooting. cloud-init failures on remote hosts can be challenging. Depending on the failure point, cloud-init may or may not provide clear error indicators. These are methods I use during provisioning issues related to cloud-init.\nUnderstanding cloud-init execution stages Before continuing, let\u0026rsquo;s cover some background. cloud-init follows five stages during boot which run sequentially. If a stage completes, output will contain a status that can be used to verify that stage was successful.\nDetect stage: The init system is responsible for calling ds_identify to determine whether cloud-init should run. With systemd hosts, this is implemented as a systemd generator.\nLocal stage: Identifies local resources that are available without network access. Configures networking, which if unsuccessful falls back to DHCP.\nNetwork stage: Retrieves user-data, sets up disk partitions, and mounts the filesystem. When complete, serial console or SSH access should become available.\nConfig stage: Runs configuration modules and executes commands specified in user-data.\nFinal stage: Installs packages, applies configuration management plugins like puppet or chef, and runs user or vendor defined scripts.\nChecking Stage Status The status submenu from the cloud-init command provides a method of checking each stage for errors. In this example I intentionally mistyped a schema key name that should be passwd as password. Output shows the failure occurred during the init stage \u0026amp; provides a suggestion on how to resolve it.\n$ cloud-init status --format json ... \u0026#34;extended_status\u0026#34;: \u0026#34;degraded done\u0026#34;, \u0026#34;init\u0026#34;: { \u0026#34;errors\u0026#34;: [], \u0026#34;finished\u0026#34;: 6.52, \u0026#34;recoverable_errors\u0026#34;: { \u0026#34;WARNING\u0026#34;: [ \u0026#34;cloud-config failed schema validation! You may run \u0026#39;sudo cloud-init schema --system\u0026#39; to check the details.\u0026#34; ] }, ... \u0026#34;status\u0026#34;: \u0026#34;done\u0026#34; Checking logs for Errors When the issue is not obvious, there logs are available for further examination.\n/var/log/cloud-init.log (execution details and errors) /var/log/cloud-init-output.log (captured output from executed commands) /run/cloud-init/result.json (summary of execution status) Example log output from cloud-init.log indicating a schema validation failure.\n2025-03-18 11:46:41,379 - schema.py[WARNING]: cloud-config failed schema validation! You may run \u0026#39;sudo cloud-init schema --system\u0026#39; to check the details. Debugging User-Data Issues cloud-init has a defined schema and it’s possible to validate user-data content with the schema submenu. To troubleshoot a possible schema issue on a remote host where a YAML key named passwd was mistyped as password.\n$ sudo cloud-init schema --system Found cloud-config data types: user-data, vendor-data, network-config 1. user-data at /var/lib/cloud/instances/docker-demo/cloud-config.txt: Invalid user-data /var/lib/cloud/instances/docker-demo/cloud-config.txt Error: Cloud config schema errors: users.0: Additional properties are not allowed (\u0026#39;password\u0026#39; was unexpected) … Error: Invalid schema: user-data To test changes made to user-data content prior to provisioning: cloud-init schema -c “my_user_data_file.yaml”.\nFor timeout issues in user or vendor scripts, cloud-init analyze will print execution times which pinpoint delays.\nCommon Failure Scenarios and Fixes A typical source of failures is from syntax errors in the user-data file. As already mentioned, cloud-init schema will show schema issues in user-data. Manually check for typos within the values in user-data. A mistyped value is still a string and can pass the schema validation.\nAnother possible issue is misconfigured network settings preventing package installation. Ensure package mirrors are reachable and contain the package. The cloud-init-output.log file can show additional hints related to package failures. If SSH is unavailable, try accessing the instance over the instance\u0026rsquo;s serial console.\nCheck for missing or incorrectly set permissions on scripts.\nUse cloud-init analyze show to help in identifying long-running stages.\n$ cloud-init analyze show -- Boot Record 01 -- The total time elapsed since completing an event is printed after the \u0026#34;@\u0026#34; character. The time the event takes is printed after the \u0026#34;+\u0026#34; character. Starting stage: init-local |`-\u0026gt;no cache found @00.00100s +00.00000s |`-\u0026gt;found local data from DataSourceNoCloud @00.00400s +00.01500s Finished stage: (init-local) 00.28900 seconds Starting stage: init-network |`-\u0026gt;restored from cache with run check: DataSourceNoCloud [seed=/dev/vda] @02.56800s +00.00100s |`-\u0026gt;setting up datasource @02.57600s +00.00000s |`-\u0026gt;reading and applying user-data @02.58000s +00.00200s |`-\u0026gt;reading and applying vendor-data @02.58200s +00.00200s |`-\u0026gt;reading and applying vendor-data2 @02.58400s +00.00000s ... Recovery and Re-Runs Additional steps are needed after modifying user-data files on the failed instance. When cloud-init runs, output is cached to disk. If the cache exists on reboot, cloud-init will not run again. To clean cached instance data, run cloud-init clean --logs and reboot the instance.\nIssues with a specific module can be exposed by using cloud-init single. This could be useful when troubleshooting user or vendor scripts. Module names can be found with grep \u0026quot;Running module\u0026quot; /var/log/cloud-init.log.\n$ sudo cloud-init single --name set_passwords Cloud-init v. 24.4.1-0ubuntu0~24.04.1 running \u0026#39;single\u0026#39; at Fri, 21 Mar 2025 20:45:47 +0000. Up 16145.16 seconds. When using the single submenu, it won\u0026rsquo;t necessarily fix dependencies unless those are also explicitly re-triggered. It\u0026rsquo;s best to reprovision the instance after troubleshooting any failed modules.\nTakeaways There’s no simple fix for understanding why instance provisioning with cloud-init failed. That’s why I’m closing with a checklist.\nCheck cloud-init status Use cloud-init status --long (or \u0026ndash;json) for execution state Use cloud-init analyze for timing analysis Inspect logs for errors /var/log/cloud-init.log: Shows errors and execution order /var/log/cloud-init-output.log: contains command output Validate user-data input cloud-init schema to validate syntax Ensure values are correct and not only properly formatted YAML Check for missing dependencies or network issues Ensure package mirrors are available and contain the necessary packages. Verify custom scripts are executable. Re-run cloud-init if needed. Clean logs and reset cloud-init: cloud-init clean --logs \u0026amp;\u0026amp; reboot Manually rerun a failed module: cloud-init single -n “some_module_name” Happy provisioning and good luck!\n","permalink":"https://amf3.github.io/articles/cloudinit/troubleshooting/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eI previously wrote an \u003ca href=\"../../../articles/cloudinit/intro/\"\u003eintroduction\u003c/a\u003e to cloud-init. I\u0026rsquo;d like to now follow up with a discussion on\ntroubleshooting. cloud-init failures on remote hosts can be challenging. Depending on the failure point, cloud-init may or may not\nprovide clear error indicators.  These are methods I use during provisioning issues related to cloud-init.\u003c/p\u003e\n\u003ch1 id=\"understanding-cloud-init-execution-stages\"\u003eUnderstanding cloud-init execution stages\u003c/h1\u003e\n\u003cp\u003eBefore continuing, let\u0026rsquo;s cover some background.  cloud-init follows\n\u003ca href=\"https://cloudinit.readthedocs.io/en/latest/explanation/boot.html\"\u003efive stages\u003c/a\u003e during boot which run sequentially. If\na stage completes, output will contain a status that can be used to verify that stage was successful.\u003c/p\u003e","title":"cloud-init troubleshooting"},{"content":"Intro In a previous post, I discussed using cloud-init and Multipass as a method of provisioning virtual machines on a local computer with a cloud-like API. Today I am going to dive deeper with Ubuntu and how their autoinstall API can simplify on-premise host provisioning.\nautoinstall is a tool that allows for unattended installations of Ubuntu, ensuring consistency, reporducibility, and providing automation across a fleet of hosts. In this post I\u0026rsquo;ll walk through an example of using autoinstall to configure networking, local storage, and demonstrate shell command execution during provisioning.\nPrerequisite Because the final target is a bare-metal instance, I find it quicker to iterate \u0026amp; test autoinstall changes with QEMU on my macOS M2 laptop. QEMU is a hardware emulator which runs on Linux, macOS, \u0026amp; Windows, allowing the emulation of different CPUs, network cards, or storage devices. Instructions to install QEMU can be found online. For macOS, this can be as simple as running brew install qemu.\nNext we need the Ubuntu install media which can be downloaded here.\nQEMU overview Let\u0026rsquo;s get started by creating a virtual disk drive for installing Ubuntu. This can be done with qemu-img create -f qcow2 virtual_disk.img 6G which creates a 6GB virtual disk named virtual_disk.img in the current directory.\nIn the example below, the -boot once=d option instructes QEMU to boot from the virtual CD-ROM on first startup. After which QEMU will boot from the virtual disk. The other options initialize a 4 core CPU with 4GB of memory. The -net user,hostfwd string will port forward from localhost on the host system to port 22 on the virtual machine. If additional port forwarding is needed, like testing https traffic on port 443 of the VM, multiple hostfwd options seperated by commas can used. Be sure to adjust the filename and path to the Ubuntu ISO as needed.\nqemu-system-x86_64 -hda virtual_disk.img -boot once=d -cdrom ./ubuntu-24.10-live-server-amd64.iso -m 4096 -smp cpus=4 -net nic,model=virtio -net user,hostfwd=tcp:127.0.0.1:22-:22 Autoinstall Autoinstall is included as part of the Ubuntu boot ISO and works with other provisioning tools from Canonical like Subiquity, Curtin, or cloud-init. When reading Autoinstall documentation, it\u0026rsquo;s useful to know which tool is being used during each install stage as often those options are passed to the underlying provisioning tool.\nLike Kickstart for RHEL, Autoinstall is Ubuntu\u0026rsquo;s answer to unattended installations, and uses YAML files for data input. Autoinstall uses default locations for finding the YAML files and locations can also be specified in the GRUB menu when the instance boots. Locations are specified as either a filepath or a URL. I\u0026rsquo;ll be using a URL for the file locations.\nLets create the empty YAML files and use python to create a simple webserver to serve the files. In another terminal type the following as the webserver runs in the foreground. Use cntl-c to terminate the python webserver when it\u0026rsquo;s no longer needed.\ntouch user-data meta-data network-config vendor-data python3 -m http.server -b 127.0.0.1 -d $PWD 8080 Next start the virtual machine.\nqemu-system-x86_64 -hda virtual_disk.img -boot once=d -cdrom ./ubuntu-24.10-live-server-amd64.iso -m 4096 -smp cpus=4 -net nic,model=virtio -net user,hostfwd=tcp:127.0.0.1:22-:22 This will open a QEMU console window where we\u0026rsquo;ll interact with the GRUB menu to specify the YAML file locations. Change focus to the console window, highlight \u0026ldquo;Try or Install Ubuntu Server\u0026rdquo; and hit the \u0026quot;e\u0026quot; key to edit the grub menu.\nOn the line starting with “linux /casper/vmlinuz” add: autoinstall ds=nocloud\\;s=http://10.0.2.2:8080/ before the three dashes. The grub menu should look something like this when the edits are complete.\nlinux /casper/vmlinuz autoinstall ds=nocloud\\;s=http://10.0.2.2:8080/ --- initrd /casper/initrd Exit grub and boot by following the on-screen instructions to hit F10 or cntl-x. Watch the terminal running the python webserver and requests for the autoinstall YAML files should be seen. As they are empty config files, an interactive menu-driven session will present itself in the QEMU console window. To cancel the install, close the QEMU console window.\nThe GRUB modification tells autoinstall to use the nocloud plugin from cloud-init to download its configuration at the specified URL. QEMU assigns the special IP address of 10.0.2.2 to the host system when using -net user. This allows the VM to reach services running on the host such as our local Python HTTP server and why autoinstall is able to download its configurations over HTTP.\nThe YAML block should be added to the user-data file that was created earlier. The other files will remain empty. The minimal config example assigns a hostname of my-qemu-vm, creates an admin account named ubuntu, and assigns the ubuntu user the password of abc123.\nIt\u0026rsquo;s possible to generate a different secure password hash with openssl, as shown in this example: echo abc123 | openssl passwd -6 -stdin. Restart the QEMU VM so it boots from the virtual CD-ROM and modify the GRUB menu so it loads the new config when the VM boots.\n#cloud-config autoinstall: version: 1 identity: hostname: my-qemu-vm username: ubuntu password: $6$xK2amorOU9tK4jt4$zLA1RZUpo4CzyDBzPDHCT61FLOngjWpV7Q/BH9KieLsJ/VG8r/Y88YIMLIOL.vc4ZHees40IAqORxjqa7GKti/ # password is \u0026#34;abc123\u0026#34; Autoinstall will take several minutes to complete and will reboot when done. In some stages autoinstall can look stuck in some stages. Remember that Linux virtual consoles are available to inspect running processes. Virtual consoles are accessible by typing alt + left/right arrow key or using alt + F2 or alt + F3. (Use the option key for alt when using a Mac keyboard.) Eventually the VM will reboot and the login prompt should be visible if everything went as expected.\nAutoinstall has a list of defaults it uses when the option is present in the user-data file. After logging into the QEMU instance, it\u0026rsquo;s possible to view the specified values from the user-data YAML file that have been merged into the defaults.\nsudo less /var/log/installer/autoinstall-user-data Before continuing lets enable the ssh server, allow passwords for ssh login, and minimize the number of packages used during the install. Other options like locale or the keyboard setup can be found in the autoinstall-user-data file and added ot the example below. Restarting the QEMU VM and modifying the GRUB menu to reinstall the host OS is needed to apply the new changes to the YAML file. Reinstalling the OS also demonstrates the ease of initializing a system to a known state with autoinstall \u0026amp; cloud-init configs.\n#cloud-config autoinstall: version: 1 identity: hostname: my-qemu-vm username: ubuntu password: $6$xK2amorOU9tK4jt4$zLA1RZUpo4CzyDBzPDHCT61FLOngjWpV7Q/BH9KieLsJ/VG8r/Y88YIMLIOL.vc4ZHees40IAqORxjqa7GKti/ # password is \u0026#34;abc123\u0026#34; ssh: # Install SSH server and allow password logins allow-pw: true install-server: true source: # id can also be ubuntu-server id: ubuntu-server-minimal Networking Both autoinstall and cloud-init support a netplan-formatted network configuration, meaning the YAML network example will work with either installer.\nNetwork device names are different between distributions that use Systemd (Ubuntu, Fedora) vs OpenRC (Alpine). Where OpenRC will use easily found device names like \u0026ldquo;eth0\u0026rdquo;, or \u0026ldquo;eth1\u0026rdquo;, Systemd will use the PCI slot number. A Systemd example might look like \u0026ldquo;enp2s0\u0026rdquo;, where \u0026ldquo;en\u0026rdquo; means ethernet, and \u0026ldquo;p2s0\u0026rdquo; is the physical PCI slot. This value will change based on which slot a PCI card is plugged into. Luckily autoinstall lets us wildcard the device names.\nThis network example will work with either OpenRC or Systemd device names. It\u0026rsquo;s similar to what\u0026rsquo;s used by Ubuntu\u0026rsquo;s LiveCD.\nnetwork: version: 2 ethernets: my-en-devices: match: # This will match Systemd naming conventions for ethernet devices which start with \u0026#34;en\u0026#34; and set them to use DHCPv4 name: \u0026#34;en*\u0026#34; dhcp4: true my-eth-devices: match: # This will match OpenRC naming conventions like \u0026#34;eth0\u0026#34; name: \u0026#34;eth*\u0026#34; addresses: # This will specify a static network address - 10.10.10.2/24 nameservers: # We can modify the DNS search path \u0026amp; specify DNS name servers. search: - \u0026#34;mycompany.local\u0026#34; addresses: - 10.10.10.253 - 8.8.8.8 Storage Configuring storage can be complex when configuring per partition byte offsets. Luckily we can provide a storage device name and let defaults handle the details. I\u0026rsquo;ll show a basic lvm example but the other supported layouts are direct, and zfs.\nHere we specify a LVM configuration with a sizing policy to use the entire disk for the logical volume. If sizing-policy were set to scaled, free space would be left on the storage device for things like snapshots or further expansion.\nstorage: layout: name: lvm sizing-policy: all Its possible to target a specific drive to wipe and install a new OS with a match statement. There are multiple ways to select a storage device, model name, serial number, path, whether its rotational or not, or even big or little in size. These values can be found in smartctl output, which comes from the smartmontools package.\nubuntu@my-qemu-vm:~$ sudo apt-get install -y smartmontools ... install stuff ... ubuntu@my-qemu-vm:~$ sudo smartctl -i /dev/sda smartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.11.0-18-generic] (local build) Copyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org === START OF INFORMATION SECTION === Device Model: QEMU HARDDISK Serial Number: QM00001 Firmware Version: 2.5+ User Capacity: 8,589,934,592 bytes [8.58 GB] Sector Size: 512 bytes logical/physical TRIM Command: Available, deterministic Device is: Not in smartctl database 7.3/5528 ATA Version is: ATA/ATAPI-7, ATA/ATAPI-5 published, ANSI NCITS 340-2000 Local Time is: Tue Mar 4 05:45:56 2025 UTC SMART support is: Available - device has SMART capability. SMART support is: Enabled If we wanted to match this disk by wild-carding the model name, we would use the following.\n#cloud-config autoinstall: storage: layout: name: lvm sizing-policy: all match: model: QEMU* Alternatively if our on-premise hardware instance had a 1GB SSD for the OS and a second 12GB spinning disk for data storage, we could use a match with size size: smallest to install the OS on the 1GB disk.\n#cloud-config autoinstall: storage: layout: name: lvm sizing-policy: all match: size: smallest Commands Running arbitrary commands is possible when autoinstall runs. Commands are specified as a list and run under \u0026ldquo;sh -c\u0026rdquo;. Its possible to specify if commands should run early in the autoinstall process, late, or when an error occurs.\nFor example we want to hit a web endpoint when the installer has completed.\n#cloud-config autoinstall: late-commands: - curl -H \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;host\u0026#34;: \u0026#34;\u0026#39;$HOSTNAME\u0026#39;\u0026#34;}\u0026#39; http://myapi.example.com/success - echo \u0026#34;Install Success\u0026#34; \u0026gt; /var/log/my.log To run a command before the autoinstall process runs, like downloading internal x509 certificates:\n#cloud-config autoinstall: early-commands: - mkdir /etc/ssl/mycerts - wget -O /etc/ssl/mycerts/internal.pem \u0026#34;http://x509api.example.com/certs/$HOSTNAME\u0026#34; Or reporting an error when autoinstall fails\n#cloud-config autoinstall: error-commands: - echo \u0026#34;Install failed\u0026#34; \u0026gt; /var/log/my.log - curl -H \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;host\u0026#34;: \u0026#34;\u0026#39;$HOSTNAME\u0026#39;\u0026#34;}\u0026#39; http://myapi.example.com/failures cloud-init It\u0026rsquo;s possible to invoke cloud-init from autoinstall, allowing for additional functionality. This is done by placing the cloud-init entries under a user-data key. Here\u0026rsquo;s a cloud-init example that installs a few packages.\ncloud-init and autoinstall sometimes perform similar tasks. When configuring a storage device with cloud-init, I found it was better to use autoinstall as the cloud-init changes were overwritten.\n#cloud-config autoinstall: user-data: package_update: true # update the list of available packages package_upgrade: true # upgrade currently installed packages. packages: - curl - ca-certificates Other It\u0026rsquo;s possible to configure a local proxy for downloading software packages. Running apt-cacher-ng as a package proxy inside a docker container on my laptop helps when I\u0026rsquo;m on a high latency Internet connection.\n#cloud-config autoinstall: proxy: http://10.0.2.2:3142 Provision a physical host A complete autoinstall user-data file can be downloaded from here. It contains all the examples listed in this post.\nProvisioning a physical host is very similar to using QEMU. The only change is when starting the python webserver. Instead of python binding to 127.0.0.1, have it bind to all interfaces so configs can be downloaded by remote hosts.\npython3 -m http.server -d $PWD 8080 A USB thumb drive is needed to make the Ubuntu ISO available to the physical host; and a monitor \u0026amp; keyboard are needed to modify the GRUB menu when the on-premise hosts boots. When modifying the GRUB menu, instead of using http://10.0.2.2 in the nocloud URL, specify the hostname of the host running the python webserver. In my scenario, the hostname would resolve to my personal laptop.\nWrapping Up By leveraging autoinstall, it\u0026rsquo;s possible to reliably reproduce system setups, whether for virtual machines or bare-metal hosts. In this post, autoinstall was explored as a method to streamline unattended provisioning for Ubuntu instances. Using a QEMU-based test environment, it was possible to quickly iterate on autoinstall configurations by modifying the GRUB menu to pull configuration files over HTTP. The process demonstrated how to format storage devices, set up networking, and run shell commands during installation.\nNext steps? If looking to extend this setup, consider integrating additional automation, such as PXE boot for network-based installs or using cloud-init to interact with configuration management systems like Puppet or Chef. If you have insights from your own experiences, feel free to share them on Bluesky.\n","permalink":"https://amf3.github.io/articles/cloudinit/autoinstall/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn a \u003ca href=\"../../../articles/cloudinit/intro/\"\u003eprevious post\u003c/a\u003e, I discussed using cloud-init and Multipass as a method of provisioning\nvirtual machines on a local computer with a cloud-like API.  Today I am going\nto dive deeper with Ubuntu and how their autoinstall API can simplify on-premise host provisioning.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://canonical-subiquity.readthedocs-hosted.com/en/latest/intro-to-autoinstall.html\"\u003eautoinstall\u003c/a\u003e\nis a tool that allows for unattended installations of Ubuntu, ensuring consistency, reporducibility, and providing automation\nacross a fleet of hosts.  In this post I\u0026rsquo;ll walk through an example of using autoinstall\nto configure networking, local storage, and demonstrate shell command execution during provisioning.\u003c/p\u003e","title":"Unattended Ubuntu Installs - Virtual Machines to Bare-Metal"},{"content":"Intro Cloud compute companies like GCP, AWS, or Azure offer a management API for allocating resources. In the on-premise space, services such as Docker or Incus provide APIs for managing containers or virtual machines (VMs). But what about installing the operating system (OS) on bare-metal hosts? What API exists for this task? This is where cloud-init enters the picture, providing the ability to provision VMs or bare-metal hardware.\ncloud-init is a useful tool that doesn\u0026rsquo;t rely on network services like PXE as a dependency. Its simplicity saves time by removing the need to navigate OS installation menus, while ensuring user accounts and installed software packages are consistent across hosts. So why should one bother using cloud-init if they are managing a single host at home? In the event the OS needs to be reinstalled due to failure, cloud-init allows one to quickly restore the system to a known state.\nThis example will use cloud-init to configure a Personal Package Archive (PPA), install Docker, and create a user account inside a Ubuntu VM.\nPrerequisite I find that using cloud-init with Multipass is a easy way to get started. Multipass is a virtual machine manager that works with Linux, MacOS (arm \u0026amp; intel), and Windows. When launching a new VM, Multipass is capable of initializing the VM with cloud-init. If Multipass isn\u0026rsquo;t already installed, this link will provide instructions for installing Multipass. For this cloud-init introduction, I\u0026rsquo;m using Multipass on a M2 Macbook running MacOS Sequoia.\ncloud-init Like many infrastructure tools, the input data for cloud-init is a YAML file. For specifics of this schema, consult the official cloud-init documentation. There one will find that cloud-init input file will need to be prefixed with #cloud-config.\nPackage Management Lets get started with package management for our Multipass instance. This section will show how to add an external PPA (software repository) to the VM with cloud-init to provide additional software packages and define a list of software packages to be installed on the VM.\nAdd External PPA Add the 3rd-party PPA provided by Docker, Inc.\n# Add Docker\u0026#39;s PPA for Ubuntu apt: sources: docker.list: # This snippet comes from https://stackoverflow.com/a/62540068 source: deb [arch=arm64] https://download.docker.com/linux/ubuntu $RELEASE stable # Key ID can be found with “gpg --show-keys \u0026lt;(curl -s https://download.docker.com/linux/ubuntu/gpg)” keyid: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88 Should the GPG key ID for the Docker PPA change, I have left a comment above on how to find that value.\nThis is how the GPG output appears in 2025.\n$ gpg --show-keys \u0026lt;(curl -s https://download.docker.com/linux/ubuntu/gpg) pub rsa4096 2017-02-22 [SCEA] 9DC858229FC7DD38854AE2D88D81803C0EBFCD88 uid Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt; sub rsa4096 2017-02-22 [S] Define Package List Specify a list of packages to install.\n# Update the list of packages available online package_update: true # Upgrade all installed packages package_upgrade: true # Install docker \u0026amp; other utilities packages: - apt-transport-https - ca-certificates - curl - gnupg-agent - software-properties-common - docker-ce - docker-ce-cli - containerd.io - docker-buildx-plugin - docker-compose-plugin User Management Here a new user account is created and added to the docker group with cloud-init. Its likely our user will require both a password \u0026amp; ssh key for remote access. A public ssh key and a password hash is needed for cloud-init input.\nSecrets: Generating a Password Hash To create a password hash, use the mkpasswd command from Ubuntu\u0026rsquo;s whois package. This example will hash the weak password of \u0026ldquo;abc123\u0026rdquo; with the sha512 algorithm. A password better than \u0026ldquo;abc123\u0026rdquo; should be used if following these examples.\n$ mkpasswd -m sha-512 \u0026#34;abc123\u0026#34; $6$EkwQ38oDCPnJDuui$QKw3IISzY3emHXgJ/QHeEH8xyzGOKB3N6.bU/wAkwf4KDRsreB2iApa/EHULbunx6v9o9Q8foq4K.d8WtHukU/ As mkpasswd is specific to Linux and doesn\u0026rsquo;t work with MacOS, one can alternatively use openssl to create a password hash.\n$ echo abc123 | openssl passwd -6 -stdin $6$tdPON3RwkVViXg41$4O9euMZeGFJQXgJ3bvP3YtVcCw9BwIMHLLkix1s/R7woSuAAFvWWtrqqQ.33ESzgcUi9/HdEwelqB9jJUIrpU0 Secrets: Generating a SSH public private key pair To create a SSH key pair, use ssh-keygen: ssh-keygen -t ed25519 -f ./docker_vm_key -C \u0026quot;app@docker_vm\u0026quot; -P abc123. This will create a public \u0026amp; private ssh key in the current directory, with the easily guessable passphrase of abc123. Once again, use a better passphrase if following these examples.\nDefining the User Account This defines an application account named \u0026ldquo;app\u0026rdquo;. The ssh_authorized_keys value comes from the contents of docker_vm_key.pub.\nAs a convenience, the public and private ssh keys from this example are provided.\n# create the docker group groups: - docker users: - name: app groups: [docker, admin, users] gecos: Application User shell: /bin/bash lock_passwd: true passwd: $6$tdPON3RwkVViXg41$4O9euMZeGFJQXgJ3bvP3YtVcCw9BwIMHLLkix1s/R7woSuAAFvWWtrqqQ.33ESzgcUi9/HdEwelqB9jJUIrpU0 ssh_authorized_keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHsPNGa1NJLd4edDLRI033Sw33Nkl6qO+52qNAhY556C app@docker_vm Putting it all together I\u0026rsquo;ve combined the YAML snippets into a single file named docker-install.yaml which can be downloaded here.\nRun the following to see cloud-init in action. This will create a virtual machine with 2 virtual CPU cores, 2 GB of ram, with a 4GB virtual disk using the LTS release of Ubuntu. Depending on your Internet speed, this may take a few minutes as you\u0026rsquo;ll be downloading packages from the Internet.\n$ multipass launch -n docker-demo --cloud-init docker-install.yaml -c 2 -m 2G -d 4G lts To find the new VM and access it over SSH with the private key so a docker command can be ran from a remote shell.\n% mp list Name State IPv4 Image docker-demo Running 192.168.64.32 Ubuntu 24.04 LTS 172.17.0.1 % ssh -l app -i ./docker_vm_key 192.168.64.32 The authenticity of host \u0026#39;192.168.64.32 (192.168.64.32)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:EUqLjr9n9CyjKY6Y8EzNQGomeEtpePMFo5BXjO8YfHY. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes ... app@docker-demo:~$ docker run hello-world Unable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world c9c5fd25a1bd: Pull complete Digest: sha256:e0b569a5163a5e6be84e210a2587e7d447e08f87a0e90798363fa44a0464a1e8 Status: Downloaded newer image for hello-world:latest Hello from Docker! ... Conclusion Several cloud-init basics have been covered in this introduction. Like adding a PPA, installing software packages, and creating a user account.\nWhile I understand that installing Docker in my example might not represent the typical workflow. Combining cloud-init concepts with Multipass creates a local mini-cloud on my Macbook. I can quickly iterate through cloud-init data file changes for other platforms like AWS or on-premise hardware.\ncloud-init is capable of much more, like formatting hard drives or managing network interfaces. These \u0026amp; other topics will be covered in followups which I will announce on Bluesky. Follow me for notifications of when its made available. Otherwise, try out these examples and let me know what works.\n","permalink":"https://amf3.github.io/articles/cloudinit/intro/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eCloud compute companies like GCP, AWS, or Azure offer a management API for allocating resources. In the on-premise space,\nservices such as Docker or Incus provide APIs for managing containers or virtual machines (VMs). But what about installing\nthe operating system (OS) on bare-metal hosts? What API exists for this task? This is where\n\u003ca href=\"https://github.com/canonical/cloud-init\"\u003ecloud-init\u003c/a\u003e enters the picture, providing the ability to provision VMs or\nbare-metal hardware.\u003c/p\u003e\n\u003cp\u003ecloud-init is a useful tool that doesn\u0026rsquo;t rely on network services like PXE as a dependency.  Its simplicity saves time by\nremoving the need to navigate OS installation menus, while ensuring user accounts and installed software packages are consistent\nacross hosts. So why should one bother using cloud-init if they are managing a single host at home? In the event\nthe OS needs to be reinstalled due to failure, cloud-init allows one to quickly restore the system to a known state.\u003c/p\u003e","title":"Getting started with cloud-init for unattended Linux deployments"}]
<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Decoupling Compute and Storage | Adam Faris</title>
<meta name="keywords" content="">
<meta name="description" content="Why a Two-Node Docker Swarm with ZFS Snapshots Is Enough">
<meta name="author" content="">
<link rel="canonical" href="https://amf3.github.io/articles/arch/2swarm_nodes/">
<meta name="google-site-verification" content="6ZFu-1_Lir3DsFJP8sshXEJ1_SjtFUw9TIISOcaJh7E">
<meta name="msvalidate.01" content="C1E02AC59FE7ECBDB6D9EFB7D5E02B65">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://amf3.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://amf3.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://amf3.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://amf3.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://amf3.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://amf3.github.io/articles/arch/2swarm_nodes/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-MKR06D6KGD"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-MKR06D6KGD');
        }
      </script><meta property="og:url" content="https://amf3.github.io/articles/arch/2swarm_nodes/">
  <meta property="og:site_name" content="Adam Faris">
  <meta property="og:title" content="Decoupling Compute and Storage">
  <meta property="og:description" content="Why a Two-Node Docker Swarm with ZFS Snapshots Is Enough">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2025-12-16T21:51:16-08:00">
    <meta property="article:modified_time" content="2025-12-16T21:51:16-08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Decoupling Compute and Storage">
<meta name="twitter:description" content="Why a Two-Node Docker Swarm with ZFS Snapshots Is Enough">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "articles",
      "item": "https://amf3.github.io/articles/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Decoupling Compute and Storage",
      "item": "https://amf3.github.io/articles/arch/2swarm_nodes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Decoupling Compute and Storage",
  "name": "Decoupling Compute and Storage",
  "description": "Why a Two-Node Docker Swarm with ZFS Snapshots Is Enough",
  "keywords": [
    
  ],
  "articleBody": "Who Should Not Read This Post This post does not discuss:\ndata support for multiple writers having zero downtime cloud-scale best practices Instead I discuss embracing single points of failure (SPOF) in exchange for simplicity within environments that can tolerate downtime.\nPermission to Not Over-Engineer In environments that tolerate downtime, having a SPOF is great because they are simple to deploy and it’s obvious what failed.\nLets examine the typical home router. In addition to routing packets between the LAN and the Internet, the home router also provides DHCP and DNS services. Then someone learns of PiHole and runs it on a separate device like a Raspberry PI. Now the router queries PiHole for adblock entries during DNS resolution. Ad-free browsing works works great; until HTTP requests stop working. With this scenario, it’s not obvious if it’s an issue on the router or an issue with PiHole.\nI recently ran into a situation on deciding how to improve my container deployments.\nCurrently I use Docker Engine with Compose files to manage running containers. Container storage is either named volumes or bind mounts, both of which write to the local disk of the host running Docker Engine. This works but it has a few issues:\nServices go offline during planned maintenance on the host Container placement is manual. Having a second hardware instance doesn’t help. Docker Compose deployments cause container restarts Failed Docker Compose deployments have no rollback I started to look at K3S, Nomad, and Docker Swarm to address these short comings. While I still plan on using a single hardware instance, orchestration would help with downtime during planned maintenance. This led to thinking how the design would change by adding a second host and because of distributed storage, would I be trapped by choices made today?\nCompute Does Not Need to Scale with Storage I know many distributed storage solutions use a consensus model requiring three hosts. Does this mean I’m required to run three computers and pay for the additional power usage?\nIt feels strange to admit, but this led to two evenings of not knowing how to proceed. I assumed that adding a second host meant I also needed distributed storage and now I need three hosts. This assumption stalled everything. Eventually I realized its okay to have orchestration for compute but not storage.\nThinking about storage and compute as separate problems was the unlock I needed to move forward. I won’t need the complications of running Kubernetes or Nomad. The simpler solution of using Docker Swarm can solve my deployment requirements with either one or two hardware instances.\nCompute: What Swarm Provides Swarm is responsible for compute orchestration and overlay networking. It does not provide storage orchestration or multi-writer safety.\nSwarm mode is enabled on each Docker instance, and the instance registers with the cluster as either a manager role or worker role. The Swarm worker provides CPU and memory resources to the running containers and must have sufficient capacity for its workloads.\nAs a Swarm manager, the instance is responsible for orchestration, (scheduling, container placement, maintaining cluster state) by using RAFT consensus. RAFT needs a quorum to make decisions and a quorum is calculated as “(N-1)/2 + 1”, where N is the number of Swarm managers. In a two node deployment I can only have a single manager, otherwise quorum will never be reached.\nIn a two node Swarm deployment, one node would run both the worker and manager. The second node only joins as a worker. Write heavy apps are assigned to the manager instance and stateless apps assigned to the worker node. Swarm labels are applied to services to ensure container placement.\nStorage Swarm itself does not manage storage. Swarm presumes storage is managed externally to its processes. For example, CephFS would be mounted by external systems on the host OS and treated as a local filesystem by Swarm. A take away is Swarm does not provide multi-writer safety. If two containers attempt to write to the same dataset, Swarm does nothing to provide locking or prevent data corruption. It assumes the underlying storage layer handles this.\nDatabases have built in multi-writer support by providing locking at either the table or row level. Usually they are capable of replicating data to a secondary instance. Because of these features, they solve my multi-instance storage problem by scaling independently of the container platform.\nLocal filesystems are more challenging. Detected filesystem changes must be transferred to secondary storage. I use ZFS snapshots with send and receive over the network. Snapshots are taken on the manager instance and replicated to the worker node every 15 minutes using zrepl.\nThis approach is a tradeoff for simplicity and assumes one is okay with a bounded data-loss window. If the primary ZFS dataset gets corrupted, up to 15 minutes of data could be lost. Write heavy applications are pinned to the manager node so their datasets can be replicated.\nAnother challenge is application-level locking. Databases enforce locking in their clients, but local filesystems do not. While mechanisms like flock or fcntl exist, they only work if the application uses them. To avoid data corruption caused by lack of locking, I make sure to deploy only a single container for each dataset. This greatly simplifies the storage design by avoiding the need of distributed locking.\nFor container storage, I use a mix of named volumes and bind mounts. Persistent data that requires replication uses bind mounts. Ephemeral data that needs to persist between container restarts uses named volumes. Because bind mounts reference host paths, the same directory structure and ownership (UID) must exist on both hardware instances. Configuration management is used to ensure consistency between the two hardware instances.\nFailure Modes Single points of failure are not accidental in this design. They provide predictable and observable failure modes, that act as constraints for modeling the system.\nWorker node failure: This is a non-event. Stateless services are rescheduled onto the manager node by Swarm. No data recovery is required.\nManager node failure: This is an expected control-plane failure where manual intervention is required. The worker node is promoted to manager and services restarted on the manager. This is a deliberate tradeoff in exchange for simplicity.\nData corruption on the manager node: This is treated the same as a manager failure. The worker node is promoted to manager and services are restored using the most recent ZFS snapshot.\nReplication lag: Replication is monitored to ensure snapshot transfer time remains below the snapshot interval. If replication falls behind, the response is to reduce the snapshot interval or increase network capacity. This keeps data loss bounded and predictable.\nMulti-writer data corruption: If because of misconfiguration, more than one container (writer) is launched for the same dataset and corruption occurs, stop all containers for the application and restore the data from the last known-good ZFS snapshot.\nDepending on snapshot age and response time, expected downtime for these scenarios is 15-30 minutes. The goal is not uninterrupted service, but fast understandable recovery.\nWhen the solution is smaller than the problem I’m not defending Swarm in this post. I’m defending clear failure modes, explicit ownership, and operational practicality. The goal is to start with requirements and choose the smallest system that satisfies them.\nBecause this design keeps the data independent of orchestration, it provides an escape hatch to K3s or Nomad without having to decouple data from Swarm.\nIf my availability requirements change, then I would add a third low-power node like a Raspberry Pi to establish a three node quorum. I would also migrate from Swarm to K3s to leverage the Longhorn distributed filesystem which is only available under Kubernetes. For multi-writer workloads, shared filesystems like NFS could work if the container application supports file locking. Neither Swarm or Longhorn provides multi-writer locking on behalf of the application.\nIf you have any thoughts on this post, feel free to share them on Bluesky.\n",
  "wordCount" : "1316",
  "inLanguage": "en",
  "datePublished": "2025-12-16T21:51:16-08:00",
  "dateModified": "2025-12-16T21:51:16-08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://amf3.github.io/articles/arch/2swarm_nodes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Adam Faris",
    "logo": {
      "@type": "ImageObject",
      "url": "https://amf3.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://amf3.github.io/" accesskey="h" title="Adam Faris (Alt + H)">Adam Faris</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://amf3.github.io/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="https://amf3.github.io/articles/" title="articles">
                    <span>articles</span>
                </a>
            </li>
            <li>
                <a href="https://amf3.github.io/about/" title="about">
                    <span>about</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://amf3.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://amf3.github.io/articles/">articles</a></div>
    <h1 class="post-title entry-hint-parent">
      Decoupling Compute and Storage
    </h1>
    <div class="post-description">
      Why a Two-Node Docker Swarm with ZFS Snapshots Is Enough
    </div>
    <div class="post-meta"><span title='2025-12-16 21:51:16 -0800 PST'>December 16, 2025</span>&nbsp;·&nbsp;<span>1316 words</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#who-should-not-read-this-post" aria-label="Who Should Not Read This Post">Who Should Not Read This Post</a></li>
                <li>
                    <a href="#permission-to-not-over-engineer" aria-label="Permission to Not Over-Engineer">Permission to Not Over-Engineer</a></li>
                <li>
                    <a href="#compute-does-not-need-to-scale-with-storage" aria-label="Compute Does Not Need to Scale with Storage">Compute Does Not Need to Scale with Storage</a></li>
                <li>
                    <a href="#compute-what-swarm-provides" aria-label="Compute: What Swarm Provides">Compute: What Swarm Provides</a></li>
                <li>
                    <a href="#storage" aria-label="Storage">Storage</a></li>
                <li>
                    <a href="#failure-modes" aria-label="Failure Modes">Failure Modes</a></li>
                <li>
                    <a href="#when-the-solution-is-smaller-than-the-problem" aria-label="When the solution is smaller than the problem">When the solution is smaller than the problem</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="who-should-not-read-this-post">Who Should Not Read This Post<a hidden class="anchor" aria-hidden="true" href="#who-should-not-read-this-post">#</a></h2>
<p>This post does not discuss:</p>
<ul>
<li>data support for multiple writers</li>
<li>having zero downtime</li>
<li>cloud-scale best practices</li>
</ul>
<p>Instead I discuss embracing single points of failure (SPOF) in exchange for simplicity within
environments that can tolerate downtime.</p>
<h2 id="permission-to-not-over-engineer">Permission to Not Over-Engineer<a hidden class="anchor" aria-hidden="true" href="#permission-to-not-over-engineer">#</a></h2>
<p>In environments that tolerate downtime, having a SPOF is great because they are simple to deploy and it&rsquo;s obvious what failed.</p>
<p>Lets examine the typical home router.  In addition to routing packets between the LAN and the Internet, the home router also
provides DHCP and DNS services.  Then someone learns of PiHole and runs it on a separate device like a Raspberry PI. Now the router
queries PiHole for adblock entries during DNS resolution.  Ad-free browsing works works great; until HTTP requests
stop working.  With this scenario, it&rsquo;s not obvious if it&rsquo;s an issue on the router or an issue with PiHole.</p>
<p>I recently ran into a situation on deciding how to improve my container deployments.</p>
<p>Currently I use Docker Engine with Compose files to manage running containers. Container storage is either named volumes or bind mounts,
both of which write to the local disk of the host running Docker Engine.  This works but it has a few issues:</p>
<ul>
<li>Services go offline during planned maintenance on the host</li>
<li>Container placement is manual. Having a second hardware instance doesn&rsquo;t help.</li>
<li>Docker Compose deployments cause container restarts</li>
<li>Failed Docker Compose deployments have no rollback</li>
</ul>
<p>I started to look at K3S, Nomad, and Docker Swarm to address these short comings. While I still plan on using a single hardware
instance, orchestration would help with downtime during planned maintenance.  This led to thinking how the design would change by adding
a second host and because of distributed storage, would I be trapped by choices made today?</p>
<h2 id="compute-does-not-need-to-scale-with-storage">Compute Does Not Need to Scale with Storage<a hidden class="anchor" aria-hidden="true" href="#compute-does-not-need-to-scale-with-storage">#</a></h2>
<p>I know many distributed storage solutions use a consensus model requiring three hosts. Does this mean I&rsquo;m required to run three computers
and pay for the additional power usage?</p>
<p>It feels strange to admit, but this led to two evenings of not knowing how to proceed. I assumed that adding a second
host meant I also needed distributed storage and now I need three hosts. This assumption stalled everything.  Eventually I realized
its okay to have orchestration for compute but not storage.</p>
<p>Thinking about storage and compute as separate problems was the unlock I needed to move forward. I won&rsquo;t need the complications of
running Kubernetes or Nomad. The simpler solution of using Docker Swarm can solve my deployment requirements with either one or two
hardware instances.</p>
<h2 id="compute-what-swarm-provides">Compute: What Swarm Provides<a hidden class="anchor" aria-hidden="true" href="#compute-what-swarm-provides">#</a></h2>
<p>Swarm is responsible for compute orchestration and overlay networking.  It does not provide storage orchestration or multi-writer safety.</p>
<p>Swarm mode is enabled on each Docker instance, and the instance registers with the cluster as either a manager role or worker role. The
Swarm worker provides CPU and memory resources to the running containers and must have sufficient capacity for its workloads.</p>
<p>As a Swarm manager, the instance is responsible for orchestration, (scheduling, container placement, maintaining cluster state) by using RAFT
consensus. RAFT needs a quorum to make decisions and a quorum is calculated as &ldquo;(N-1)/2 + 1&rdquo;, where N is the number of Swarm
managers.  In a two node deployment I can only have a single manager, otherwise quorum will never be reached.</p>
<p>In a two node Swarm deployment, one node would run both the worker and manager. The second node only joins as a worker.  Write heavy apps are assigned
to the manager instance and stateless apps assigned to the worker node. Swarm labels are applied to services to ensure container placement.</p>
<h2 id="storage">Storage<a hidden class="anchor" aria-hidden="true" href="#storage">#</a></h2>
<p>Swarm itself does not manage storage. Swarm presumes storage is managed externally to its processes. For example, CephFS
would be mounted by external systems on the host OS and treated as a local filesystem by Swarm.  A take away is Swarm does not provide
multi-writer safety. If two containers attempt to write to the same dataset, Swarm does nothing to provide locking or prevent data
corruption.  It assumes the underlying storage layer handles this.</p>
<p>Databases have built in multi-writer support by providing locking at either the table or row level.  Usually they are capable of replicating
data to a secondary instance.  Because of these features, they solve my multi-instance storage problem by scaling independently of the container
platform.</p>
<p>Local filesystems are more challenging. Detected filesystem changes must be transferred to secondary storage. I use ZFS snapshots with
send and receive over the network.  Snapshots are taken on the manager instance and replicated to the worker node every 15 minutes using
<a href="https://github.com/zrepl/zrepl">zrepl</a>.</p>
<p>This approach is a tradeoff for simplicity and assumes one is okay with a bounded data-loss window. If the primary ZFS dataset gets corrupted, up
to 15 minutes of data could be lost.  Write heavy applications are pinned to the manager node so their datasets can be replicated.</p>
<p>Another challenge is application-level locking.  Databases enforce locking in their clients, but local filesystems do not. While mechanisms
like flock or fcntl exist, they only work if the application uses them.  To avoid data corruption caused by lack of locking, I make sure
to deploy only a single container for each dataset.  This greatly simplifies the storage design by avoiding the need of distributed locking.</p>
<p>For container storage, I use a mix of named volumes and bind mounts. Persistent data that requires replication uses bind mounts. Ephemeral data
that needs to persist between container restarts uses named volumes. Because bind mounts reference host paths, the same directory structure and
ownership (UID) must exist on both hardware instances. Configuration management is used to ensure consistency between the two hardware instances.</p>
<h2 id="failure-modes">Failure Modes<a hidden class="anchor" aria-hidden="true" href="#failure-modes">#</a></h2>
<p>Single points of failure are not accidental in this design. They provide predictable and observable failure modes, that act as constraints for
modeling the system.</p>
<ul>
<li>
<p><strong>Worker node failure:</strong> This is a non-event. Stateless services are rescheduled onto the manager node by Swarm. No data recovery is required.</p>
</li>
<li>
<p><strong>Manager node failure:</strong> This is an expected control-plane failure where manual intervention is required. The worker node is promoted to manager and services restarted on the
manager. This is a deliberate tradeoff in exchange for simplicity.</p>
</li>
<li>
<p><strong>Data corruption on the manager node:</strong> This is treated the same as a manager failure. The worker node is promoted to manager and services are restored using the most recent ZFS snapshot.</p>
</li>
<li>
<p><strong>Replication lag:</strong> Replication is monitored to ensure snapshot transfer time remains below the snapshot interval. If replication falls behind, the response is to reduce the snapshot interval or increase network capacity. This keeps data loss bounded and predictable.</p>
</li>
<li>
<p><strong>Multi-writer data corruption:</strong> If because of misconfiguration, more than one container (writer) is launched for the same dataset and corruption occurs, stop all containers for the application and restore the data from the last known-good ZFS snapshot.</p>
</li>
</ul>
<p>Depending on snapshot age and response time, expected downtime for these scenarios is 15-30 minutes.  The goal is not uninterrupted
service, but fast understandable recovery.</p>
<h2 id="when-the-solution-is-smaller-than-the-problem">When the solution is smaller than the problem<a hidden class="anchor" aria-hidden="true" href="#when-the-solution-is-smaller-than-the-problem">#</a></h2>
<p>I&rsquo;m not defending Swarm in this post. I&rsquo;m defending clear failure modes, explicit ownership, and operational practicality. The goal is to start
with requirements and choose the smallest system that satisfies them.</p>
<p>Because this design keeps the data independent of orchestration, it provides an escape hatch to K3s or Nomad without having to decouple data from Swarm.</p>
<p>If my availability requirements change, then I would add a third low-power node like a Raspberry Pi to establish a three node quorum.  I would also
migrate from Swarm to K3s to leverage the Longhorn distributed filesystem which is only available under Kubernetes. For multi-writer workloads, shared
filesystems like NFS could work if the container application supports file locking. Neither Swarm or Longhorn provides multi-writer locking on behalf of
the application.</p>
<p>If you have any thoughts on this post, feel free to share them on <a href="https://bsky.app/profile/af9.us">Bluesky</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://amf3.github.io/">Adam Faris</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
